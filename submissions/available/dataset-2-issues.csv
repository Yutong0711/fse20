key,no,text,label,Reason,Fuzzy Rule Set,,
CASSANDRA-10052,1,"Misleading down-node push notifications when rpc_address is shared.When a node goes down, the other nodes learn that through the gossip.And I do see the log from (Gossiper.java): 
  <SOURCECODE> 
  Saying: ""InetAddress 192.168.101.1 is now Down"", in the Cassandra's system log.Now on all the other nodes the client side (java driver) says, "" Cannot connect to any host, scheduling retry in 1000 milliseconds"".They eventually do reconnect but some queries fail during this intermediate period.To me it seems like when the server pushes the nodeDown event, it call the getRpcAddress(endpoint), and thus sends localhost as the argument in the nodeDown event.As in org.apache.cassandra.transport.Server.java 
  <SOURCECODE> 
  the getRpcAddress returns localhost for any endpoint if the cassandra.yaml is using localhost as the configuration for rpc_address (which by the way is the default).",no,,"[ss2_all_to_one, pf2_duration]",,
CASSANDRA-10188,2,"sstableloader does not use MAX_HEAP_SIZE env parameter.Currently the sstableloader script hard codes java's max heap size parameter to 256MB.The issue was discussed in 
   <URL> and it looks like the agreed solution was to allow the value to change through parameters that were going to be introduced as part of 
   <URL>.This parameter wasn't added to sstableloader, making it inconsistent with the other utilities and provides a problem loading large files.",no,,[tk3_memory_unit],,
CASSANDRA-10208,3,"Windows dtest 2.2: commitlog_test.TestCommitLog.stop_failure_policy_test.<tt>commitlog_test.TestCommitLog.stop_commit_failure_policy_test and <tt>commitlog_test.TestCommitLog.stop_failure_policy_test are failing here: 
   <URL> 
  The stack trace is as follows: 
  <SOURCECODE> 
  I can only reproduce the failure for <tt>stop_commit_failure_policy_test locally, but I assume that's just flakiness.",no,,[],,
CASSANDRA-10337,4,"Make SSTableReader and Partition access paths uniform.<URL> attempts to do this in a quick manner to help simplify the codebase a little, but it's still a long way from being optimal.We can follow up with some changes to make it cleaner still:  
  We can modify <tt>Partition a little so that <tt>SSTableReader could return a lightweight instance (or a common base class) with the set of accessors necessary for answering queries.Filters can then operate on this object uniformly.We can <em>perhaps</em> simultaneously unify the access methods within <tt>Partition by removing <tt>getRow and <tt>unfilteredIterator(Slices), and introduce an <tt>UnfilteredRowSearchIterator that permits indexed access to the underlying data.",no,,"[gk7_speed, tk6_iteration]",,
CASSANDRA-10356,5,"Add row filters like in HBase.I would like to propose a new feature in Cassandra, namely HBase-like filters (
   <URL>).This would be beneficial especially in context of Hadoop jobs - where input data to a job may be greatly reduced resulting in faster execution.Besides, this may be a nice feature that allows querying values within a given wide row.In HBase this functionality performs a wide row scan and filters out those columns not satisfying given predicate.",yes,resulting in faster execution,"[gk7_speed, gk3_reduce]",,
CASSANDRA-10522,6,"counter upgrade dtest fails on 3.0 with JVM assertions disabled.<tt>counter_tests.TestCounters.upgrade_test will fail when run on a cluster with JVM assertions disabled.The tests will hang when cassandra throws the following exception: 
  <SOURCECODE> 
  These tests both pass with/without JVM assertions on C* 2.2 and pass on 3.0 when assertions are enabled.Ran against: apache/cassandra-2.2: <tt>7cab3272455bdd16b639c510416ae339a8613414 apache/cassandra-3.0: <tt>f21c888510b0dbbea1a63459476f2dc54093de63 
  Ran with cmd: <tt>JVM_EXTRA_OPTS=-da PRINT_DEBUG=true nosetests -xsv counter_tests.TestCounters.upgrade_test",no,,[sk2_block_hang_crash],,
CASSANDRA-1074,7,"check bloom filters to make minor compaction able to delete (some) tombstones.Given a tombstoned key which is older than GCGraceSeconds, current (0.6.1) compaction implementation still requires a major compaction for the key to actually be deleted.The major compaction is required is because we must know whether there is a version of the key inside all SSTables associated with the columnfamily, including ones not involved in minor compactions.However, as we have bloom filters into each one of these SSTables, minor compaction can relatively inexpensively check for existence of this key in SSTable files not involved in the current minor compaction, and thereby delete the key, assuming all bloom filters return negative.If the filter returns positive, a major compaction would of course still be required.For use cases like 
   <URL> where users are strongly motivated to not do a major compaction, this seems to answer the jbellis objection : 
  ""You don't want to skip large files in major compactions, since the definition of major is ""compact everything so it is safe to remove tombstones."" ""The above described improvement appears to provide ""safe to remove (some) tombstones"" without requiring ""compact everything"", and so may be a useful optimization.#NAME?",no,,"[ss15_each_one, sk10_skip, ss26_negative_do_everything]",,
CASSANDRA-10891,8,"Cassandra driver doesn't allow to insert null as a value in map.When cassandra has column with type map&lt;text,text&gt; and I try to insert entry with null value cassandra driver throws following exception: 
  <SOURCECODE>",no,,[],,
CASSANDRA-11042,9,"Dropping a keyspace while nodes are joining causes them to fail.Bootstrap a node and watch it happily enter joining state and start streaming data.Decide you don't want to wait that long and drop a keyspace containing a pile of unnecessary data.Notice that the new joining node is now missing from 'juju status' 
  Discover the traceback in system.log: 
  java.io.IOException: CF 6ce71111-8fa0-11e5-927b-ef57ab5def0e was dropped during streaming at org.apache.cassandra.streaming.compress.CompressedStreamReader.read(CompressedStreamReader.java:71) ~
   <ERROR></ERROR> at org.apache.cassandra.streaming.messages.IncomingFileMessage$1.deserialize(IncomingFileMessage.java:48) ~
   <ERROR></ERROR> at org.apache.cassandra.streaming.messages.IncomingFileMessage$1.deserialize(IncomingFileMessage.java:38) ~
   <ERROR></ERROR> at org.apache.cassandra.streaming.messages.StreamMessage.deserialize(StreamMessage.java:56) ~
   <ERROR></ERROR> at org.apache.cassandra.streaming.ConnectionHandler$IncomingMessageHandler.run(ConnectionHandler.java:250) ~
   <ERROR></ERROR> at java.lang.Thread.run(Thread.java:745) 
   <ERROR></ERROR> INFO 
   <ERROR></ERROR> 2016-01-20 08:50:42,693 StreamResultFuture.java:180 - 
   <URL> Session with /xx.xx.xx.xx is complete WARN 
   <ERROR></ERROR> 2016-01-20 08:50:42,693 StreamResultFuture.java:207 - 
   <URL> Stream failed ERROR 
   <ERROR></ERROR> 2016-01-20 08:50:42,694 CassandraDaemon.java:581 - Exception encountered during startup java.lang.RuntimeException: Error during boostrap: Stream failed",no,,[sk1_negative_necessary],,
CASSANDRA-11084,10,"cassandra-3.0 eclipse-warnings.REF = origin/cassandra-3.0  COMMIT = 414c1c5771ca05c23c8c1279dbdb90a673dda040 
  <SOURCECODE> 
  Check the latest job on 
   <URL> for the most recent warnings",no,,[],,
CASSANDRA-11434,11,"Support EQ/PREFIX queries in CONTAINS mode without tokenization by augmenting SA metadata per term.We can support EQ/PREFIX requests to CONTAINS indexes by tracking ""partiality"" of the data stored in the OnDiskIndex and IndexMemtable, if we know exactly if current match represents part of the term or it's original form it would be trivial to support EQ/PREFIX since PREFIX is subset of SUFFIX matches.Since we attach uint16 size to each term stored we can take advantage of sign bit so size of the index is not impacted at all.",no,,[ss1_one_per],,
CASSANDRA-11520,12,Implement optimized local read path for CL.ONE.<del>Add an option to the CQL SELECT statement to</del> Bypass the coordination layer when reading locally at CL.ONE.,no,,[],,
CASSANDRA-11708,13,"sstableloader not work with ssl options.My cassandra server start with client_encryption_options.<SOURCECODE> 
  when I use sstableloader with -ts/-tspw/-ks/-kspw, I get 
  <SOURCECODE> 
  after looked at the code, I found only one way to active ssl is use -f with a configuration yaml file which has  
  <SOURCECODE> 
  so I think it's a bug, better to active ssl when I set -ts and -tspw.",no,,[],,
CASSANDRA-11945,14,"CQLSH COPY FROM attempts to parse null timestamp value representatives.CQLSH COPY FROM is broken when the table includes timestamp columns with null values 
  create table foo (id uuid primary key, t timestamp); insert into foo (now(), null); copy foo to 'foo.csv'; copy foo from 'foo.csv'; 
  ... borks with  
  Failed to import 1 rows: ParseError - can't interpret '' as a date with format %Y-%m-%d %H:%M:%S.%f%z or as int, given up without retries Failed to process 1 rows; failed rows written to import_erdev_foo.err",no,,[pf1_percentage],,
CASSANDRA-1201,15,"Red Hat files needed for packaging.The following files are needed for packaging an RPM.This is in line with the debian based things put in contrib.A spec file, an init script.I would prefer that this go in a redhat folder.And then fedora and RHEL can use it.",no,,[],,
CASSANDRA-12100,16,"Compactions are stuck after TRUNCATE.Hi, 
  since the upgrade to C* 3.0.7 I see compaction tasks getting stuck when truncating the column family.I verified this on all nodes of the cluster.Pending compactions seem to disappear after restarting the node.<SOURCECODE>",no,,[],,
CASSANDRA-12199,17,"Config class uses boxed types but DD exposes primitive types.<tt>Config&nbsp;class contains a lot of properties that are defined using boxed types - (<tt>Config.dynamic_snitch_update_interval_in_ms) but the corresponding get-methods in <tt>DatabaseDescriptor require them to be not null.Means, setting such properties to <tt>null will lead to NPEs anyway.Proposal: 
   
   Identify all properties that use boxed values and have a default value (e.g. <tt>public Integer rpc_port = 9160;) 
   Refactor those to use primitive types",no,,[pf3_profiling],,
CASSANDRA-12206,18,CommitLogTest.replaySimple failure.Fails on both regular and compression runs.<SOURCECODE>,no,,[],,
CASSANDRA-12474,19,Define executeLocally() at the ReadQuery Level.We have <tt>execute and <tt>executeInternal at the <tt>ReadQuery level but <tt>executeLocally is missing and this makes the abstraction incomplete.,no,,[],,
CASSANDRA-1263,20,"Push replication factor down to the replication strategy.Currently the replication factor is in the keyspace metadata.As we've added the datacenter shard strategy, the replication factor becomes more computed by the replication strategy.It seems reasonable to therefore push the replication factor for the keyspace down to the replication strategy so that it can be handled in one place.This adds on the work being done in 
   <URL> since that ticket will make the replication strategy a member variable of keyspace metadata instead of just a quasi singleton giving the replication strategy state for each keyspace.That makes it able to have the replication factor.",no,,[],,
CASSANDRA-12813,21,"NPE in auth for bootstrapping node.<SOURCECODE> 
  I have a node that has been joining for around 24 hours.My application is configured with the IP address of the joining node in the list of nodes to connect to (ruby driver), and I have been getting around 200 events of this NPE per hour.I removed the IP of the joining node from the list of nodes for my app to connect to and the errors stopped.",no,,"[ss1_one_per, pf2_duration]",,
CASSANDRA-12854,22,"CommitLogTest.testDeleteIfNotDirty failed in 3.X.Example failure: 
  
   <URL> 
  <SOURCECODE>",no,,[],,
CASSANDRA-12884,23,"Batch logic can lead to unbalanced use of system.batches.It looks as though there are some odd edge cases in how we distribute the copies in system.batches.The main issue is in the filter method for org.apache.cassandra.batchlog.BatchlogManager 
  <SOURCECODE> 
  So with one or two racks we just return the first 2 entries in the list.There's no shuffle or randomisation here.",no,,[],,
CASSANDRA-12940,24,"Large compaction backlogs should slow down repairs.Repairs cause a flood of small sstables.In some situations the small sstables come in so fast that it takes longer to commit the compaction transaction than it takes to stream in the tables.This will cause a buildup in sstables, and this buildup causes compaction to go even slower (see 
   <URL>).For a cluster of mine this means running into nodes with &gt;100 loadavg, with tables that have 10k sstables.After the repair finishes the nodes go back to normal, but it takes a while and affects query latency a lot.The compaction paths could probably be faster, though I'm more interested in making repairs wait for compaction.When we have a L0 with 10000+ tables, the repair path should probably wait a minute.All I did was run 'nodetool repair' : 
  <SOURCECODE> 
  `nodetool compactionstats' shows 17 pending tasks (seems a bit low) and `nodetool netstats' shows 1861 lines of text over 138 stream sessions.",yes,affects query latency a lot.The compaction paths could probably be faster,"[gk7_speed, gk5_latency, pf2_duration]",,
CASSANDRA-12962,25,"SASI: Index are rebuilt on restart.Apparently when cassandra any index that does not index a value in every live SSTable gets rebuild.The offending code can be found in the constructor of SASIIndex.You can easilly reproduce it: 
  <SOURCECODE> 
  Log (I added additional traces): 
  <SOURCECODE> 
  I think a better behavior would be to ask users to explicitly rebuild indexes if they remove the files, that's fine as long as we handle correctly the case of new indexes.",no,,[],,
CASSANDRA-13013,26,"Potential regression from CASSANDRA-6377.As noted by 
   <URL> in 
   <URL>, in 3.0 (and prior) 
   <URL> for a partition if it is the only results when the query is a 2ndary index query.This doesn't seem to happen anymore in 3.X and that was removed by 
   <URL> (see the 
   <URL>, but that removal is actually part of the original 
   <URL>).The removal looks intentional but it's unclear to 
   <URL> and myself why it's not a potentially breaking change, and even if it's a legit change, why it was done in 3.X (then trunk) but not 3.0?<URL>, can you enlighten us?",no,,[sk13_regression],,
CASSANDRA-13278,27,"Update build.xml and build.properties.default maven repos.Only 2 of the 5 urls in build.properties.default are currently valid.java.net2, jclouds, and oauth urls all 404.<SOURCECODE>",no,,[],,
CASSANDRA-13319,28,Fix typo in cassandra-stress.Fix typo in cassandra-stress.<URL>,no,,[],,
CASSANDRA-13324,29,Outbound TCP connections ignore internode authenticator.When creating an outbound connection pool and connecting from within andOutboundTcpConnection it doesn't check if internode authenticator will allow the connection.In practice this can cause a bunch of orphaned threads perpetually attempting to reconnect to an endpoint that is will never accept the connection.,no,,[],,
CASSANDRA-13432,30,"MemtableReclaimMemory can get stuck because of lack of timeout in getTopLevelColumns().This might affect 3.x too, I'm not sure.<SOURCECODE> 
  <SOURCECODE> 
  As you can see MemtableReclaimMemory is waiting on the read barrier to be released, but there are two queries currently being executed which are locking this.Since most of the time is spent pretty low in the stack, these read operations will never timeout (they are reading rows with tons of tombstones).We also can easily detect or purge the offending line because there is no easy way to find out which partition is currently being read.The TombstoneFailureThreshold should also protect us, but it is probably being checked too high in the call stack.Looks like RangeTombstoneList or DeletionInfo should also check for DatabaseDescriptor.getTombstoneFailureThreshold()",no,,"[ss12_spend_time, sk8_timeout_expiration]",,
CASSANDRA-13505,31,dtest failure in user_functions_test.TestUserFunctions.test_migration.<SOURCECODE>,no,,[],,
CASSANDRA-13543,32,"Cassandra SASI index gives unexpected number of results.I've faced the issue with LIKE query to the column indexed by SASI index.Cassandra can return different number of rows when the data stays immutable.<SOURCECODE> 
  Query: 
  <SOURCECODE> 
  This query mostly returns 0 rows, but sometimes 1 row appears in result row set as: 
  <SOURCECODE>",no,,[],,
CASSANDRA-13662,33,"Remove unsupported CREDENTIALS message.Remove CREDENTIAL message, as protocol v1 isn't supported anyways.Let's try not to keep unused legacy classes around for any security relevant features.",no,,[],,
CASSANDRA-13707,34,"Warn or error when receiving Merkle Tree requests for unowned token ranges.When a node receives a validation request to construct a Merkle Tree, if the requested ranges are outside of the node's owned ranges we should log warning.Maintaining metrics for these events and/or rejecting the request and so failing the repair job might also be useful.",no,,[],,
CASSANDRA-13915,35,"Create a Docker container to build the docs.As requested by 
   <URL>, I will be adding a Docker container to build the docs without any prereqs (other than Docker).",no,,[],,
CASSANDRA-14082,36,"Do not expose compaction strategy index publicly.Before 
   <URL> we used the compaction strategy index to decide which disk to place a given sstable, but now we can get this directly from the disk boundary manager and keep the compaction strategy index internal only.This will ensure external consumers will use a consistent <tt>DiskBoundaries object to perform operations on multiple disks, rather than risking getting inconsistent indexes if the compaction strategy indexes change between successive calls to <tt>CSM.getCompactionStrategyIndex.",no,,[],,
CASSANDRA-14168,37,"Throw error when attempting to mutate non-existant table.When a node receives a write request for a table that was just created but it's TableMetadata is not yet registered, it will 
   <URL>.There is a small racy period though between when the <tt>TableMetadata 
   <URL> and it's <tt>ColumnFamilyStore is 
   <URL>, that a write request can be 
   <URL> - even though an error is logged, the request will be completed and an ack wrongly sent to the coordinator.This was detected during investigation of a flakiness on <tt>materialized_views_test.py:TestMaterializedViews.populate_mv_after_insert_wide_rows_test.ps: this is not an issue before 4.0, because a new table's <tt>TableMetadata was only registered after its <tt>ColumnFamilyStore object 
   <URL> (which appears to be changed by 
   <URL>).",no,,[],,
CASSANDRA-14311,38,"Allow Token-Aware drivers for range scans.Currently, range scans are not token aware.This means that an extra hop is needed for most requests.Since range scans are usually data intensive, this causes significant extra traffic.Token awareness could be enabled by having the coordinator return the token for the next (still unread) row in the response, so the driver can select&nbsp;a next coordinator that owns this row.",no,,[],,
CASSANDRA-14402,39,"Remove StreamCoordinator.streamExecutor thread pool.<tt>StreamCoordinator.streamExecutor was previously introduced to initiate stream connections on a separate thread from the session invocation logic.With 
   <URL> streaming now uses non-blocking IO, and connection establishment is asynchronous via netty.Thus, the thread pool in <tt>StreamCoordinator is unneeded.",no,,[sk1_negative_necessary],,
CASSANDRA-1480,40,"CFMetaData.convertToThrift makes subcomparator_type empty string instead of null.As a result of 
   <URL> adding a CFMetaData.convertToThrift method, the values such as subcomparator_type are defaulted to empty string instead of null.That makes it so, for example, in ColumnFamilyRecordReader, in its RowIterator, the check for only null is insufficient.It also needs to check for a blank value.After a discussion about it in IRC, Jonathan said it was probably easier to just change the creation to give a null value instead of empty string.",no,,[tk6_iteration],,
CASSANDRA-1688,41,"Enhance cassandra-cli with more flexible querying and better data type support.In trying to use cassandra-cli, I've felt the need to have better support for non-String data types, and more flexibility in the types of queries possible.The attached patch is an attempt to address some of those issues.It enhances the GET command with a more flexible syntax, outlined below.The new syntax adds to and partially duplicates the current GET syntax, but is more verbose.Functionally it's a superset of the LIST command, but I haven't removed any functionality yet.I added support for the Thrift getSlice and getRangeSlices calls.Syntax overview: 
  getSlice examples: 
  <SOURCECODE> 
  getRangeSlices examples: 
  <SOURCECODE> 
  Pseudo-Antlr syntax 
  <SOURCECODE> 
  Questions: 
   
   Should I use a different keyword?Perhaps GET should be reserved for the simple bracket-based, single-key case and this functionality should use LIST or SELECT as a keyword.Should the syntax be more SQL-like?I started out doing that, but it seemed to me that the C* model is so different that mapping it to the SQL syntax was difficult.I haven't looked at Eric Evans' CQL work in any detail yet, but perhaps that is a better model.Additional work: 
   
   The KEYS and COLUMNS keywords should be added to the GET / WHERE syntax for getIndexedSlices.The LIST command should be deprecated or removed.The SET command should be enhanced to allow for non-string keys and column names.I've used a different model for processing the syntax tree in the code.If other people like it, it would make sense to convert the rest of CliClient to the same model.",no,,[],,
CASSANDRA-1730,42,"Fat clients are never removed.After a failed bootstrap, these lines repeat infinitely: 
   INFO 
   <ERROR></ERROR> 2010-11-11 01:58:32,708 Gossiper.java (line 406) FatClient /10.104.73.164 has been silent for 3600000 ms, removing from gossip INFO 
   <ERROR></ERROR> 2010-11-11 01:59:03,685 Gossiper.java (line 591) Node /10.104.73.164 is now part of the cluster 
  Changing the IP on the node but using the same token causes a conflict, requiring either a full cluster restart or changing the token.This is especially easy to run into in practice in a virtual environment such as ec2.",no,,[pf3_profiling],,
CASSANDRA-1899,43,"Failed to get columns from super column in cassandra-cli (0.7-rc2).I'm using cassandra 0.7.0-rc2.When I tried to get column contents in a super column of Super CF like below; 
  <SOURCECODE> 
  the client reply: supercolumn parameter is not optional for super CF user",no,,[],,
CASSANDRA-1906,44,"Sanitize configuration code.Multipart: 
  <ul class=""alternate"" type=""square""> 
   Drop deprecated YAML config.Only config allowed is via thrift/JMX.Make this gratuitously easy to do with sane defaults and accepting changesets as opposed to full definitions.Combine common code between KS/CF/ColumnDefs and between thrift/avro defs.Provide an obvious and clean interface for changing settings locally versus globally (JMX vs. thrift).Dox here.",no,,[],,
CASSANDRA-2020,45,"stress.java performance falls off heavily towards the end.This is due to threads completing towards the end, such that there aren't enough to fully stress the cluster.The main problem here is that stress.java is a straight port of stress.py, where each thread runs through some range until it's done, and the threads finish at different times (probably offset by jvm warmup time.)Instead, a producer/consumer model would work better.",no,,[gk4_performance],,
CASSANDRA-2115,46,"Keep endpoint state until aVeryLongTime.In 
   <URL> we changed the gossiper so it holds onto endpoint state until QUARANTINE_DELAY has elapsed.However, if node X is leaving and node Y goes down and stays down longer than QUARANTINE_DELAY after X has left, Y will return thinking X is still a member of the cluster.Instead, let's hold onto the endpoint state even longer, until aVeryLongTime which is currently set to 3 days.",no,,[pf2_duration],,
CASSANDRA-2168,47,"SSTable2Json tool returns a different key when a querying for a specific key in an SSTable that does not exist.bin/sstable2json storage/core/data/Foo/BAR-1-Data.db -k NonExistantKey 
  returns { ""ExistantKey"" }",no,,[],,
CASSANDRA-2232,48,"Clean up and document EstimatedHistogram.EstimatedHistogram treats adding value n as adding a value infinitesimally greater than n.This barely made sense for the original goal of latency tracking but is clearly broken for inherently integral data like sstables-per-read.Also, median() is broken, but even a non-broken median() would not be correct to use in mean row size reporting which is its only caller.",no,,[gk5_latency],,
CASSANDRA-2658,49,"Pig + CassandraStorage should work when trying to cast data after it's loaded.We currently do a lot with pig + cassandra, but one thing I've found is that currently it's very touchy with data that comes from Cassandra for some reason.For example, if I try to a SUM of data that has not been validated as an LongType in Cassandra, it borks.See this schema script for Cassandra - 
   <URL> - and remove the validation on the num_heads data type and try to SUM that over the data and it gives data type errors.(It breaks with the num_heads validation removed and with or without the default_validation class being set.)We currently do analysis over data that is either just String (UTF8) data or that we have validated, so it works for us.However, I've seen a couple of people trying to use Cassandra with Pig that have had issues because of this.One of the tenets of pig is that it will eat anything and it kind of goes against this if the load/store somehow interferes with that.So in essence, I think this is a big deal for those wanting to use pig with cassandra in the ways that pig is normally used.",no,,[],,
CASSANDRA-2793,50,"SSTable ""Corrupt (negative) value length encountered"" exception blocks compaction..A node was consistently experiencing high CPU load.Examination of the logs showed that compaction of an sstable was failing with an error: 
   INFO 
   <ERROR>",no,,"[ss8_load_nn, sk2_block_hang_crash]",,
CASSANDRA-2849,51,"InvalidRequestException when validating column data includes entire column value.If the column value fails to validate, then ThriftValidation.validateColumnData() calls bytesToHex() on the entire column value and puts this string in the Exception.Since the value may be up to 2GB, this is potentially a lot of extra memory.The value is likely to be logged (and presumably returned to the thrift client over the network?).This could cause a lot of slowdown or an unnecessary OOM crash, and is unlikely to be useful (the client has access to the full value anyway if required for debugging).Also, the reason for the exception (extracted from the MarshalException) is printed <em>after</em> the data, so if there's any truncation in the logging system at any point, the reason will be lost.The reason should be displayed before the column value, and the column value should be truncated in the Exception message.",yes,This could cause a lot of slowdown or an unnecessary OOM crash,"[ss4_memory, tk3_memory_unit, sk1_negative_necessary, gk7_speed]",,
CASSANDRA-2860,52,"Versioning works *too* well.The scenario goes something like this: you upgrade from 0.7 to 0.8, but all the nodes remember that the remote side is 0.7, so they in turn speak 0.7, causing the local node to also think the remote is 0.7, even though both are really 0.8.",no,,[ss2_all_to_one],,
CASSANDRA-2914,53,"Simplify HH to always store hints on the coordinator.Moved from 
   <URL>: 
  Since we're storing the full mutation post-2045, there's no benefit to be gained from storing the hint on the replica node, only an increase in complexity.Let's switch it to always store hints on the coordinator instead.",no,,[gk2_complexity],,
CASSANDRA-2949,54,"Batch mutation of counters in multiple supercolumns throws an exception during replication..Steps to reproduce: 
   
   Perform a batch mutation of more than one counter in more than one super-column in the same column-family.The following exception is thrown during replication: 
  DEBUG 
   <ERROR>",no,,[],,
CASSANDRA-3037,55,"Could not get input splits Caused by: java.io.IOException: failed connecting to all endpoints slave1/123.198.69.242.After upgrade of cassandra from 0.8.2 to 0.8.4, got this error, before upgrade everything was working fine I have restarted cassandra, removed data, etc. nothing helps  I have 6 identical machines in the cloud, before it was working fine.If I make netstat then it shows port 9160 listening nodetool ... ring - responces with 6 machines UP.Finally I have truncated all 6 servers and reinstalled hadoop + cassandra 0.8.4 from scratch.compiled and tried to execute word_cound receive the same error as after upgrade 
  Error: 
  11/08/15 15:23:54 INFO WordCount: output reducer type: cassandra 11/08/15 15:23:54 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId= Exception in thread ""main"" java.io.IOException: Could not get input splits at org.apache.cassandra.hadoop.ColumnFamilyInputFormat.getSplits(ColumnFamilyInputFormat.java:157) at org.apache.hadoop.mapred.JobClient.writeNewSplits(JobClient.java:885) at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:779) at org.apache.hadoop.mapreduce.Job.submit(Job.java:432) at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:447) at WordCount.run(Unknown Source) at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65) at WordCount.main(Unknown Source) Caused by: java.util.concurrent.ExecutionException: java.io.IOException: failed connecting to all endpoints slave1/154.198.69.242 at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222) at java.util.concurrent.FutureTask.get(FutureTask.java:83) at org.apache.cassandra.hadoop.ColumnFamilyInputFormat.getSplits(ColumnFamilyInputFormat.java:153) ... 7 more Caused by: java.io.IOException: failed connecting to all endpoints slave1/154.198.69.242 at org.apache.cassandra.hadoop.ColumnFamilyInputFormat.getSubSplits(ColumnFamilyInputFormat.java:234) at org.apache.cassandra.hadoop.ColumnFamilyInputFormat.access$200(ColumnFamilyInputFormat.java:70) at org.apache.cassandra.hadoop.ColumnFamilyInputFormat$SplitCallable.call(ColumnFamilyInputFormat.java:190) at org.apache.cassandra.hadoop.ColumnFamilyInputFormat$SplitCallable.call(ColumnFamilyInputFormat.java:175) at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303) at java.util.concurrent.FutureTask.run(FutureTask.java:138) at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) at java.lang.Thread.run(Thread.java:662)",no,,"[ss2_all_to_one, tk7_concurrent]",,
CASSANDRA-3103,56,"Slashes added before Node Names.Since 0.8.4, node names are now preceded by a '/'.This did not occur in 0.8.3 and I see no reference to the change in the ChangeLog.This is breaking the Ruby Cassandra Gem (we had to patch it) and I would imagine other systems.See a sample from output.log 
   INFO 13:20:58,105 Node /10.2.115.166 has restarted, now UP again INFO 13:20:58,106 InetAddress /10.2.115.166 is now UP INFO 13:20:58,107 Node /10.2.115.166 state jump to normal INFO 13:20:58,113 Node /10.34.141.11 has restarted, now UP again INFO 13:20:58,113 InetAddress /10.34.141.11 is now UP INFO 13:20:58,113 Node /10.34.141.11 state jump to normal INFO 13:20:58,114 Node /10.100.219.107 has restarted, now UP again INFO 13:20:58,115 InetAddress /10.100.219.107 is now UP INFO 13:20:58,115 Node /10.100.219.107 state jump to normal 
  Before it would just list the IP, not with the / in front.",no,,[],,
CASSANDRA-3117,57,"StorageServiceMBean is missing a getCompactionThroughputMbPerSec() method.Without a getter, you can assign a new value but not query the existing one (which is strange).",no,,[],,
CASSANDRA-3211,58,"Enhanced IP resolution for machines with multiple network interfaces .On unix machines that have multiple network interfaces whereby the IP associated with the split is not on the network interface associated with localhost, the getLocation method cannot find the proper IP and throws an exception ""no connection available"".I changed the implementation to use NetworkInterface instead of InetAddress using getLocalHost().This is more reliable.See the following references: 
   <URL> 
   <URL> 
   <URL>",no,,[],,
CASSANDRA-3409,59,"CFS reloading of the compaction strategy is done for every metadata update and is not thread safe.The reloading of the compaction strategy done during CFS.reload is not thread safe.In particular, this is a problem for leveled compactions.It could leads to some sstable not being added to the manifest and also breaks the 'only one leveledCompactionTask can run at any given time' assumption (which, at least without 
   <URL> can likely leads to blocking compactions completely).",no,,"[sk2_block_hang_crash, ss11_rework]",,
CASSANDRA-3619,60,Use a separate writer thread for the SSTableSimpleUnsortedWriter.Currently SSTableSimpleUnsortedWriter doesn't use any threading.This means that the thread using it is blocked while the buffered data is written on disk and that nothing is written on disk while data is added.,no,,[sk2_block_hang_crash],,
CASSANDRA-3742,61,"Allow to do a range slice with a limit on the number of column across all rows.The proposal is to add a new max_columns option to RangeSliceCommand so that as to bound the total amount of columns returned rather than just the number of keys.In that case, the SlicePredicate.count would thus be ignored.",no,,[tk2_bound],,
CASSANDRA-3970,62,(100% reproducible) JVM crash in streamingTransferTest on Windows.<SOURCECODE>,no,,[pf1_percentage],,
CASSANDRA-4021,63,"CFS.scrubDataDirectories tries to delete nonexistent orphans.The check only looks for a missing data file, then deletes all other components, however it's possible for the data file and another component to be missing, causing an error: 
  <SOURCECODE>",no,,[],,
CASSANDRA-4166,64,"BulkRecordWriter constructor crashes on wrong default parameter in conf.get() which is letter instead of number.I've pulled the newest code and my bulkloading started to crash on: 
  <SOURCECODE> 
  The problem is this line and default parameter of conf.get() which was accidentally set to ""O"" (letter O) instead of 0 (zero).<SOURCECODE>",no,,[],,
CASSANDRA-4170,65,"cql3 ALTER TABLE ALTER TYPE has no effect.running the following with cql3: 
  <SOURCECODE> 
  does not actually change the column type of bar.It does under cql2.Note that on the current cassandra-1.1.0 HEAD, this causes an NPE, fixed by 
   <URL>.But even with that applied, the ALTER shown here has no effect.",no,,[],,
CASSANDRA-4590,66,"""The system cannot find the path specified"" when creating hard link on Windows.When upgrading from Cassandra 1.0.5 to 1.1.3, we have a test case (uses embedded Cassandra) that started failing as shown below.Other than the upgrade, no changes were made to the code or config.I believe this MAY be related to the change made in CASSANDRA-3101.We verified that the file it is trying to create the hard link to does exist - so it is purely the creation of the link that is failing.Here is the basic failure: <ERROR>.",no,,[],,
CASSANDRA-4604,67,"to small stack size with oracle java 1.6.3x.The stack size specified is too small, Specify at least 160 kb Cannot create Java VM Service exit with a return value of 1 
  cassandra-env.sh shoud set Xss to 160 kB no matter of java version",no,,[tk3_memory_unit],,
CASSANDRA-4940,68,"Truncate doesn't clear row cache.Truncate doesn't clear the row cache.select * from &lt;table&gt; which skips the row cache returns no data, but selecting by key does.cqlsh:temp&gt; select v1..v3 from temp2 where k in (3,2,1); v1 | v2 | v3 ---<del><ins></ins></del>--<del></del>--- 16 | 17 | 18 12 | 13 | 14 8 | 9 | 10 
  cqlsh:temp&gt; truncate temp2; cqlsh:temp&gt; select v1..v3 from temp2 where k in (3,2,1); v1 | v2 | v3 ---<del><ins></ins></del>--<del></del>--- 16 | 17 | 18 12 | 13 | 14 8 | 9 | 10 
  cqlsh:temp&gt; select * from temp2; cqlsh:temp&gt; select v1..v3 from temp2 where k in (3,2,1); v1 | v2 | v3 ---<del><ins></ins></del>--<del></del>--- 16 | 17 | 18 12 | 13 | 14 8 | 9 | 10",no,,[sk10_skip],,
CASSANDRA-4981,69,"Error when starting a node with vnodes while counter-add operations underway.Start both nodes, start stress on one node like this: ""cassandra-stress --replication-factor=2 --operation=COUNTER_ADD"" 
  While that is running: On the other node, kill cassandra, wait for ""nodetool status"" to show the node as down, and restart cassandra.I sometimes have to kill and restart cassandra several times to get the problem to happen.I get this error several times in the log: 
  <SOURCECODE>",no,,[],,
CASSANDRA-5076,70,"Murmur3Partitioner#describeOwnership calculates ownership% wrong.When I issued 'nodetool status' on Murmur3-partitioned cluster I got the following output: 
  <SOURCECODE> 
  Notice that 'Owns' percentages add up to 200%.I think the problem is that Murmur3Partitioner#describeOwnership currently calculate ownership% based on 
   <ERROR></ERROR> range, but we have to consider about negative tokens.",no,,[pf1_percentage],,
CASSANDRA-5334,71,"Don't announce migrations to pre-1.2 nodes.I have a mixed version cluster consisting of two 1.1.9 nodes and one 1.2.2 node upgraded from 1.1.9.The upgrade works, and I don't see any end user problems, however I see this exception in the logs on the non-upgraded nodes: 
  <SOURCECODE> 
  Steps to reproduce: 
  <SOURCECODE> 
  edit ~/.ccm/1.1.9/cluster.conf and configure cassandra_dir to point to 1.2.2.Edit node1's cassandra.yaml to be 1.2 compliant.<SOURCECODE> 
  The cluster is now a mixed version, and works for user queries, but with the exception above.",no,,[],,
CASSANDRA-5375,72,"CQL3: IN clause on last key not working when schema includes set,list or map.This is an exception on the fix of 
   <URL> 
  Looks like any schema using map,list or set won't work with IN clauses on the last key (in this example c) 
  Schema: 
  <SOURCECODE> 
  Query: 
  <SOURCECODE> 
  This will lead to an assertion error on the nodes: 
  <SOURCECODE>",no,,[],,
CASSANDRA-565,73,"BufferedRandomAccessFile.read doesn't always do full reads.BufferedRandomAccessFile.read may read fewer bytes than required, even when EOF is not reached.This breaks commit log recovery, which assumes that when a read returns less than required, the EOF is reached.",no,,[sk3_byte],,
CASSANDRA-5801,74,"AE during validation compaction.While repairing with vnodes enabled: 
  <SOURCECODE>",no,,[],,
CASSANDRA-5900,75,"Setting bloom filter fp chance to 1.0 causes ClassCastExceptions.In 1.2, we introduced the ability to turn bloom filters off completely by setting fp chance to 1.0.It looks like there is a bug with this though.When it's set to 1.0 the following errors occur because AlwaysPresentFilter is not present in the switch statement here at 
   <URL>, and we default to Murmur3BloomFilter for an unknown type.Exception in thread ""main"" java.lang.ClassCastException: org.apache.cassandra.utils.AlwaysPresentFilter cannot be cast to org.apache.cassandra.utils.Murmur3BloomFilter at org.apache.cassandra.utils.FilterFactory.serializedSize(FilterFactory.java:91) at org.apache.cassandra.io.sstable.SSTableReader.getBloomFilterSerializedSize(SSTableReader.java:531) at org.apache.cassandra.metrics.ColumnFamilyMetrics$15.value(ColumnFamilyMetrics.java:273) at org.apache.cassandra.metrics.ColumnFamilyMetrics$15.value(ColumnFamilyMetrics.java:268) at org.apache.cassandra.db.ColumnFamilyStore.getBloomFilterDiskSpaceUsed(ColumnFamilyStore.java:1825)",no,,[],,
CASSANDRA-6023,76,"CAS should distinguish promised and accepted ballots.Currently, we only keep 1) the most recent promise we've made and 2) the last update we've accepted.But we don't keep the ballot at which that last update was accepted.And because a node always promise to newer ballot, this means an already committed update can be replayed even after another update has been committed.Re-committing a value is fine, but only as long as we've not start a new round yet.Concretely, we can have the following case (with 3 nodes A, B and C) with the current implementation: 
   
   A proposer P1 prepare and propose a value X at ballot t1.It is accepted by all nodes.A proposer P2 propose at t2 (wanting to commit a new value Y).If say A and B receive the commit of P1 before the propose of P2 but C receives those in the reverse order, we'll current have the following states: 
    <SOURCECODE> Because C has received the t1 commit after promising t2, it won't have removed X during t1 commit (but note that the problem is not during commit, that example still stand if C never receive any commit message).Now, based on the promise of A and B, P2 will propose Y at t2 (C don't see this propose in particular, not before he promise on t3 below at least).A and B accepts, P2 will send a commit for Y.In the meantime a proposer P3 submit a prepare at t3 (for some other irrelevant value) which reaches C before it receives P2 propose&amp;commit.That prepare reaches A and B too, but after the P2 commit.At that point the state will be: 
    <SOURCECODE> In particular, C still has X as update because each time it got a commit, it has promised to a more recent ballot and thus skipped the delete.The value is still X because it has received the P2 propose after having promised t3 and has thus refused it.P3 gets back the promise of say C and A. Both response has t3 as in-progress ballot (and it is more recent than any mrc) but C comes with value X.So P3 will replay X. Assuming no more contention this replay will succeed and X will be committed at t3.At the end of that example, we've comitted X, Y and then X again, even though only P1 has ever proposed X. 
  I believe the correct fix is to keep the ballot of when an update is accepted (instead of using the most recent promised ballot).That way, in the example above, P3 would receive from C a promise on t3, but would know that X was accepted at t1.And so P3 would be able to ignore X since the mrc of A will tell him it's an obsolete value.",no,,"[sk10_skip, ss11_rework]",,
CASSANDRA-6306,77,"""No indexed columns"" error when using equals on compound primary key.Given the following table and secondary index: 
  <SOURCECODE> 
  I get the following error 
  <SOURCECODE> 
  I'm new to Cassandra, but of what I've read, this query should work.",no,,[],,
CASSANDRA-638,78,"Check SlicePredicate/ColumnParent Column versus SuperColumn consistency .This crops up in any call taking both a ColumnParent and a SlicePredicate, as some settings apply only to Columns while others apply only to SuperColumns.For example, it doesn't make sense to call setColumn_names on the SlicePredicate when the ColumnParent is a ColumnFamily that contains SuperColumns, as the return values will be SuperColumns, not Columns.The resulting error is currently difficult to decipher (16 byte UUID required).Similarly, I'm not sure what happens if you don't call setColumn_names on the SlicePredicate in the case where you are querying a ColumnFamily with Columns, although I'm guessing it returns all column names.Just a quick check to make sure the SlicePredicate doesn't have column names set when the ColumnParent is a SuperColumnFamily, returning a more informative error if so, should do the trick.",no,,"[gk7_speed, sk3_byte]",,
CASSANDRA-6409,79,"gossip performance improvement at node startup.With large clusters (&gt; 500 nodes) and num_tokens &gt; 255 we sometimes see a node have trouble starting up.CPU usage for one thread is pegged.We see this concurrent with Gossip flaps on the node trying to learn the ring topology.Other nodes on the ring, that are already at steady state do not seem to suffer.It is the node joining the large ring that has trouble.",yes,performance improvement,"[gk4_performance, tk7_concurrent, ss5_cpu]",,
CASSANDRA-6566,80,"Differencer should not run in AntiEntropy Stage.The Differencing currently runs in AntiEntropy stage.When there are lot of ranges which do not match, it takes sometime to compute the diff in ranges.Also with increase in Merkle tree height it will take even more time in case of large diffs.This causes other things to get blocked behind this.Also no other repair messages can be processed.Example: If a node is doing differencing for a repair, and Validation compaction is done for another repair, it needs to block to send the tree over till Differencing is done.",no,,[sk2_block_hang_crash],,
CASSANDRA-6576,81,"StreamMessage.deserialize takes high cpu and does not seem to make progress.One of my machine seems to be stuck at streaming in the data.At node 10.97.135.32 htop 
  <SOURCECODE> 
  Jstack for the above ""busy"" threads: 
  <SOURCECODE> 
  After several hours, these above stacks look almost exactly the same, except some thread scheduling.<SOURCECODE> 
  If you login into the node that it is talking to, say 10.122.50.31 
  <SOURCECODE> 
  It is NOT talking to 10.97.135.32, which makes me suspect the 10.97.135.32 is somehow in a ""busy-wait"" state.Thoughts?",no,,[pf2_duration],,
CASSANDRA-6578,82,"Slower shutdown on trunk than previous versions.Trunk takes a good bit longer to shudown than 2.0.A stack trace reveals it's hanging for a bit here: 
  <SOURCECODE>",yes,Slower shutdown on trunk than previous versions.,"[sk2_block_hang_crash, gk7_speed]",,
CASSANDRA-6656,83,"Exception logging.Reporting a few cases where informative exceptions can be silently swallowed.Attaching a proposed patch.========================= Case 1 Line: 95, File: ""org/apache/cassandra/utils/Hex.java"" 
  An actual failure in the underlying constructor will be lost.Propose to log it.<SOURCECODE> 
  ========================================== ========================= Case 2 Line: 192, File: ""org/apache/cassandra/db/marshal/DynamicCompositeType.java"" 
  The actual cause of comparator error can be lost as it can fail in multiple locations.<SOURCECODE> 
  Propose to log the exception.========================================== ========================= Case 3 Line: 239, File: ""org/apache/cassandra/tools/NodeProbe.java"" Exception ignored in finally.Propose log them with debug or trace.<SOURCECODE> 
  Similar case is at line 264 in the same file.==========================================",no,,[],,
CASSANDRA-6799,84,"schema_version of newly bootstrapped nodes disagrees with existing nodes.After bootstrapping new nodes 172.18.33.23 and 172.18.33.24 last weekend, I noticed that they have a different schema_version to the existing nodes.The existing nodes have all been around for a while, saw some schema changes in the past (eg: timeuuid -&gt; timestamp on a column family) but none recently, and were originally running 1.2 (they were upgraded to 2.0.5).Here you see the different schema version 0d9173d5-3947-328e-a14d-ce05239f61e0 for the two nodes: 
  cqlsh&gt; select peer, data_center, host_id, preferred_ip, rack, release_version, rpc_address, schema_version from system.peers; 
   peer | data_center | host_id | preferred_ip | rack | release_version | rpc_address | schema_version ---------------<del><ins></ins></del>-----------<del></del>------------------------------------<del><ins></ins></del>------------<del></del>----<del><ins></ins></del>---------------<del></del>--------------<del>+</del>------------------------------------- 192.168.21.12 | rdm | 55e4b4b6-2e64-4542-87a4-d8a8e28b5135 | null | RAC1 | 2.0.5 | 192.168.21.12 | f673ced0-8cfd-3d69-baba-4f81dc60c5b5 172.18.33.24 | ldn | 6e634206-94b6-4dcf-9cf8-72bfe190feee | null | RAC1 | 2.0.5 | 172.18.33.24 | 0d9173d5-3947-328e-a14d-ce05239f61e0 172.18.33.22 | ldn | 75c9c81f-b00b-4335-8483-fb7f1bc0be1e | null | RAC1 | 2.0.5 | 172.18.33.22 | f673ced0-8cfd-3d69-baba-4f81dc60c5b5 192.168.60.136 | adm | c83d403f-ef0d-4c54-a844-d69730fa54d3 | null | RAC1 | 2.0.5 | 192.168.60.136 | f673ced0-8cfd-3d69-baba-4f81dc60c5b5 192.168.60.137 | adm | b12e6d71-e189-4fe8-b00a-8ff2cc9848fd | null | RAC1 | 2.0.5 | 192.168.60.137 | f673ced0-8cfd-3d69-baba-4f81dc60c5b5 192.168.21.11 | rdm | dd2e69cb-232f-4236-89f2-b5479669d9f7 | null | RAC1 | 2.0.5 | 192.168.21.11 | f673ced0-8cfd-3d69-baba-4f81dc60c5b5 172.18.33.21 | ldn | 6942404c-e512-46b4-977a-243defa48d0f | null | RAC1 | 2.0.5 | 172.18.33.21 | f673ced0-8cfd-3d69-baba-4f81dc60c5b5 192.168.60.138 | adm | a229bc0f-201b-479e-8312-66891f37ca85 | null | RAC1 | 2.0.5 | 192.168.60.138 | f673ced0-8cfd-3d69-baba-4f81dc60c5b5 192.168.60.134 | adm | 7b860a54-59ea-4a92-9b47-44b52793cc70 | null | RAC1 | 2.0.5 | 192.168.60.134 | f673ced0-8cfd-3d69-baba-4f81dc60c5b5 172.18.33.23 | ldn | a08bad62-55bb-492b-be64-7cf5d5073d6d | null | RAC1 | 2.0.5 | 172.18.33.23 | 0d9173d5-3947-328e-a14d-ce05239f61e0 192.168.60.130 | adm | 3498b4b8-1047-4b42-b13b-bf27b3aa3177 | null | RAC1 | 2.0.5 | 192.168.60.130 | f673ced0-8cfd-3d69-baba-4f81dc60c5b5 192.168.60.133 | adm | 21d3faad-5c5d-447e-bab4-ad9323bdf4c1 | null | RAC1 | 2.0.5 | 192.168.60.133 | f673ced0-8cfd-3d69-baba-4f81dc60c5b5 192.168.60.135 | adm | 860ff4bb-4fcf-43ba-b270-f1844bdd3e65 | null | RAC1 | 2.0.5 | 192.168.60.135 | f673ced0-8cfd-3d69-baba-4f81dc60c5b5 192.168.60.131 | adm | d8b7b0b2-d697-43ae-ad6e-982b24637865 | null | RAC1 | 2.0.5 | 192.168.60.131 | f673ced0-8cfd-3d69-baba-4f81dc60c5b5 
  (14 rows) 
  I've attached the Cassandra log showing the 172.18.33.23 node bootstrapping.",no,,[pf2_duration],,
CASSANDRA-6819,85,"The command ""nodetool setcompactionthroughput"" sometimes takes a long time to take effect.It's often necessary to throttle a large compaction.When the nodetool setcompactionthroughput command is issued, the setting doesn't seem to take hold until another compaction starts, which may be some time on a bungled node.This command should take hold immediately.",yes,sometimes takes a long time to take effect,"[ss12_spend_time, sk18_throttle]",,
CASSANDRA-6837,86,"Batch CAS does not support LOCAL_SERIAL.The batch CAS feature introduced in Cassandra 2.0.6 does not support the LOCAL_SERIAL consistency level, and always uses SERIAL.Create a cluster with 4 nodes with the following topology: 
  <SOURCECODE> 
  In cqlsh: 
  <SOURCECODE> 
  Kill nodes 127.0.0.3 and 127.0.0.4: 
  <SOURCECODE> 
  Connect to 127.0.0.1 in DC1 and run a CAS batch at CL.LOCAL_SERIAL+LOCAL_QUORUM: 
  <SOURCECODE> 
  The batch fails with: 
  <SOURCECODE>",no,,[],,
CASSANDRA-6915,87,"Show storage rows in cqlsh.In Cassandra it's super important to understand how your CQL schema translates to the underlying storage rows.Right now the only way to see this is to create the schema in cqlsh, write some data, then query it using the CLI.Obviously we don't want to be encouraging people to use the CLI when it's supposed to be deprecated.So I'd like to see a function in cqlsh to do this.",no,,[],,
CASSANDRA-6964,88,"error in logs: ByteBuf.release() was not called before it's garbage-collected.Running some of our paging tests against 2.1 several of these exceptions are triggered: 
  <SOURCECODE> 
  These tests are run through jython with the java driver, so there's a little bit of setup needed (if you have ccm and dtest you are most of the way there): 1.clone and set up 
   <URL> .You may need to install ivy and copy ivy.jar to ~/.ant/lib/ivy.jar 2.you should have ccm, and CASSANDRA_DIR should be set in your environment 3.from the root of cassandra-dtest-jython run the tests with 'ant run_tests'.The tests take about 10 minutes run completely.4if you don't want to wait for the entire test suite to run, change the bottom of paging_test.py to just run a single test like so: 
  <SOURCECODE>",no,,[pf2_duration],,
CASSANDRA-7197,89,"Dead code in trunk.I did some code analysis, and as a byproduct was able to identify some dead code in the form of files that can be safely removed.After filtering out some false positives, this is what remained: 
  gms/IFailureNotification.java 
  <ul class=""alternate"" type=""square""> 
   was there since FB open-sourced Cassandra; has never been used for anything.No classes implement the interface, and it's not mentioned anywhere in the codebase.service/PendingRangeCalculatorServiceMBean.java  
  <ul class=""alternate"" type=""square""> 
   empty MBean used as a base class for PendingRangeCalculatorService, but has not been touched since being introduced several months ago.NOTE: removing this will require editing PendingRangeCalculatorService to not derive from this anymore.db/ColumnFamilyNotDefinedException.java 
  <ul class=""alternate"" type=""square""> 
   used to be thrown in original FB Cassandra; no longer used anywhere.db/context/IContext.java 
  <ul class=""alternate"" type=""square""> 
   introduced in 2c4ac98c9ffa8ea52da801830c7cdb745ddc28f0 (
    <URL>); was used extensively then, but no longer used anywhere.db/columniterator/SimpleAbstractColumnIterator.java 
  <ul class=""alternate"" type=""square""> 
   introduced in 48093358fb9022947592813a6aae43db148847ca (
    <URL>); was used then; no longer used anywhere.thrift/RequestType.java 
  <ul class=""alternate"" type=""square""> 
   enum introduced in 72199e23ec9d604449bef87733a32e1da9924437 (
    <URL>); was used then; no longer used anywhere.utils/AtomicLongArrayUpdater.java 
  <ul class=""alternate"" type=""square""> 
   introduced in 22e18f5a348a911f89deed9f9984950de451d28a (
    <URL>), but has never been used for anything.Not sure what the original intent might have been.utils/DefaultDouble.java 
  <ul class=""alternate"" type=""square""> 
   introduced in 96588d4f322dfbb1f5ff9328afe4377babfb1d2c (
    <URL>); was used then; no longer used anywhere.utils/LatencyTracker.java 
  <ul class=""alternate"" type=""square""> 
   introduced in 979a022f896aaa5a799b27a973cd476e5727820e (
    <URL>); was used then; no longer used anywhere.utils/SkipNullRepresenter.java 
  <ul class=""alternate"" type=""square""> 
   introduced in a6777492280ae481392cd4cb4ba613923f84989d(
    <URL>) was used then; no longer used anywhere.",no,,[tk6_iteration],,
CASSANDRA-7220,90,"Nodes hang with 100% CPU load.I've ran a test that both reads and writes rows.After some time, all writes succeeded and all reads stopped.Two of the four nodes have 16 of 16 threads of the ""ReadStage"" thread pool running.The number of pending task continuouly grows on these nodes.I have attached outputs of the stack traces and some diagnostic output from ""nodetool tpstats"" ""nodetool status"" shows all nodes as UN.I had run that test previously without any issues in with the same configuration.Some ""specials"" from cassandra.yaml: 
  <ul class=""alternate"" type=""square""> 
   key_cache_size_in_mb: 1024 
   row_cache_size_in_mb: 8192 
   
  The nodes running at 100% CPU are ""node2"" and ""node3"".node1&amp;node4 are fine.I'm not sure if it is reproducable - but it's definitly not a good behaviour.",yes,Nodes hang with 100% CPU load.,"[ss8_load_nn, pf1_percentage, sk2_block_hang_crash, tk3_memory_unit]",,
CASSANDRA-7320,91,"Swap local and global default read repair chances.See the discussion in 
   <URL> on why.Sylvain: ""instead of having read_repair_chance=0.1 and dclocal_read_repair_chance=0, to switch to read_repair_chance=0 and dclocal_read_repair_chance=0.1.If you have only one DC, then it won't change from the current default, and if you have multiple-DC, I can agree that not crossing DC boundaries for read repair is a better default.""Not basing on 1.2 because it's rather late for that.Not too late for the 2.0 cycle though, so setting fixver to 2.0.9.",no,,[],,
CASSANDRA-7329,92,"repair sessions not cleared out of nodetool netstats output on completion.After a repair completion or failure, messages like the following show in nodetool netstats: 
  Repair 0514e710-e83a-11e3-9ff2-0195b332f618 /192.168.1.31 Repair c67d7bb0-e835-11e3-9ff2-0195b332f618 /192.168.1.31 Repair c654d320-e837-11e3-9ff2-0195b332f618 /192.168.1.31 Repair f593f7c0-e836-11e3-9ff2-0195b332f618 /192.168.1.31",no,,[],,
CASSANDRA-7341,93,Emit metrics related to CAS/Paxos.We can emit metrics based on Paxos.One of them is when there is contention.I will add more metric in this JIRA if it is helpful.,no,,[],,
CASSANDRA-7377,94,"Should be an option to fail startup if corrupt SSTable found.We had a server that crashed and when it came back, some SSTables were corrupted.Cassandra happily started, but we then realised the corrupt SSTable contained some tombstones and a few keys were resurrected.This means corruption on a single replica can bring back data even if you run repairs at least every gc_grace.There should be an option, probably controlled by the disk failure policy, to catch this and stop node startup.",no,,[ss11_rework],,
CASSANDRA-7676,95,"bin/cassandra should complain if $JAVA is empty or not an executable.If JAVA_HOME points to a non-existing directory, bin/cassandra just complains with <tt>
    <ERROR></ERROR> /Users/snazy/.ccm/test/node1/bin/cassandra: line 151: exec: : not found (via ccm).Patch checks if <tt>$JAVA is empty or does not point to an executable and prints appropriate error message.(Patch against trunk)",no,,[],,
CASSANDRA-7720,96,"Add a more consistent snapshot mechanism.We???????????? hit an interesting issue with snapshotting, which makes sense in hindsight, but presents an interesting challenge for consistent restores: 
   
   initiate snapshot 
   snapshotting flushes table A and takes the snapshot 
   insert into table A 
   insert into table B 
   snapshotting flushes table B and takes the snapshot 
   snapshot finishes 
   
  So what happens here is that we end up having a B, but NOT having an A, even though B was chronologically inserted after A.It makes sense when I think about what snapshot is doing, but I wonder if snapshots actually should get a little fancier to behave a little more like what I think most people would expect.What I think should happen is something along the lines of the following: 
  For each node: 
   
   pass a client timestamp in the snapshot call corresponding to ""now"" 
   snapshot the tables using the existing procedure 
   walk backwards through the linked snapshot sstables in that snapshot 
   if the earliest update in that sstable is after the client's timestamp, delete the sstable in the snapshot 
   if the earliest update in the sstable is before the client's timestamp, then look at the last update.Walk backwards through that sstable.if any updates fall after the timestamp, make a copy of that sstable in the snapshot folder only up to the point of the timestamp and then delete the original sstable in the snapshot (we need to copy because we're likely holding a shared hard linked sstable) 
   
  I think this would guarantee that you have a chronologically consistent view of your snapshot across all machines and columnfamilies within a given snapshot.",no,,[],,
CASSANDRA-7754,97,"FileNotFoundException in MemtableFlushWriter.Exception in cassandra logs, after upgrade to 2.1: 
  
   <ERROR></ERROR> ERROR o.a.c.service.CassandraDaemon - Exception in thread Thread
   <ERROR></ERROR> java.lang.RuntimeException: java.io.FileNotFoundException: /xxx/cassandra/data/system/batchlog-0290003c977e397cac3efdfdc01d626b/system-batchlog-tmp-ka-186-Index.db (No such file or directory) at org.apache.cassandra.io.util.SequentialWriter.&lt;init&gt;(SequentialWriter.java:75) ~
   <ERROR></ERROR> at org.apache.cassandra.io.util.SequentialWriter.open(SequentialWriter.java:104) ~
   <ERROR></ERROR> at org.apache.cassandra.io.util.SequentialWriter.open(SequentialWriter.java:99) ~
   <ERROR></ERROR> at org.apache.cassandra.io.sstable.SSTableWriter$IndexWriter.&lt;init&gt;(SSTableWriter.java:550) ~
   <ERROR></ERROR> at org.apache.cassandra.io.sstable.SSTableWriter.&lt;init&gt;(SSTableWriter.java:134) ~
   <ERROR></ERROR> at org.apache.cassandra.db.Memtable$FlushRunnable.createFlushWriter(Memtable.java:383) ~
   <ERROR></ERROR> at org.apache.cassandra.db.Memtable$FlushRunnable.writeSortedContents(Memtable.java:330) ~
   <ERROR></ERROR> at org.apache.cassandra.db.Memtable$FlushRunnable.runWith(Memtable.java:314) ~
   <ERROR></ERROR> at  org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48) ~
   <ERROR></ERROR> at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~
   <ERROR></ERROR> at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297) ~
   <ERROR></ERROR> at org.apache.cassandra.db.ColumnFamilyStore$Flush.run(ColumnFamilyStore.java:1054) ~
   <ERROR></ERROR> at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) ~
   <ERROR></ERROR> at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) ~
   <ERROR></ERROR> at java.lang.Thread.run(Thread.java:745) ~
   <ERROR></ERROR> Caused by: java.io.FileNotFoundException: /xxx/cassandra/data/system/batchlog-0290003c977e397cac3efdfdc01d626b/system-batchlog-tmp-ka-186-Index.db (No such file or directory) at java.io.RandomAccessFile.open(Native Method) ~
   <ERROR></ERROR> at java.io.RandomAccessFile.&lt;init&gt;(RandomAccessFile.java:241) ~
   <ERROR></ERROR> at org.apache.cassandra.io.util.SequentialWriter.&lt;init&gt;(SequentialWriter.java:71) ~
   <ERROR></ERROR> ... 14 common frames omitted",no,,[tk7_concurrent],,
CASSANDRA-7876,98,"Errors in locally handled range slices do not propagate correctly.Until 
   <URL> is fixed, the attached repro.py demonstrates the problem.The client does not receive an error response from the server, leading to a client-side timeout.",no,,[sk8_timeout_expiration],,
CASSANDRA-7898,99,"IndexOutOfBoundsException comparing incompatible reversed types in DynamicCompositeType.In the following setup, an IndexOutOfBoundsException is often observed: 
   
   A DynamicCompositeType value is used in a column name (e.g. in a CQL3 cluster key) 
   Two incompatible types (e.g. Int32Type and DoubleType) are used in the dynamic composite value in their reversed form (e.g. ReversedType(Int32Type) and ReversedType(DoubleType) 
   Values for the incompatible types are inserted 
   One of various scenarios occurs that trigger comparison of the column names 
   
  The exception can be variously observed (sometimes not immediately) during query execution, memtable flushing, commit log replay etc.In some cases this can prevent Cassandra server startup (e.g. during commit log replay).Typical stack traces follow: 
  <SOURCECODE> 
  <SOURCECODE> 
  <SOURCECODE>",no,,[],,
CASSANDRA-7903,100,"tombstone created upon insert of new row.Tombstone is created upon insert of new row, depending on how the row is inserted.Simple way to observe this behavior: 
  Using cqlsh: 
  CREATE TABLE users1 ( userid text PRIMARY KEY, first_name text, last_name text); 
  insert into users1 (userid, first_name) values ('a','a'); 
  tracing on; 
  select * from users; 
  Trace results show 1 live cell and 0 tombstone cells created as a result: 
   userid | first_name | last_name -------<del><ins></ins></del>----------<del></del>---------- a | a | null 
  (1 rows) 
  ???????????? Read 1 live and 0 tombstoned cells | 00:31:31,487 | 10.240.203.201 | 1275 Scanned 1 rows and matched 1 | 00:31:31,487 | 10.240.203.201 | 1328 ???????????? 
  Now, 
  insert into users1 (userid, first_name,last_name) values ('b','b',null); 
  select * from users; 
  Trace results show 1 live cell and 1 tombstone cell created as a result: 
   userid | first_name | last_name -------<del><ins></ins></del>----------<del></del>---------- a | a | null b | b | null 
  (2 rows) 
  ???????????? Read 1 live and 0 tombstoned cells | 00:35:09,357 | 10.240.203.201 | 1243 Read 1 live and 1 tombstoned cells | 00:35:09,357 | 10.240.203.201 | 1383 Scanned 2 rows and matched 2 | 00:35:09,357 | 10.240.203.201 | 1438 ???????????? 
  Tombstone is not expected to be created in either case.",no,,[],,
CASSANDRA-793,101,"64MB transferTo chunk size is too large for win32.As reported by ruslan usifov on cassandra-user, he sees exceptions like 
<ERROR>.this appears to translate to ""insufficient buffer space.""He also reports that reducing CHUNK_SIZE to 32MB fixes the problem.",no,,"[tk3_memory_unit, gk3_reduce]",,
CASSANDRA-7985,102,stress tool doesn't support auth.stress tool in 2.1 doesn't seem to support username / password authentication (like cqlsh).,no,,[],,
CASSANDRA-8164,103,"OOM due to slow memory meter.Memory meter holds strong reference to memtable while it iterates over memtable cells.Because meter is not fast, it prevents memtable from being GCed after it has been flushed for some time.If write rate is fast enough, this makes node OOM.Fixed this by aborting metering if table becomes not active in datatracker, i.e. flushing or flushed.",yes,OOM due to slow memory meter.,"[ss4_memory, gk7_speed, ss18_fast_to, tk6_iteration]",,
CASSANDRA-8207,104,I have a 5 node cassandra cluster and i commissioned 1 new node to the cluster.when i added 1 node.it received streams from 3 nodes out of which 2 were completed successfully and one stream got failed.how can i resume the stream which has failed?.,no,,[],,
CASSANDRA-8274,105,"Node fails to rejoin cluster on EC2 if private IP is changed.Nodes in Amazon AWS EC2 Classic (not a VPC) may be assigned a new private IP if the node is stopped and then started again.In this case we have puppet update the configured listen_address to the new private IP.However, once the cassandra service starts, it is unable to communicate with the existing nodes(single region) and vice versa.'nodetool status' shows that each node believes that it is 'UN' and the other node is 'DN'.'nodetool gossipinfo' on the node that remained running shows the old private IP listed as the 'INTERNAL_IP' of the node that was stopped and restarted.The situation is resolved by restarting the cassandra service on the node that remained running.Once it has restarted, the INTERNAL_IP is correctly updated to the new private IP.'nodetool status' shows that both nodes are up and the cluster appears to function normally.This appears to me to be the root cause of 
   <URL>.<del>Possibly 
    <URL> as well, but I am not convinced they are actually duplicates.</del>",no,,[],,
CASSANDRA-8341,106,Expose time spent in each thread pool.Can increment a counter with time spent in each queue.This can provide context on how much time is spent percentage wise in each stage.Additionally can be used with littles law in future if ever want to try to tune the size of the pools.,no,,[],,
CASSANDRA-8447,107,"Nodes stuck in CMS GC cycle with very little traffic when compaction is enabled.Behavior - If autocompaction is enabled, nodes will become unresponsive due to a full Old Gen heap which is not cleared during CMS GC.Test methodology - disabled autocompaction on 3 nodes, left autocompaction enabled on 1 node.Executed different Cassandra stress loads, using write only operations.Monitored visualvm and jconsole for heap pressure.Captured iostat and dstat for most tests.Captured heap dump from 50 thread load.Hints were disabled for testing on all nodes to alleviate GC noise due to hints backing up.Data load test through Cassandra stress - /usr/bin/cassandra-stress write n=1900000000 -rate threads=&lt;different threads tested&gt; -schema replication(factor=3) keyspace=""Keyspace1"" -node &lt;all nodes listed&gt; 
  Data load thread count and results: 
   
   1 thread - Still running but looks like the node can sustain this load (approx 500 writes per second per node) 
   5 threads - Nodes become unresponsive due to full Old Gen Heap.CMS measured in the 60 second range (approx 2k writes per second per node) 
   10 threads - Nodes become unresponsive due to full Old Gen Heap.CMS measured in the 60 second range 
   50 threads - Nodes become unresponsive due to full Old Gen Heap.CMS measured in the 60 second range (approx 10k writes per second per node) 
   100 threads - Nodes become unresponsive due to full Old Gen Heap.CMS measured in the 60 second range (approx 20k writes per second per node) 
   200 threads - Nodes become unresponsive due to full Old Gen Heap.CMS measured in the 60 second range (approx 25k writes per second per node) 
   
  Note - the observed behavior was the same for all tests except for the single threaded test.The single threaded test does not appear to show this behavior.Tested different GC and Linux OS settings with a focus on the 50 and 200 thread loads.JVM settings tested: 
  <ol> 
   default, out of the box, env-sh settings 
   10 G Max | 1 G New - default env-sh settings 
   10 G Max | 1 G New - default env-sh settings 
     
     JVM_OPTS=""$JVM_OPTS -XX:CMSInitiatingOccupancyFraction=50"" 
      
   20 G Max | 10 G New JVM_OPTS=""$JVM_OPTS -XX:+UseParNewGC"" JVM_OPTS=""$JVM_OPTS -XX:+UseConcMarkSweepGC"" JVM_OPTS=""$JVM_OPTS -XX:+CMSParallelRemarkEnabled"" JVM_OPTS=""$JVM_OPTS -XX:SurvivorRatio=8"" JVM_OPTS=""$JVM_OPTS -XX:MaxTenuringThreshold=8"" JVM_OPTS=""$JVM_OPTS -XX:CMSInitiatingOccupancyFraction=75"" JVM_OPTS=""$JVM_OPTS -XX:+UseCMSInitiatingOccupancyOnly"" JVM_OPTS=""$JVM_OPTS -XX:+UseTLAB"" JVM_OPTS=""$JVM_OPTS -XX:+CMSScavengeBeforeRemark"" JVM_OPTS=""$JVM_OPTS -XX:CMSMaxAbortablePrecleanTime=60000"" JVM_OPTS=""$JVM_OPTS -XX:CMSWaitDuration=30000"" JVM_OPTS=""$JVM_OPTS -XX:ParallelGCThreads=12"" JVM_OPTS=""$JVM_OPTS -XX:ConcGCThreads=12"" JVM_OPTS=""$JVM_OPTS -XX:+UnlockDiagnosticVMOptions"" JVM_OPTS=""$JVM_OPTS -XX:+UseGCTaskAffinity"" JVM_OPTS=""$JVM_OPTS -XX:+BindGCTaskThreadsToCPUs"" JVM_OPTS=""$JVM_OPTS -XX:ParGCCardsPerStrideChunk=32768"" JVM_OPTS=""$JVM_OPTS -XX:-UseBiasedLocking"" 
   20 G Max | 1 G New JVM_OPTS=""$JVM_OPTS -XX:+UseParNewGC"" JVM_OPTS=""$JVM_OPTS -XX:+UseConcMarkSweepGC"" JVM_OPTS=""$JVM_OPTS -XX:+CMSParallelRemarkEnabled"" JVM_OPTS=""$JVM_OPTS -XX:SurvivorRatio=8"" JVM_OPTS=""$JVM_OPTS -XX:MaxTenuringThreshold=8"" JVM_OPTS=""$JVM_OPTS -XX:CMSInitiatingOccupancyFraction=75"" JVM_OPTS=""$JVM_OPTS -XX:+UseCMSInitiatingOccupancyOnly"" JVM_OPTS=""$JVM_OPTS -XX:+UseTLAB"" JVM_OPTS=""$JVM_OPTS -XX:+CMSScavengeBeforeRemark"" JVM_OPTS=""$JVM_OPTS -XX:CMSMaxAbortablePrecleanTime=60000"" JVM_OPTS=""$JVM_OPTS -XX:CMSWaitDuration=30000"" JVM_OPTS=""$JVM_OPTS -XX:ParallelGCThreads=12"" JVM_OPTS=""$JVM_OPTS -XX:ConcGCThreads=12"" JVM_OPTS=""$JVM_OPTS -XX:+UnlockDiagnosticVMOptions"" JVM_OPTS=""$JVM_OPTS -XX:+UseGCTaskAffinity"" JVM_OPTS=""$JVM_OPTS -XX:+BindGCTaskThreadsToCPUs"" JVM_OPTS=""$JVM_OPTS -XX:ParGCCardsPerStrideChunk=32768"" JVM_OPTS=""$JVM_OPTS -XX:-UseBiasedLocking"" 
  </ol> 
  Linux OS settings tested: 
  <ol> 
   Disabled Transparent Huge Pages echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled echo never &gt; /sys/kernel/mm/transparent_hugepage/defrag 
   Enabled Huge Pages echo 21500000000 &gt; /proc/sys/kernel/shmmax (over 20GB for heap) echo 1536 &gt; /proc/sys/vm/nr_hugepages (20GB/2MB page size) 
   Disabled NUMA numa-off in /etc/grub.confdatastax 
   Verified all settings documented here were implemented 
    <URL> 
  </ol> 
  Attachments: 
  <ol> 
   .yaml 
   fio output - results.tar.gz 
   50 thread heap dump - 
    <URL> 
   100 thread - visual vm anonymous screenshot - visualvm_screenshot 
   dstat screen shot of with compaction - Node_with_compaction.png 
   dstat screen shot of without compaction ????? Node_without_compaction.png 
   gcinspector messages from system.log 
   gc.log output - gc.logs.tar.gz 
  </ol> 
  Observations: 
  <ol> 
   even though this is a spinning disk implementation, disk io looks good.output from Jshook perf monitor 
      <URL> is attached 
     note, we leveraged direct io for all tests by adding direct=1 to the .global config files 
      
   cpu usage is moderate until large GC events occur 
   once old gen heap fills up and cannot clean, memtable post flushers start to back up (show a lot pending) via tpstats 
   the node itself, i.e. ssh, is still responsive but the Cassandra instance becomes unresponsive 
   once old gen heap fills up Cassandra stress starts to throw CL ONE errors stating there aren't enough replicas to satisfy....heap dump from 50 thread, JVM scenario 1 is attached 
     
     appears to show a compaction thread consuming a lot of memory 
      
   sample system.log output for gc issues 
   strace -e futex -p $PID -f -c output during 100 thread load and during old gen ""filling"", just before full % time seconds usecs/call calls errors syscall 100.00 244.886766 4992 49052 7507 futex 100.00 244.886766 49052 7507 total 
   htop during full gc cycle - 
    <URL> 
   nothing is blocked via tpstats on these nodes 
   compaction does have pending tasks, upwards of 20, on the nodes 
   Nodes without compaction achieved approximately 20k writes per second per node without errors or drops 
  </ol> 
  Next Steps: 
  <ol> 
   Will try to create a flame graph and update load here - 
    <URL> 
   Will try to recreate in another environment 
  </ol>",no,,"[ss8_load_nn, ss1_one_per, tk13_huge_data_chunk, ss2_all_to_one, ss4_memory, pf1_percentage, tk3_memory_unit, sk2_block_hang_crash, ss6_per_per, pf2_duration, ss5_cpu, ss10_called_frequently]",,
CASSANDRA-8487,108,"system.schema_columns sometimes missing for 'system' keyspace.Occasionally a Cassandra node will have missing schema_columns information where keyspace_name='system'.<SOURCECODE> 
  All keyspace and column family schema info is present for 'system' ????? it's only the column information missing.This can occur on an existing cluster following node restart.The data usually appears again after bouncing the node.This is impactful to client drivers that expect column meta for configured tables.Reproducible in 2.1.2.Have not seen it crop up in 2.0.11.",no,,[],,
CASSANDRA-8521,109,"RangeSliceQueryPager may fetch too much data in the first partition.As described in 
   <URL>, it looks like RangeSliceQueryPager may fetch more than it needs to in the first partition: 
  <blockquote> 
   when we actually query the underlying partition, the slice filter count might be a lot more than what we care for (it could be Integer.MAX_VALUE if there wasn't any LIMIT on the statement in the first place) and if that's the case, we will read a lot more than we should.This will be only true for the first partition, because after that we will update the SliceQueryFilter at the end of the loop of CFS.filter(), but still, it's potentially inefficient for that that first partition and might even end up blowing up the heap if the partition is big, which defeats the purpose of paging.I'll note that provided we don't blow up the heap then the resultSet returned to the user will be fine since we'll trim it in SelectStatement, but it's still a bug (provided I'm not missing something).</blockquote>",no,,"[gk1_efficiency, ss19_lot_reading, tk6_iteration]",,
CASSANDRA-8604,110,Batch Schema Changes at protocol level.It would be nice to have a way to send to a cluster several DDL queries in 1 request and get a response when the schema change has been applied on all the (live) nodes.This could be helpful for dev teams that are used to create schema changes db scripts and deploy them as part of a version change (ie: Rails migrations).Also it can avoid race conditions between schema changes and querying that schema (in test env or other deployments).,no,,[ss2_all_to_one],,
CASSANDRA-8641,111,"Repair causes a large number of tiny SSTables.I have a 3 nodes cluster with RF = 3, quad core and 32 GB or RAM.I am running 2.1.2 with all the default settings.I'm seeing some strange behaviors during incremental repair (under write load).Taking the example of one particular column family, before running an incremental repair, I have about 13 SSTables.After finishing the incremental repair, I have over 114000 SSTables.<SOURCECODE> 
  Looking at the logs during the repair, it seems Cassandra is struggling to compact minuscule memtables (often just a few kilobytes): 
  <SOURCECODE> 
  Here is an excerpt of the system logs showing the abnormal flushing: 
  <SOURCECODE> 
  At the end of the repair, the cluster has become unusable.",no,,"[ss8_load_nn, tk3_memory_unit]",,
CASSANDRA-8647,112,"Unify ARE#makeDataRequests() and ARE#makeDigestRequests().Two methods should be essentially the same, yet 
   <URL> gave madeDataRequests() some preferential treatment - confusing at least one person in the process.The attached patch cleans up and unifies both methods.Will not include in CHANGES.txt.",no,,[],,
CASSANDRA-8704,113,"IllegalStateException when running SSTableLoader.The sstableloader dtests are failing on 2.1-HEAD and trunk: 
   <URL> 
  The stack trace is as follows: 
  <SOURCECODE> 
  Git bisect points to #7705 as the first commit where the failures begin, though 7705 may have simply uncovered a bug, rather than caused it./cc 
   <URL>",no,,[],,
CASSANDRA-8708,114,"inter_dc_stream_throughput_outbound_megabits_per_sec to defaults to unlimited.inter_dc_stream_throughput_outbound_megabits_per_sec was introduced in 
   <URL>.There's some discussion in the ticket of the intention to link the default to whatever stream_throughput_outbound_megabits_per_sec is set to.However, it looks like it's just set to 0 - from /src/java/org/apache/cassandra/config/Config.java 
  This is a bit of a pain - usually folks want to set the inter dc limits lower than the base streaming figure.",no,,"[ss1_one_per, gk9_throughput, pf2_duration]",,
CASSANDRA-8810,115,"incorrect indexing of list collection.in a table with one indexed field type list&lt;text&gt; ,data retrieval is not working properly: 
  I have a simple table with an indexed list&lt;text&gt; field, but it shows unexpected behavior when I query the list.<SOURCECODE> 
  This is expected behavior: <SOURCECODE>.<ERROR>
  From the following query I expect a subset of the previous query result, but it returns no data --------------------------------------------------- select * from test where parts contains 'a' and parts contains 'b' ALLOW FILTERING; 
   whole | parts ------<del>+</del>------",no,,[],,
CASSANDRA-8829,116,"Add extra checks to catch SSTable ref counting bugs.There have been some bad affects from ref counting bugs (see e.g. 
   <URL>).We should add extra checks so we can more easily diagnose any future problems and avoid some of the side effects.",no,,[],,
CASSANDRA-8873,117,"Add PropertySeedProvider.Add a PropertySeedProvider that allows administrators to set a seed on the command line with -Dcassandra.seeds=127.0.0.1,127.0.0.2 instead of rewriting cassandra.yaml.It looks like the yaml parser expects there to always be a parameters: option on seeds, so unless we change it to be optional, there needs to be a dummy map or the yaml will not parse, e.g. 
  seed_provider: 
  <ul class=""alternate"" type=""square""> 
   class_name: org.apache.cassandra.locator.PropertySeedProvider parameters: 
   stub: ""this is required for the yaml parser and is ignored""",no,,[ss11_rework],,
CASSANDRA-8898,118,"sstableloader utility should allow loading of data from mounted filesystem.When trying to load data from a mounted filesystem onto a new cluster, following exceptions is observed intermittently, and at some point the sstableloader process gets hung without completing the loading process.Please note that in my case the scenario was loading the existing sstables from an existing cluster to a brand new cluster.Finally, it was found that there were some hard assumptions been made by sstableloader utility w.r.t response from the filesystem, which was not working with mounted filesystem.The work-around was to copy each existing nodes sstable data files locally and then point sstableloader to that local filesystem to then load data onto new cluster.In case of restoring during disaster recovery from backups the data using sstableloader, this copying to local filesystem of data files and then loading would take a long time.It would be a good enhancement of the sstableloader utility to enable use of mounted filesystem as copying data locally and then loading is time consuming.Below is the exception seen during the use of mounted filesystem.<SOURCECODE>",yes,then loading would take a long time,[ss12_spend_time],,
CASSANDRA-898,119,Add LoadBytes JMX Attribute (similar to LoadString) to help tracking node load.Cassandra JMX currently exposes org.apache.cassandra.service:type=StorageService attribute "LoadString" which contains node load as a string but this value is hard to use by monitoring processes.There's already public double getLoad() in StorageService class so it would just need to be exported to its MBean.Exposing the raw numeric value will ease node load tracking in monitoring systems like Zabbix.,no,,[ss8_load_nn],,
CASSANDRA-9117,120,"LEAK DETECTED during repair, startup.When running the <tt>incremental_repair_test.TestIncRepair.multiple_repair_test dtest, the following error logs show up: 
  <SOURCECODE> 
  The test is being run against trunk (commit <tt>1dff098e).I've attached a DEBUG-level log from the test run.",no,,[],,
CASSANDRA-9395,121,"Prohibit Counter type as part of the PK.C* let me do this: 
  <SOURCECODE> 
  and then treated a as an int!<SOURCECODE> 
  <SOURCECODE> 
  (should have given can't insert must update error) 
  Even though desc table still shows it as a counter type: 
  <SOURCECODE>",no,,[],,
CASSANDRA-9525,122,"Commitlog allocation failure doesn't stop the entire node..If there is an error opening a commit log segment, the thread dies, but it doesn't stop the whole node.Got the following on a node: 
  <SOURCECODE> 
  And the node stayed kind of up, didn't notice something wrong until the node died OOM because some threads were dead and others weren't.",no,,[ss4_memory],,
CASSANDRA-969,123,"Server fails to join cluster if IPv6 only.When configuring Cassandra for IPv6 connectivity on the server to server side the addition of a second node causes the both servers to loop on ArrayIndexOutOfBoundsExection for 5 minutes 
  The first server has  
  Caused by: java.lang.ArrayIndexOutOfBoundsException: 65536 at org.apache.cassandra.net.HeaderSerializer.deserialize(Header.java:155) 
  While the second has 
  Caused by: java.lang.ArrayIndexOutOfBoundsException: 131072 at org.apache.cassandra.net.HeaderSerializer.deserialize(Header.java:155) 
  the index is double.These servers work find in a cluster together if they are configured IPv4 
  server1 in the output is 2607:f3d0:0:2::16 server2 is 2607:f3d0:0:1::f",no,,"[pf2_duration, tk6_iteration]",,
CASSANDRA-9859,124,"IndexedReader updateBlock exception.While testing the Materialized View, an exception is thrown on create.<SOURCECODE>",no,,[],,
HIVE-10362,125,"Support Type check/conversion in dynamic partition column.There are quite a lot of issues associated with the non-noramlized or type-mismatched values for partition column.Hive has many ways to introduce such problematic data.<URL> mainly provides the support to type check/convert/normalize the partition column value in static partition specification.This JIRA tries to deal with the partition column type in dynamic partition insert.Currently any data can be inserted as a partition column value as long as it is quoted as a string.For example, create table dynparttypechecknum (key int, value string) partitioned by (part int); insert into dynparttypechecknum partition (part) select key, value, '00001' from src limit 1; show partitions dynparttypechecknum; ???????????? part=00001 
  The partition column value is non-normalized int 00001.It causes some unnecessary problems such as integer partition column JDO filter pushdown (see 
   <URL>) and others like 
   <URL>.",no,,[sk1_negative_necessary],,
HIVE-10630,126,"Renaming tables across encryption zones renames table even though the operation throws error.Create a table with data in an encrypted zone 1 and then rename it to encrypted zone 2.<SOURCECODE> 
  Even though the command errors out the table is renamed.I think the right behavior should be to not rename the table at all including the metadata.",no,,[],,
HIVE-10680,127,"Hive Cassandra throws  org.apache.cassandra.exceptions.ConfigurationException: Unable to find partitioner class 'org.apache.cassandra.dht.RandomPartitioner'.I am successfully able create keyspace and table in Cassandra using Hive.Select operation fails and throws error: 

   <ERROR>",no,,[],,
HIVE-11725,128,"Delete on ACID enabled table provides wrong results.This is based on the master codebase (last commit: 492c8b1d88ffcb68ba4f77a3a49ae8fc768cdd7c) 
  Created a table with TBLPROPERTIES(""transactional""=""true"", ""NO_AUTO_COMPACTION""=""true"").Populated bunch of records in the table and deleted few of them.Select query after delete does not seem to consider the deleted records.<SOURCECODE>",no,,[],,
HIVE-12444,129,"Global Limit optimization on ACID table without base directory may throw exception.Steps to reproduce: 
  set hive.fetch.task.conversion=minimal; set hive.limit.optimize.enable=true; 
  create table acidtest1( c_custkey int, c_name string, c_nationkey int, c_acctbal double) clustered by (c_nationkey) into 3 buckets stored as orc tblproperties(""transactional""=""true""); 
  insert into table acidtest1 select c_custkey, c_name, c_nationkey, c_acctbal from tpch_text_10.customer; 
  select cast (c_nationkey as string) from acidtest.acidtest1 limit 10; 
  <SOURCECODE>",no,,[],,
HIVE-12859,130,"MSCK Repair table gives error for higher number of partitions.Hi, For large number of partitions (specifically, when number of present directories is higher than 1.000.000) msck repair table gives the error from hive cli: ""FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask"" The exception is taken from also beeline cli: Getting log thread is interrupted, since query is done!Error: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask (state=08S01, code=1) java.sql.SQLException: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask",no,,[],,
HIVE-13471,131,"TinyLFU cache.Hive currently uses either the 
   <URL> or 
   <URL> caching algorithms.My understanding is that LRFU is O(lg n) and makes predictions based on the current working set.A more modern alternative is 
   <URL> which is O(1) by capturing the frequency in a sketch and recency through LRU queues.This allows it to 
   <URL> ARC and LIRS, two of the best policies to date.<URL> provides a concurrent implementation by using the write-ahead log approach to record &amp; replay updates (see 
   <URL>).It is currently being migrated to in 
   <URL> and 
   <URL> for their on-heap caches.If there is interest let me know if I can be of help.",yes,Time complexity,"[sk9_frequency, tk16_big_O, ss13_cache, tk7_concurrent]",,
HIVE-13761,132,Select data from no Encrypt table  where my cluste enable Encrypt .<ERROR>,no,,[],,
HIVE-13991,133,"Union All on view fail with no valid permission on underneath table.When sentry is enabled.create view V as select * from T; When the user has read permission on view V, but does not have read permission on table T, 
  select * from V union all select * from V  failed with: 
  <SOURCECODE>",no,,[],,
HIVE-14375,134,"hcatalog-pig-adaptor pom.xml uses joda-time 2.2 instead of ${joda.version} that uses 2.8.1.<URL> changed the joda-time dependency to 2.8.1 version.However, the hcatalog-pig-adapter has 2.2 hardcoded.",no,,[],,
HIVE-14667,135,"Eliminate OrcRecordUpdate.OPERATION.With 
   <URL>, for acid tables with 'transactional_properties'='default', it should be possible to eliminate the type of event property (at least on disk) since each type of delta file contains exactly 1 type events.Also, for insert deltas as well as base files, all rows have originalTxnId=currentTxnId.Not sure if it's worth the effort given RLE in ORC",no,,[],,
HIVE-14675,136,"Ensure directories are cleaned up on test cleanup in QTestUtil.Need to verify whether they are cleaned up or not.There's 4-5 different directories involved.If I'm not mistaken, they get cleaned up before each test invocation via mvn.",no,,[],,
HIVE-14740,137,"LLAP DynamicServiceInstasnce.isAlive does not seem to work as it should.Follow up from 
   <URL>.Is used in multiple places in LlapTaskScheduler.Critical to validate the broken functionality does not cause problems in the scheduler.",no,,[],,
HIVE-1475,138,".gitignore files being placed in test warehouse directories causing build failure.from last comments on 
   <URL>: 
  test warehouse is being polluted with .gitignore files.these need to be removed.",no,,[],,
HIVE-14865,139,"Fix comments after HIVE-14350.there are still some comments in the code that should've been updated in 
   <URL>",no,,[],,
HIVE-15851,140,CompactorMR.launchCompactionJob() should use JobClient.submitJob() not runJob.otherwise hadoop JobId of the job is not visible in SHOW COMPACTIONS until the job completes.,no,,[],,
HIVE-1641,141,"add map joined table to distributed cache.Currently, the mappers directly read the map-joined table from HDFS, which makes it difficult to scale.We end up getting lots of timeouts once the number of mappers are beyond a few thousand, due to  concurrent mappers.It would be good idea to put the mapped file into distributed cache and read from there instead.",no,,"[ss13_cache, sk8_timeout_expiration, tk7_concurrent]",,
HIVE-16591,142,"DR for function Binaries on HDFS .<ol> 
   We have to make sure that during incremental dump we dont allow functions to be copied if they have local filesystem ""file://"" resources.????? depends how much system side work we want to do, We are going to explicitly provide a caveat for replicating functions where in, only functions created ""using"" clause will be replicated and the ""using"" clause prohibits creating functions with the local ""file://"" resources and hence doing additional checks when doing repl dump might not be required.We have to make sure that during the bootstrap / incremental dump we append the namenode host + port if functions are created without the fully qualified location of uri on hdfs, not sure how this would play for S3 or WASB filesystem.We have to copy the binaries of a function resource list on CREATE / DROP FUNCTION .The change management file system has to keep a copy of the binary when a DROP function is called, to provide capability of updating binary definition for existing functions along with DR. An example of list of steps is given in doc (ReplicateFunctions.pdf ) attached in parent Issue.</ol>",no,,[],,
HIVE-17103,143,"HIVE-14251 severely impacts backward compatibility of UNION ALL queries.<URL> looks like it changed behavior of type conversion during UNION ALL queries, where types that are of different primitive groups (date, string, numeric, binary, boolean) cannot be unioned together with implicit conversions, and requires an explicit type cast to the desired type.That 
   <URL> breaks backward compatibility is spelled out in the release notes.However I'm wondering if this was the right fix - this has quite a big impact on the usability of UNION ALL.Some alternative suggestions: 1.Should we have simply not fixed 
   <URL>?While almost all DBMSs recommend users to use explicit casts, a lot of people just rely on implicit casting (if supported), and then do explicit casting if they want something other than the default behavior.This might have been one of the times the user would want to use explicit casts to override the default Hive behavior.2Try to specifically fix the case for 
   <URL> (union behavior of date, numeric).I was actually surprised union of date and numeric values worked, because when I originally did DATE type I tried to make sure DATE was not convertible to anything except string/timestamp.For example I don't think you can actually explicitly cast DATE to INT.We could try to make this particular implicit union conversion invalid (as well as any others we feel may not ).cc 
   <URL> 
   <URL> 
   <URL> 
   <URL> 
   <URL> 
   <URL> as these are the names I've seen on 
   <URL>, as well as related Jira 
   <URL>",no,,[],,
HIVE-18543,144,"Add print sessionid in console.Hive client log file already contains sessionid information, but the console does not have sessionid information, the user can not be associated with the log well.",no,,[],,
HIVE-18619,145,"Verification of temporary Micromanaged table atomicity is needed .Session based temporary table by 
   <URL> had no consideration of Micromanaged table  (MM) since there was no insert-only ACID table at its creation tije.<URL> addressed the issue of no writes during CTTAS (Create Temporary Table As Select) on Micro-Managed table.But atomicity of temporary MM table is not verified.",no,,[],,
HIVE-209,146,Enable assertions for unit tests.We should enable assertions while running unit tests.Many assertions currently fail while running tests.,no,,[],,
HIVE-2180,147,"Log message improvement and use StringBuilder for constructing messages instead of String.Need to properly format/present the error messages for better traceability.For example in classes like ScriptOperator, FetchOperator, PartitionPruner etc,  
  <SOURCECODE> 
  The two lines which are being logged separately can be clubbed using string builder and logged at once like  
  <SOURCECODE>",no,,[ss3_in_one_go],,
HIVE-2528,148,"use guava's LexicographicalComparator for Hive.ref: 
  
   <URL> 
   <URL>",no,,[],,
HIVE-3318,149,"LazySimpleSerDe should support more levels of nested structures.<URL> hard-codes the level of separators to 8, causing structures with more than that level of nestedness to fail.Doubling this value to 16 allowed me to read an extremely nested structure.This code was added in 
   <URL> and reading through the jira I can't see a reason 8 was chosen, other than it seemed like a reasonable value.Any reason not to double this, or allow the user to configure via a property?",no,,[],,
HIVE-3331,150,"plan for union all followed by mapjoin may be wrong.test query: 
  <SOURCECODE> 
  I got result: 
  <SOURCECODE> 
  what's wrong with Stage-1?In code (about line 362): 
  <SOURCECODE> 
  Does the ""else"" block forget to set ""setCurrMapJoinOp""?I hack the code as so, and get the plan what I want.plan after my modification: 
  <SOURCECODE>",no,,[sk2_block_hang_crash],,
HIVE-3336,151,"[Hive-Hbase Integration]:Data is not inserted into the hbase table from the hive table if the value is null even if the row key is not null.Scenario ------------ 1.create a new HBase table which is to be managed by Hive, use the STORED BY clause on CREATE TABLE as follows Query: 
  <SOURCECODE> 
  2.Create two tables in hive and load data into the table as follows table1: 
  <SOURCECODE> 
  table2: 
  <SOURCECODE> 
  3.Insert the join result of these tables into hbase_table_join as follows 
  <SOURCECODE> 
  Actual result of the select query is  ---------------  
  <SOURCECODE> 
  But the above insert query is failing because of the following exception  --------------------- 
  <SOURCECODE>",no,,[ss8_load_nn],,
HIVE-3469,152,"When new hive cluster is started, hive jdbc client hangs..The scenario is as follows :  For the new hive cluster , when multiple hive client is getting the connection for hive server , there are 3 problems in metastore , hive client.1Since the derby is booted at first connection, multiple client try to create database.One of the client succeed and another one throw exception because of unique primary constraint in metatables.2Once the Runtime exception is throw, the hive client which is made connection request will hang forever.3Potentially ""ConcurrentModificationException"" is thrown by JDOQuery.",yes,the hive client which is made connection request will hang forever.,"[sk11_infinity, sk2_block_hang_crash, tk7_concurrent]",,
HIVE-4395,153,Support TFetchOrientation.FIRST for HiveServer2 FetchResults.Currently HiveServer2 only support fetching next row (TFetchOrientation.NEXT).This ticket is to implement support for TFetchOrientation.FIRST that resets the fetch position at the begining of the resultset.,no,,[],,
HIVE-4977,154,HS2: support an alternate resultset serialization format between client and server.Current serialization protocol between client and server as defined in cli_service.thrift results in 2x (or more) throughput degradation compared to HS1.Initial proposal is to introduce HS1 serialization protocol as a negotiable alternative.,no,,[gk9_throughput],,
HIVE-5234,155,"partition name filtering uses suboptimal datastructures.Some DSes used in name-based partition filtering, as well as related methods, are suboptimal, which can cost 100-s of ms on large number of partitions.I noticed while perf testing 
   <URL>, but it can also be applied separately given that the patch over there will take some time to get in.",yes,Mentioned specific performance testing: can cost 100-s of ms on large number of partitions.I noticed while perf testing ,[pf3_profiling],,
HIVE-5520,156,"Use factory methods to instantiate HiveDecimal instead of constructors.Currently HiveDecimal class provided a bunch of constructors that unfortunately also throws a runtime exception.For example, 
  <SOURCECODE> 
  As a result, it's hard for the caller to detect error occurrences and the error handling is also complicated.In many cases, the error handling is omitted or missed.For instance, 
  <SOURCECODE> 
  Throwing runtime exception while expecting caller to catch seems anti-pattern.In the case of constructor, factory class or methods seem more appropriate.With such a change, the apis are cleaner, and the error handling is simplified.",no,,[gk2_complexity],,
HIVE-6070,157,"document HIVE-6052.See comments in 
   <URL> - this is the followup jira",no,,[],,
HIVE-6467,158,"metastore upgrade script 016-HIVE-6386.derby.sql uses char rather than varchar.Trying to tinker with the metastore upgrade scripts and did the following steps on a brand new Derby DB: 
  From derby: 
  <SOURCECODE> 
  From Hive: 
  <SOURCECODE> 
  I then hit the following error below.It appears that in the metastore DBS table, the row with defaultdb was created with the value ""ROLE "", with spaces at the end, where it was expecting ""ROLE"".<SOURCECODE>",no,,[],,
HIVE-7312,159,"CBO throws ArrayIndexOutOfBounds.Running tpcds query 17.Still confirming if col stats are available.When I turn CBO on (this is just the relevant snipped, the actual exception is pages long): 
  Caused by: java.lang.IndexOutOfBoundsException: Index: 24, Size: 0 at java.util.ArrayList.rangeCheck(ArrayList.java:635) at java.util.ArrayList.get(ArrayList.java:411) at org.apache.hadoop.hive.ql.optimizer.optiq.RelOptHiveTable.getColStat(RelOptHiveTable.java:97) at org.apache.hadoop.hive.ql.optimizer.optiq.reloperators.HiveTableScanRel.getColStat(HiveTableScanRel.java:73) at org.apache.hadoop.hive.ql.optimizer.optiq.stats.HiveRelMdDistinctRowCount.getDistinctRowCount(HiveRelMdDistinctRowCount.java:47) at org.apache.hadoop.hive.ql.optimizer.optiq.stats.HiveRelMdDistinctRowCount.getDistinctRowCount(HiveRelMdDistinctRowCount.java:36) ... 272 more",no,,[],,
HIVE-7642,160,"Set hive input format by configuration.[Spark Branch].Currently hive input format is hard coded as HiveInputFormat, we should set this parameter from configuration.",no,,[],,
HIVE-9071,161,"Enhance Vectorization to read non-ORC tables for better test coverage.By enabling Vectorization to read non-ORC tables, many more query unit tests will execute in vectorization mode.This is the 2nd step towards turning Vectorization on by default.Goal: 80% of MapWork tasks vectorize in new MiniDrivers (TestVecCliDriver, TestVecNegativeCliDriver, TestVecMiniTezCliDriver).",no,,"[pf1_percentage, ss19_lot_reading]",,
HIVE-9579,162,"Support all get tables [hbase-metastore branch].The initial patch only supported getting a single table, creating a table, altering a table, and dropping a table.Support is needed for getting multiple tables at a time.",no,,[],,
HIVE-9641,163,Fill out remaining partition functions in HBaseStore.A number of the listPartition and getPartition methods are not implemented.The rest need to be implemented.,no,,[],,
IGNITE-10047,164,"MVCC: Wrong coordinator assignment when two oldest nodes fail..Reproducer: <tt>CacheContinuousQueryFailoverMvccTxSelfTest#testLeftPrimaryAndBackupNodes.This test can sporadically hangs when topology is unstable.The problem here is when two oldest nodes A and B failed, other nodes elect B node as a new coordinator despite it is down.This happens because the new mvcc coordinator is assigned in <tt>GridDhtPartitionsExchangeFuture#init method which is called only ones in case of multiple nodes fail simultaneously.",no,,[sk2_block_hang_crash],,
IGNITE-10214,165,"Web console: dependency to open source JDBC driver is not generated in the project's pom file.Steps to reproduce: 
  <ol> 
   import caches from for example MySql DB 
   check generated pom file 
  </ol>",no,,[],,
IGNITE-10344,166,"Speed up cleanupRestoredCaches.if (!cctx.kernalContext().clientNode() &amp;&amp; !isLocalNodeInBaseline()) { &nbsp;// Stop all recovered caches and groups.cctx.cache().onKernalStopCaches(true); &nbsp;cctx.cache().stopCaches(true); &nbsp;cctx.database().cleanupRestoredCaches(); &nbsp;// Set initial node started marker.cctx.database().nodeStart(null); } 
  If we have 100 cache groups we spent a lot of time about 36sec to cleanupRestoredCaches().We need to speed up this phase and add metrics on this.",yes,we spent a lot of time about 36sec to cleanupRestoredCaches().We need to speed up this phase and add metrics on this.,"[ss12_spend_time, gk7_speed]",,
IGNITE-10439,167,[ML] Examples of DBSCAN.We need an example for DBSCAN usage,no,,[],,
IGNITE-10490,168,Web Console: ui-grid should refresh its content on parent element resize.In some situation ui-grid not refreshed if parent element resized.We can handle this with&nbsp;ResizeObserver,no,,[ss11_rework],,
IGNITE-10669,169,"NPE in freelist.PagesList.findTailIndex.Run node with 1 cache and put to it.Kill node and try run back - it broken on start 
  <SOURCECODE>",no,,[],,
IGNITE-10682,170,"Disable unnecessary loaded plugins for the Inspection test suite.As part of discussion 
   <ERROR></ERROR> we've faced with the problem: the set of unnecessary plugins are loaded during the Inspection test suite run.This leads to unnecessary checking inspection rules which are not used in the Apache Ignite project and wasting agent CPU resources.The log can be found at 
   <ERROR></ERROR> execution suite results.<SOURCECODE> 
  We need to disable these loaded plugins as they don't need for checking core inspection rules.<ERROR></ERROR> 
   <URL> 
   <ERROR></ERROR> 
   <URL>",no,,[sk1_negative_necessary],,
IGNITE-10734,171,"Add documentation for the list of operations that should be retried in case of cluster topology changes.Some of the operations, like get or getAll would throw ClusterTopologyException if primary node left topology, while other operations not.So, some operations should be re-tried from user code, while some operation will do it internally.We should prepare documentation for the list of these operations.",no,,[ss11_rework],,
IGNITE-1077,172,SQL Query hangs on node restart.Thread <ERROR>.,yes,query hangs when node restart,[sk2_block_hang_crash],,
IGNITE-1080,173,"Incorrect link to ignite documentation in node's log.In node's log (verbose mode) we can see the follow string: 
  <SOURCECODE> 
  I think here should be direct link to documentation: 
  <SOURCECODE>",no,,[],,
IGNITE-10866,174,[ML] Add an example of LogRegression model loading.Load the LogReg model from Spark via Spark ML Writable to parquet file,no,,"[ss8_load_nn, sk13_regression]",,
IGNITE-10876,175,"""Affinity changes (coordinator) applied"" can be executed in parallel.There is for loop over all cache groups which execution N*P operations in exchange worker where N is number of cache groups, P is number of partitions.We spend 80% of time in a loop 
  for (CacheGroupContext grp : cctx.cache().cacheGroups()) { GridDhtPartitionTopology top = grp != null ?grp.topology() : cctx.exchange().clientTopology(grp.groupId(), events().discoveryCache()); top.beforeExchange(this, true, true); } 
  &nbsp; 
  I believe we can execute it in parallel",yes,We spend 80% of time in a loop and we can make it parallel,"[ss13_cache, pf1_percentage, tk6_iteration]",,
IGNITE-10930,176,"[TC Bot] Support PR-less contributions.An Apache Ignite committer can prepare issue fix with origin/ignite-1nnnn branch.And this contributions testing does not require any PR creations, because TC RunAll may be created without it.Support contributions detection by provided TC run for branches matching ignite-NNNNN, where N=0-9 
  If such RunAll presented for any tracked branch and JIRA issue is in PA state, we may display these branches as addition to PRs, show its report and setup VISA into IGNITE-NNNNN ticket.",no,,[],,
IGNITE-1100,177,"TcpDiscoveryMultiTreadedTest.testMultiThreaded() fails.TcpDiscoveryMultiThreaded wasn't added to any suite.After it's addition it became clear that testMultiThreaded fails permanently.This message always repeats in the log.<SOURCECODE> 
  The full log is attached to the issue.",no,,[],,
IGNITE-11178,178,TcpDiscovertySpi.Print real wait time for timeout exception..We should print to log real wait time in TcpDiscoverySpi in case of timeout exception occurred.It's could help to understand how long we wait in create connection/socket read and etc.We must check time before network communication and print diff between current time and checked time in catched timeoutException block.,yes,we wait long time in create connection/socket read and etc,"[sk8_timeout_expiration, sk2_block_hang_crash]",,
IGNITE-11348,179,"Ping node procedure may fail when another node leaves the cluster.Additional pinging of node on join implemented in 
   <URL> may incorrectly fail leading to shutting down joining node.The reason for this is that if another node from the same host bound to the same discovery port as joining node has left the cluster right before joining node, socket used for pinging gets closed.This leads to the situation when pinging node considers joining node as ""unreachable"" and fails it with JOIN_IMPOSSIBLE error code.Workaround: simply start again node failed on join.",no,,[tk2_bound],,
IGNITE-11373,180,varchar_ignorecase doesn't work properly.Looks like a field with type varchar_ignorecase can't be used for filtering the values for different cases.<SOURCECODE>,no,,[],,
IGNITE-1382,181,Consider using "JavaCritical_" prefix to speedup short Java -> platform callbacks..This could improve performance of various native methods such as "destroy"-family and "reallocate".,no,,"[gk4_performance, gk7_speed]",,
IGNITE-1453,182,"CPP: Big endian support in streams..Currently we write data with assumption that all parties has little endian.While true in most cases, we still not to count for big endian machines.Implementation notes: 1) Define cross-platform method to determine endian.This can be as easy as Bits.java:byteOrder() routine.2) Add big endian support for all read/write operations.",no,,[],,
IGNITE-1468,183,CPP: Support date-time in marshaller..We need to support date-time written on Java/.Net sides.Probably we will have to introduce our own thin wrapper for this.,no,,[],,
IGNITE-1657,184,"Spontaneous scrolling of summary page.Note: this is copied from 
   <URL>.We have no idea right now how to fix it.#NAME?Then switched Client preview to Java view - the page is scrolling down.",no,,[],,
IGNITE-1794,185,"Ignite should support Hibernate 5.Currently Ignite supports Hibernate 4.In Hibernate 5 org.hibernate.cache.spi.RegionFactory.start() method signature has been changed from 
  <tt>void start(Settings var1, Properties var2) throws CacheException; 
  on 
  <tt>void start(SessionFactoryOptions settings, Properties properties) throws CacheException; 
  Original user list: 
   <URL>",no,,[],,
IGNITE-182,186,Free space handling is incorrect when colocated=true in IgniteFS file create request.Set collocated flag to true and copy several large files in grid with more than 1 node with igniteFS.The issue is caused by mistakenly set colocated=true flag.In this case space handling should be different and take into account current free space on node.Also estimated free space for GGFS should be calculated more accurate and allocate less space when heap is small.,no,,[],,
IGNITE-1846,187,"CPP: Adopt portable API changes..Important API changes will be introduced as a part of 
   <URL>.Once these changes are in place, we need to port them to CPP.",no,,[],,
IGNITE-1874,188,Adjust initial placement and size of metadada popup on SQL.Currently if user opens two metadata popups then they will cross each other (please see attachment).Because of that I suggest: 1) change initial position - align with top of query be vertical 2) change initial height - the same as initial query height,no,,[],,
IGNITE-1884,189,".Net: JNI local ref can't be accessed from another thread.Documentation: 
   <URL> 
  <SOURCECODE> 
  We have two places where we DO pass local JNI reference to another thread: 
   
   CacheParallelLoadStoreAdapter 
   CacheTestStore.LoadCache 
   
  And, potentially (due to user code): 
   
   UnmanagedCallbacks.DataStreamerStreamReceiverInvoke 
   
  For some reason it has worked for us before.But renamings in 
   <URL> have caused test execution order to change, and these store tests cause process crash.To reproduce, BinaryBuilderSelfTest (former PortableApiSelfTest) has to be executed before store tests.All other tests can be excluded.100% repro rate: ""FATAL ERROR in native method: Bad global or local ref passed to JNI"".",no,,"[pf1_percentage, sk2_block_hang_crash]",,
IGNITE-1932,190,"Wrong value of BusyTimePercentage metric after ignite.cluster().resetMetrics().I found that after call of ignite.cluster().resetMetrics() 
  BusyTimePercentage became 100%  
  After some debug I found that BusyTimePercentage - calculated as: 1 - IdleTimePercentage  
  and IdleTimePercentage calculated as IdleTime / UpTime 
  and on reset we set IdleTime to 0.So the possible fix is to preserve IdleTime on reset.",no,,[pf1_percentage],,
IGNITE-202,191,OptimizedClassNamesGenerator.checkSerialVersionUid should throw exception if class have SerialVersionUid but do not implement Serializable.,no,,[],,
IGNITE-213,192,To delete licenseUrl from IgniteConfiguration and to refactor related staff .DFLT_LIC_FILE_NAME constant should be moved to ggprivate project,no,,[],,
IGNITE-2380,193,".NET: Ignite configuration in app.config and web.config.Define custom ConfigurationSection for IgniteConfiguration (see 
    <URL>) 
   Extend Ignition class to be able to start Ignite from app.config / web.config configuration 
   Make sure to include xml schema",no,,[],,
IGNITE-2416,194,TcpDiscoverySharedFsIpFinder doesn't work with IPv6 addresses.<tt>TcpDiscoverySharedFsIpFinder fails if there is IPv6 address in the list.<SOURCECODE>,no,,[],,
IGNITE-2450,195,"Proxy classes are not serialized properly using OptimizedMarshaller and BinaryMarshaller.I tried to deploy several services using Java's Proxy class, to dynamically define the needed interfaces.When using a single process (even with several nodes running in the same JVM) everything worked.But when working with a large cluster of several machines I got the following error: 
  <SOURCECODE> 
  I think the problem is that the marshaller tries to reference the proxy class by name, but deserialization of proxy classes should be special (
   <URL>) and create the classes on the fly.When using the JdkMarshaller the problem doesn't happen.This problem doesn't show up in existing tests (
   <URL>) because they all run in the same JVM, where the same proxy classes exist before and after serialization.You must have nodes in two different processes, and create several proxy classes, for the problem to manifest.",no,,[],,
IGNITE-2518,196,Untrue 'unsaved changes'.I've noticed untrue 'unsaved changes' is appear in case when I click Back to Configuration button in the 'Connection with web agent is not established' dialog,no,,[],,
IGNITE-2524,197,"Save and revert buttons should not be active after any input and clean.Steps 
  <ol> 
   Create new cluster and save 
   Enter into port number -1 
   Clean 
   Save and revert buttons are active 
  </ol> 
  Expected Save and revert buttons are not active",no,,[],,
IGNITE-2656,198,"Documentation on debugging and fixing the reasons of node disconnection from the cluster.Sometimes a node can be abruptly kicked off from the cluster buy some reason.The documentation must contain information on how to get to the root of the issue by looking at logs files.Usually the node that was kicked off contains ""Local node segmented"" message and the node that failed its next neighbor contains a message with more details ""Failed to send message to next node"".Next the article must list possible reasons of the disconnection: 
  <ul class=""alternate"" type=""square""> 
   long GC pauses.Give recommendations on how to check; 
   high node utilization so that it responds with a delay; 
   low network configuration parameters that are not suited for an environment; 
   
  There should be a section about <tt>IgniteConfiguration.failureDetectionTimeout describing its behavior and showing all its pros and cons.The article must say when it makes sense to 'disable' this timeout by switching to explicit configuration of TcpDiscoverySpi.socketTimeout, TcpDiscoverySpi.ackTimeout, TcpDiscoverySpi.maxAckTimeout, TcpDiscoverySpi.reconnectCount.Pros and cons of manual configuration has to be mentioned as well.Also I would list the usage of TcpDiscoverySpi.joinTimeout, TcpDiscoverySpi.networkTimeout (used on client reconnect, servers waits for join result, node stop, socket reader first message.)there as well.",no,,[sk8_timeout_expiration],,
IGNITE-2751,199,"GridH2TreeIndex.getRowCount unwraps objects' values that are not used later.If to run query like <SOURCECODE>.then our implementation will perform a full scan filtering out entries that are not primary for a given node and will calculate only a number of primary ones.If entries are stored off-heap then both a key and value of an entry are unswapped, Unswapped values are not used by backup filters thus we have to omit unswapping of the values at all.",no,,[],,
IGNITE-2792,200,"System caches should not use user-defined TransactionConfiguration..Problem Currently if user set his custom TransactionConfiguration, it will be used not only user public caches, but by internal Ignite caches as well.This could lead to some bad situations, such a transaction timeouts, optimistic exceptions, etc..Proposed solution Ensure that transactional system caches do not use custom TransactionConfiguraton.",no,,[sk8_timeout_expiration],,
IGNITE-2807,201,"IGFS: re-create lock relaxed version.Earlier we had IGFS implementation that seemed to be fast, but was not absolutely correct in terms of synchronization logic.As now we suspect some problems with IGFS performance, we need to create such version of the implementation again.",yes,some problems with IGFS performance,"[gk4_performance, ss11_rework]",,
IGNITE-2899,202,"BinaryObject is deserialized before getting passed to CacheInterceptor.If <tt>CacheInterceptor is configured for a cache that stores <tt>BinaryObjects then the objects are always deserialized before being passed to the interceptor body.Refer to BinaryInterceptorIssue test attached to the ticket to reproduce the following stack trace 
  <SOURCECODE> 
  Moreover if to modify the test a bit we will reproduce one more bug when an object type is not being registered with <tt>BinaryContext if the object is being created with <tt>BinaryObjectBuilder.That bug hides the issue described above.Refer to BinaryInterceptorNoTypeIssue to see the following stack trace 
  <SOURCECODE>",no,,[],,
IGNITE-2902,203,"CPP: Make IgniteError subclass of the std::exception..IgniteError should implement std::exception so it can be catched by the following block: 
  <SOURCECODE>",no,,[sk2_block_hang_crash],,
IGNITE-2939,204,".NET: Review default Xms/Xmx settings.Currently IgniteConfiguration in .NET defaults to 512m/1024m, which is not correct for x86 mode.Instead, we should rely on JVM defaults: 
   
   Treat negative IgniteConfiguration.Jvm*MemoryMb property values as ""none"" and don't send them to Java 
   Use these by default 
   Remove Xmx/Xms settings from examples",no,,[],,
IGNITE-2981,205,CPP: Implement EnableSharedFromThis for internal use..We need to have an analogue of the <tt>std::enable_shared_from_this to be able to create new shared pointer instance inside of method of the current shared instance.Needed for example to implement transactions.,no,,[],,
IGNITE-3132,206,Distributed SQL query hangs if rebalanceMode=NONE.See attached files for reproducer.Needs one server node and client node.,yes,query hangs when node restart,[sk2_block_hang_crash],,
IGNITE-3245,207,"IGFS: Optimize performance for client nodes..Problem Ignite can work in two modes - server (default) and client.Both modes provide transparent cache API.IGFS heavily rely on cache operations and sometimes perform lots of them with assumption that metadata is located on the same node, so these operations are cheap and do not require any cross-node communication.But when user works with IGFS from client node, this code could be very inefficient because even single operation could cause many network round trips between a client and a server.Solution Provide special implementations of IGFS operations which will delegate the whole execution to the server, thus leaving only a single network round trip.",yes,Optimize performance for client nodes,"[gk1_efficiency, ss13_cache, gk4_performance]",,
IGNITE-3255,208,".NET: Script execution.Introduce an ability to execute C# scripts on remote nodes.This may be useful for: 
   
   Diagnostics and debugging on live cluster 
   Quick try-out for computations 
   Demo purposes 
   
  This is a light-weight alternative to 
   <URL>: we don't have to deal with automatically detecting whether assembly needs to be loaded remotely, with it's dependencies and AppDomain lifecycle.Instead, we send a string, compile it remotely, execute in a separate AppDomain and unload it.",no,,[gk7_speed],,
IGNITE-336,209,We need to add dynamic turn on/off cache statistics for Visor..We need create task to turn on/off of cache statistics collecting.Create Visor command for toggling.,no,,[ss13_cache],,
IGNITE-3362,210,"Event EVT_CACHE_REBALANCE_STOPPED fires prematurely..EVT_CACHE_REBALANCE_STOPPED fires earlier then the partiotions has been rebalanced.You can see detail here:  
   <URL>",no,,[],,
IGNITE-351,211,"Ensure that IPC communication works in both TCP and shmem modes in Hadoop..Our Hadoop module can interact with IGFS in several ways: 1) Over TCP (Linux, Windows).2) Over shmem (Linux only).Actual mode is selected based on file system URL.We need to ensure that both modes work fine at the moment.Some unit tests sohuld already exists for this.We must ensure that they cover both cases.",no,,[],,
IGNITE-3560,212,"Unify type descriptor removal and destruction in IgniteH2Indexing.Currently I see at least 3 places where we have to deal with table destruction: 
  1.unregisterCache() -&gt; Schema.onDrop() 2.unregisterType() -&gt; removeTable() 3.stop() 
  We need to refactor them to call the same logic which has to always do the following steps: 1.dataTables.remove() 2.GridH2Table.destroy() 3.U.closeQuiet(TableDescriptor.luceneIdx)",no,,[],,
IGNITE-3576,213,.NET: Implement distributed data structures.Umbrella ticket to host data structures efforts.,no,,[],,
IGNITE-3629,214,Web Console: Implement test for backend API routes.Need to implement integration tests for all backend routes.,no,,[],,
IGNITE-3725,215,Web console: Import model from DB: error with unchecking schemas.Try to import model from any DB (in my case it was a MySQL) On 'select schema' step uncheck all schemas by clicking on 'Schema' checkbox.Expected - all schemas should become unchecked.Observed - only first schema was unchecked.All the others stayed marked.,no,,[],,
IGNITE-3759,216,"ODBC: Add tests for SQL_DATETIME_LITERALS..Let's add tests for SQL_DATETIME_LITERALS 
   <ERROR>",no,,[],,
IGNITE-3869,217,Reduce number of temporary objects produced by H2.Presently during the execution of a query H2 generates significant number of temporal objects (kind of wrappers) that eventually exhaust the heap and trigger long GC pauses.Need to revisit present implementation improving Ignite SQL engine and/or H2.,yes,exhaust the heap and trigger long GC pauses,[gk3_reduce],,
IGNITE-3912,218,"Hadoop: rework HadoopClassLoader infrastructure.This is an umbrella ticket to host tickets related to <tt>HadoopClassLoader rework logic.In general, we should achieve clean and consistent separation of ignite-core on the one hand, and (ignite-hadoop + Hadoop dependencies) on the other.",no,,[ss11_rework],,
IGNITE-3924,219,".NET: Integrate with Foundatio.Foundatio (
   <URL>) wraps multiple frameworks (such as Redis) to provide unified interface for building distributed applications.Ignite.NET can provide implementations for modules such as Caching, Queues, Messaging, Jobs.Overview: 
   <URL>",no,,[],,
IGNITE-3997,220,".NET: Readme.io documentation update for 1.8.Update documentation for new features: 
   
   Logging 
   ASP.NET Session State 
   Entity Framework caching etc 
   
  Mention EF6 2nd level cache on StackOverflow: google.com/search?q=efcache.codeplex.com+site%3Astackoverflow.com",no,,[ss13_cache],,
IGNITE-41,221,Support ExpiryPolicy.Implement support for javax.cache.expiry.ExpiryPolicy.Also IgniteCache.withExpiryPolicy(ExpiryPolicy plc).,no,,[],,
IGNITE-4118,222,".NET: documentation and examples for deadlock-free transactions.Presently the documentation and example about the deadlock-free transactions are missing on .NET side.Let's fill this gap.<ul class=""alternate"" type=""square""> 
   Documentation can be taken from Java side 
    <URL> 
    <URL> 
   
  <ul class=""alternate"" type=""square""> 
   The example that can be (should be) added to the distribution is the following 
    <URL>",no,,[],,
IGNITE-4249,223,"ODBC: Even simple request takes a lot of time..Even if you submit very simple request with <tt>SQLExecute on a small data set, it's going to take ~40ms to be processed, while the same request in C++ client takes ~200us.",yes,"Slowness: it's going to take ~40ms to be processed, while the same request in C++ client takes ~200us",[ss12_spend_time],,
IGNITE-436,224,"Remove 'compact' command from visor.After IGNTIE-51 'compact' command is not supported anymore, need to remove this command from Visor.",no,,[],,
IGNITE-4500,225,".NET: Support identity resolver for binary enums.Enums are a special case when it comes to binary objects.They are written as 
   <ERROR></ERROR> and do not include hash code.However, if user sets a custom identity resolver vi <tt>BinaryTypeConfiguration.EqualityComparer, he may expect it to be used when <tt>GetHashCode and <tt>Equals are called in binary form.Investigate whether this is a right approach, add enum support to predefined identity resolvers.",no,,[],,
IGNITE-4540,226,"Ability to disable default H2 Indexing..It is impossible for now to use IndexingSPI without H2Indexing due to next reasons: 
  <ul class=""alternate"" type=""square""> 
   H2Indexing always starts if IgniteH2Indexing class found in classpath.if IndexingTypes are set and IndexingSPI is configured, then both IndexingSPI and H2Indexing is used.If IndexingTypes is not set, then both indexers are disabled.We need to add an option that force disabling H2IndexingSpi.It look like the only workaround for now is remove ignite-indexing jar from classpath.",no,,[],,
IGNITE-4552,227,"Optimize GridDhtLocalPartition.rmvQueue.Current implementation stores deferred entry removals in rmvQueue for consistency guaranties.This can lead to significant heap over-usage(I observed several Gbs) in case of many caches with removals, because currently queue is cleared lazily after reaching max capacity(200_000 by default).This can be mitigated by using lower IGNITE_ATOMIC_CACHE_DELETE_HISTORY_SIZE, but can lead to consistency issues in case of frequent cache updates.Possible optimizations: 
   
   Use single fixed size queue per all caches to overcome limitations of IGNITE_ATOMIC_CACHE_DELETE_HISTORY_SIZE workaround.Do queue cleaning in background 
   
   
   Move queue to an off-heap.",no,,"[ss1_one_per, sk9_frequency, ss13_cache]",,
IGNITE-4576,228,".NET: Rename IgniteConfiguration.gridName.The same as 
   <URL> for .NET",no,,[],,
IGNITE-4581,229,Async API: IgniteCache refactoring .<tt>IgniteCache refactoring to simplify the async API.,no,,[],,
IGNITE-4713,230,"refactoring of GridFinishedFuture and GridFutureAdapter.I propose to do refactoring of classes ""GridFinishedFuture"" and ""GridFutureAdapter"".There is field ""resFlag"" which can equals ""ERR = 1"" or ""RES = 2"".So I can replace it to one ""bool haveResult"" field.If there are no objections, I'm ready to proceed.If you find more such classes, please write about them.",no,,[],,
IGNITE-4714,231,Web console: Disable refresh rate for explain query.After execution of explain query start of refresh rate will execute explain query with specified frequency.,no,,[sk9_frequency],,
IGNITE-4777,232,Web Console: Add notification on 'Download project' button.After the configuration user need to download project and run it to get the result.Let's notify about this!Also in this task we need to revise tips.And improve instructions for new users.,no,,[],,
IGNITE-4800,233,"Lucene query may fails with NPE..GridLuceneFile 'buffers' field is set to null regardless file can be used in GridLuceneInputStream by some query.We should add a guard, that will prevent 'buffers' to be cleared until all GridLuceneInputStream-s is closed.Userlist discussion: 
   <URL>",no,,[],,
IGNITE-4965,234,"Document enum fileds handling and changes with 'select * ...' in SQL .Document the following: 
   
   enum fields handling in SQL (
    <URL>) 
   _key and _val are no longer returned in ""select *..."" result set (
    <URL>) 
   
  Readme page: 
   <URL>",no,,[],,
IGNITE-4984,235,"AssertionError GridNearAtomicSingleUpdateFuture.onAllReceived on node left.Observed this assertion: 
  <SOURCECODE>",no,,[],,
IGNITE-5125,236,"Need to improve logging in case of hang.1When cache operation hangs on node it is not reported as hanged although partition map exchange cannot finish.<SOURCECODE> 
  <SOURCECODE> 
  2.Partition exchnage future dumps objects only limited number of times.I would suggest to switch to mode when we double the delay between dumps each time, but no more than 30min 
  3.If exchange worker is stuck at GridDhtPartitionsExchangeFuture.waitPartitionRelease then unreleased partitions should be reported (same rules as of pt 2 apply)  
  <SOURCECODE>",no,,"[sk2_block_hang_crash, pf2_duration]",,
IGNITE-530,237,"Implement IgniteTwitterStreamer to stream data from Twitter feeds.We have <tt>IgniteDataStreamer which is used to load data into Ignite under high load.It was previously named <tt>IgniteDataLoader, see ticket 
   <URL>.We should create <tt>IgniteTwitterStreamer which will consume messages from Twitter and stream them into Ignite caches.More details to follow, but to the least we should be able to: 
   
   Convert Twitter data to Ignite data using an optional pluggable converter.Specify the cache name for the Ignite cache to load data into.Specify other flags available on <tt>IgniteDataStreamer class.",no,,[ss8_load_nn],,
IGNITE-5380,238,"Validate cache QueryEntities in discovery thread.Consider the following case: 1) Execute SQL: TABLE Person ...}} 2) Then again: TABLE Person ...}} Second call will lead to exception in exchange thread and will hang the whole cluster.We need to add validation of <tt>CacheConfiguration.queryEntities wrt to other caches.This check should be performed in discovery thread.Note that we cannot rely on <tt>GridQueryProcessor or <tt>IgniteH2Indexing state, as some cache start requests may already be enqueued to exchange worker.Instead, we should perform cross-cache validation base only on two things: 1) <tt>DynamicCacheDescriptor.cacheCfg 2) <tt>DynamicCacheDescriptor.schema 
  That is, we should resolve cache schema name from configuration, tables and indexes from schema, and then cross-validate them with other caches.",no,,"[ss13_cache, sk2_block_hang_crash]",,
IGNITE-5489,239,Possible connection leaks when loadPreviousValue set to true.When <tt>CacheConfiguration#setLoadPreviousValue set to true on owning node does not call <tt>CacheStore#sessionEnd method.It can to lead to leak of connections to DB.See attached test.,no,,[],,
IGNITE-5559,240,"New method on MarshallerContext to register mappings locally.As Ignite got a new persistent store functionality, it becomes possible to save caches content to disk, restore from it later and even move data using plain files from one cluster to another.In that case marshaller mappings may be saved on disk as well and restored later as well on other Ignite clusters.As mappings are restored on each node locally and don't require exchanging any messages among the cluster new convenient method <tt>MarshallerContext::registerClassNameLocally is needed.",no,,"[ss13_cache, ss26_negative_do_everything]",,
IGNITE-561,241,"Website Improvements.<ol> 
   Add Community and Committers table on community.html 
   Design page-per-feature format 
   Change syntax highlighting to work w/ non-java code 
   Create coding guidelines page on the website and link to it from the community page.</ol>",no,,[],,
IGNITE-568,242,Schema Import Utility Should generate code as mirror for XML.Ignite Schema Import utility generate XML with cache store type metadata and Query fields and indexes.But utility generate code for initializing only store type metadata.We need also generate code that will initialize query fields and indexes.,no,,[ss13_cache],,
IGNITE-57,243,"Support size/localSize with CachePeekMode.See IgniteCache.size(CachePeekMode), IgniteCache.localSize(CachePeekMode).",no,,[],,
IGNITE-5729,244,IgniteCacheProxy instances from "with..." methods are not reusable.On cache restart all IgniteCacheProxy instances must be reset in order to reuse them.But bunch of methods in IgniteCache interface including withKeepBinary create new instances of proxy for each call and these instances are not reset on cache restart.E.g. it leads to CacheStoppedException when reusing them after restoring from snapshot.,no,,[ss26_negative_do_everything],,
IGNITE-5859,245,"IgniteUtils.ceilPow2 overflow for values greater than 2^30.Method ""IgniteUtils.ceilPow2"" can overflow for values greater than 2^30 and return Integer.MIN_VALUE for them.Maybe this check was skipped for method speed.In this case, we need to add information about this into javaDoc.",yes,memory overflow,[sk10_skip],,
IGNITE-5949,246,"DDL: Support ALTER TABLE DROP COLUMN.Ignite should support <tt>DROP COLUMN operation for <tt>ALTER TABLE command.Design considerations: 1) Drop should only be possible on binary types without schema (see 
   <URL>).Probably we will need a new option for <tt>CREATE TABLE command 2) Drop should not block other operations for a long time.We should synchronously block the table, change meta, then release the lock and let operations continue.3) Actual data remove should be performed asynchronously in the same way we create index.During this time we should not allow any other modifications to the table 4) Be careful with node stop - we do not want to wait for years for this command to complete.",no,,"[sk2_block_hang_crash, pf2_duration]",,
IGNITE-6017,247,"Ignite IGFS: IgfsStreamsSelfTest#testCreateFileFragmented fails.Failure is almost can't be reproduced locally.Suppose it is the same problem as in 
   <URL> .<SOURCECODE>",no,,[],,
IGNITE-6070,248,"Infinite redirects at Spring Security Web Session Clustering with Tomcat.See 
   <URL> description.When Session comes from Ignite but its Authentication is anonymous, Spring Security does the following check: 
  <SOURCECODE> 
  The problem is, in Ignite we never override isRequestedSessionIdValid() in our request wrappers, so it falls back to Tomcat's own (session) Manager, which will then find that it has never seen this Session and it is therefore invalid.Thus failover is triggered, and if there's ""invalid-session-url"" clause in Spring Security config, redirect will be issued, possibly circular.I've attaching a sample Maven WAR project.It should be deployed to two different Tomcat instances operating on two different ports of same machine, e.g. 8080 and 8180, and then 
   <URL> should be opened in the same Web Browser one after another for two ports.The second one should cause infinite 302 Redirect to same URL.There's also a minor bug in the same code: see session.jsp in the example.It will needlessly throw NPE in WebSessionFilter:1001 and confuse web server.Should output ""OK"" when fixed.Discussion: 
  By the way, letting the web server to get hold on Sessions that it creates for us causes additional problems: it's going to store these sessions in parallel with Ignite, consuming memory in the process that first saw a given session.We should probably have (possibly a third party) an Ignite-using Manager implementation for Tomcat specifically.It will be much simpler than filter-based approach while performing better.Or maybe we should create our own Sessions that we never allow the web server to see.",no,,"[sk11_infinity, ss4_memory]",,
IGNITE-6210,249,"Inefficient memory consumption for checkpoint buffer.Current implementation allows configure checkpoint buffer size in PersistentStoreConfiguration, but checkpoint buffer will be created for each memory configuration with size equals the one indicated in PersistentStoreConfiguration.For example: 
  <SOURCECODE> 
  pl1(max size 10Gb) will be have checkpoint buffer = 5GB and pl2(max size 100Mb) buffer= 5GB",yes,Inefficient memory consumption for checkpoint buffer,"[ss15_each_one, gk1_efficiency, sk7_buffer, ss4_memory, tk3_memory_unit]",,
IGNITE-6343,250,"Index is not used properly if changing sort order..Unit test reproducer: 
  <SOURCECODE>",no,,[],,
IGNITE-6356,251,"Signed overflow when reading partition greater than Short.MAX_VALUE.When starting a cache with the number of partitions greater than 32768, the following error is observed: 
  <SOURCECODE>",no,,[],,
IGNITE-6358,252,"JDBC thick: support multiple SQL statements.See 
   <URL>.We need to implement the same thing for thick JDBC driver.",no,,[],,
IGNITE-6389,253,"Cache.invoke fails on atomic cache with configured AccessedExpiryPolicy.When calling <tt>invoke(...) on atomic cache with <tt>AccessedExpiryPolicy, and the target key already exists in cache, then the following exception is thrown: 
  <SOURCECODE>",no,,[],,
IGNITE-6515,254,".NET: Enable persistence on per-cache basis.Propagate new configuration to .NET: 
   <URL>",no,,[],,
IGNITE-6522,255,"Binary metadata is invalid after creation on BinaryConfiguration.classNames with wildcard.The enum flag and enum's fields are calculated invalid on iterating through classes specified with wildcard <tt>&lt;package_name&gt;.* at the <tt>BinaryConfiguration.classNames 
  See:  <tt>BinaryContext#configure() <tt>typeCfg is created for type with wildcard.Enum flag &amp; fields are gathered for type with wildcard instead of recalculate for each type (inside the loop: {<tt>for (String clsName0 : classesInPackage(pkgName)))",no,,"[tk6_iteration, ss11_rework]",,
IGNITE-6546,256,"Update faveicon.ico in rest-htttp module.Correct icon: 
   <URL>",no,,[],,
IGNITE-6554,257,"Atomic cache remove operations are not logged into WAL.Create cache with atomicity mode CacheAtomicityMode ATOMIC  and perform put remove for one cache entry 
  <SOURCECODE> 
  Actual WAL content loaded by standalone WAL iterator is as follows: 
  <SOURCECODE> 
  For TRANSACTIONAL AtomicityMode same test is passing.Reproducing test can be found in IgniteWalReaderTest.java in PR 2797 (
   <URL>): 
   <URL> 
  testRemoveOperationPresentedForDataEntryForAtomic fails, but testRemoveOperationPresentedForDataEntry is passing",no,,"[ss13_cache, tk6_iteration]",,
IGNITE-664,258,[TC] Need to split Ignite cache suite.Need to split Ignite Cache suite.Ignite Cache build require about a hour for execution.Need to split on suites by 15-20 minutes.,no,,"[ss13_cache, pf2_duration]",,
IGNITE-6653,259,"Check equality configuration between metasore and XML.To introduce another point of configuration validation between cluster (XML, java code) and metasore.Provide detailed information and decline to start cluster if different",no,,[],,
IGNITE-6756,260,"Primary Node Change causes invalidation of GridNearCacheEntry topVer.When using a near cache after a cache exists in a cluster, their appears to be a bug which causes the GridNearCacheEntry.topVer to be set to NONE on a check of the primaryNode after a topology change when the !nodeId.equals(primary.id().This will prevent the topVer from being updated to the latest, which then cause the GridNearCacheEntry.valid to return false and force a hard hit to the cluster.Steps to reproduce:  
  <ol> 
   Create 2 node 
   create cache on 1 node 
   populate cache 1 
   create client 
   create nearCache of original cache 
   warm nearCache 
   Cause topolgy change 
  </ol> 
  Expected:  Topology change cause 1 hard hit to cluster to make sure value value is up to date (guessing on this)  All future request will be retrived from near cache 
  Actual All request are to cluster since the GridNearCacheEntry.valid method return false.",no,,[ss13_cache],,
IGNITE-6828,261,"Confusing messages ""SLF4J: Failed to load class"" at Ignite start.How to reproduce: 1.build Ignite 2.go to examples\ 3.run: mvn exec:java -Dexec.mainClass=""org.apache.ignite.examples.ExampleNodeStartup"" 
  you will see the following confusing output: SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".SLF4J: Defaulting to no-operation (NOP) logger implementation SLF4J: See 
   <URL> for further details.SLF4J dependency comes from org.springframework.data:spring-data-commons: 
  
   <ERROR>
  We should remove this dependency because it is confusing and does not affects to Ignite logging functionality 
  Dev-list: 
   <URL>",no,,[],,
IGNITE-6846,262,"Add metrics for entry processor invocations.<tt>CacheMetrics object has multiple metrics for various cache operations like <tt>get, <tt>put and <tt>remove, but nothing for <tt>invoke/<tt>EntryProcessor.It makes sense to add such metrics, for example: 
   
   Total number of `invoke` operations executed.Number of `invoke` operations that included updates.Number of read-only `invoke` operations.Min/max/avg execution time....",no,,[],,
IGNITE-7095,263,JdbcConnection hard linked in IgniteH2Indexing preventing old connections from closing.Every time a new thread performs <tt>SqlFieldsQuery the new <tt>org.h2.jdbc.JdbcConnection will be created and added to the <tt>org.apache.ignite.internal.processors.query.h2.IgniteH2Indexing#conns.Such hard link prevents from the <tt>org.h2.util.CloseWatcher to be enqueued and old connections will remain in heap even after the thread exits.After some number of SQL queries the <tt>java.lang.OutOfMemoryError will occur.,no,,[],,
IGNITE-7175,264,"Throttling is not applied to page allocation.Found during 
   <URL> testing, throttling check is not done for newly allocated pages.Allocated page already has dirty flag, but Throttling parkNanos is applied only for intially not dirty page.For initial load case it is required to do Throttling check for mostly new pages",no,,"[ss8_load_nn, sk18_throttle]",,
IGNITE-7233,265,Problems with classpath on Windows when IGNITE_HOME contains spaces.<SOURCECODE>,no,,[],,
IGNITE-7266,266,"SQL TX: Joins return duplicated result when MVCC is disabled.Affected tests: <tt>IgniteCacheJoinQueryWithAffinityKeyTest <tt>IgniteCacheCrossCacheJoinRandomTest 
  All tests failed for the same reason: <tt>expected=X, actual=2*X. Looks like we return too much results in some cases.",no,,[],,
IGNITE-7271,267,UPDATE and DELETE statements do not work through thin JDBC work in MVCC mode.See relevant failures JDBC suite.,no,,[],,
IGNITE-7326,268,"Fix ignitevisorcmd | sqlline scripts to be able to run from /usr/bin installed as symbolic links.Currenlty, <tt>ignitevisorcmd.sh and <tt>sqlline.sh being installed into <tt>/usr/bin will fail to run because of: 
   
   their unawarelessness of theirs real location; 
   necessity to write to <tt>${IGNITE_HOME}/work which can have different permissions and owner (in packages, for example).It is required to rewrite these scripts to be able to run from anywhere by theirs symbolic linka and with some temporary dir (<tt>/tmp for example) as workdir.",no,,[ss11_rework],,
IGNITE-7368,269,"IgniteCache.getAll() throws high volume of GridDhtInvalidPartitionException if on-heap is enabled.With on-heap option enabled, <tt>IgniteCache.getAll() throws a GridDhtInvalidPartitionException if the input set of keys contains at least one non-local key (a key for which the node executing <tt>getAll is not the primary).In general case, the input set always contains such keys which, given sufficiently high request rate, results in thousands of exceptions thrown per second.Attached is a screenshot of a JFR recording showing the exceptions and their stack traces.",no,,[ss1_one_per],,
IGNITE-74,270,"Move configuration classes to correct packages.Need to discuss where to move classes IgniteAddressResolver, IgniteDeploymentMode, ClientMessageInterceptor.",no,,[],,
IGNITE-7570,271,"Client nodes failed with ""Failed to process invalid partitions response"" during failover.Some client nodes fail with ""Failed to process invalid partitions response"" during failover test: 
  <SOURCECODE> 
  Test config: 
  CacheRandomOperationBenchmark 
  <ul class=""alternate"" type=""square""> 
   20 server nodes, 10 client nodes at 10 hosts 
   34 caches with different configs with and without PDS, 3 backups 
   preload amount 250 
   
  <ul class=""alternate"" type=""square""> 
   key range 500K 
   
  <ul class=""alternate"" type=""square""> 
   operations: PUT PUT_ALL GET GET_ALL INVOKE INVOKE_ALL REMOVE REMOVE_ALL PUT_IF_ABSENT REPLACE 
   
  <ul class=""alternate"" type=""square""> 
   2 of 20 servers are being restarted every 15 minutes 
   
  Complete yardstick configs are attached.",no,,"[ss13_cache, ss11_rework, pf4_benchmark]",,
IGNITE-7639,272,"NullPointerException in publicApiActiveState.This exception is observed in the test: 
  <SOURCECODE> 
  The reason is that cluster state is transferred to a joining node and prevState is null in this case.",no,,[],,
IGNITE-7648,273,"Fix IGNITE_ENABLE_FORCIBLE_NODE_KILL system property..IGNITE_ENABLE_FORCIBLE_NODE_KILL system property was introduced in 
   <URL> as a way to prevent&nbsp;unnecessary node drops in case of short network problems.I suppose it's wrong decision to fix it in such way.We had faced some issues in our production due to lack of automatic kicking of ill-behaving nodes (on example, hanging due to long GC pauses) until we realised the necessity of changing default behavior via property.Right solution is to kick nodes only&nbsp;if failure threshold is reached.Such behavior should be always enabled.UPDATE: During a discussion it was decided what the property will remain disabled by default.We decided to change timeout logic in case of failure detection enabled.We start performing connect and handshake from 500ms increasing using exponential backoff strategy.",no,,"[sk2_block_hang_crash, sk8_timeout_expiration, sk1_negative_necessary]",,
IGNITE-7694,274,"testActiveClientReconnectToInactiveCluster hangs because of an assertion.This is a regression from 
  The test hangs because there is an assertion happened after the client reconnects to the cluster: 
  <SOURCECODE> 
  The reason for the assertion is that the client does not clear <tt>lastAffVer field when disconnected, and cluster is restarted when the client is in the disconnected state.",yes,performance regression,"[sk13_regression, sk2_block_hang_crash]",,
IGNITE-776,275,"Fix version update notification during Ignite start..To avoid the confusion about different binaries the version update notification should work as follows: 
  <ul class=""alternate"" type=""square""> 
   Apache Ignite should notify only about Apache Ignite new versions.",no,,[],,
IGNITE-7849,276,"Generating a CacheEntryEventFilter from a SqlQuery.Currently when we want to use the same predicate for the continuous query and initial query, it's easy enough to write something like this, assuming we are fine with the performance of ScanQuery: 
  <SOURCECODE> 
  However, this becomes more inconvenient when we want to use SqlQuery in the initial query to take advantage of indexing.This is the best that I can do: 
  <SOURCECODE> 
  This is obviously not ideal because we have to specify the predicate in two different ways.A quick Google revealed that there are products out there that more seamlessly support this use case of continuous querying.I understand that Ignite isn't built on top of SQL, unlike the commercial RDBMSes I found, so maybe this is an out-of-scope feature.",no,,"[gk4_performance, gk7_speed]",,
IGNITE-8056,277,"Ignite Cassandra integration doesn't correctly handle Inet4Address and Inet6Address objects.PropertyMappingHelper.getCassandraType correctly handles the inet &lt;-&gt; java.net.InetAddress type, but not it's sub classes of java.net.Inet4Address or java.net.Inet6Address.As there is no standard way to directly create an InetAddress, this makes it very difficult to use an inet data types for keys in cassandra.In general, shouldn't Cassandra types handle any subclasses of supported classes?",no,,[],,
IGNITE-8092,278,"Put operation may hang if cache was destroyed asynchronously..If there is more than one cache in the cache group then put operation on cache may hang if it was destroyed asynchronously.For now this applies to all cache modes (PARTITIONED/REPLICATED/LOCAL) and to all atomicity modes (ATOMIC/TRANSACTIONAL).This problem can not be reproduced if there is only one cache in the cache group.Reproducer: 
  <SOURCECODE> 
  p.s. for ATOMIC cache additional cache status check in GridCacheGateway#onStopped busy wait resolve this problem.",yes,put operation hangs,"[ss13_cache, ss2_all_to_one, sk2_block_hang_crash]",,
IGNITE-8146,279,"JDK9: IgniteUtils classLoaderUrls() JDK9 bug.Reporting a probable miss that breaks JDK9 compatibility.As part of&nbsp;
   <URL>&nbsp;&nbsp;in&nbsp;
   <URL>/modules/core/src/main/java/org/apache/ignite/internal/util/IgniteUtils.java,&nbsp;below lines will throw&nbsp;ClassCastException in JDK9 during runtime.<em>return ((URLClassLoader)urlClsLdrField.get(clsLdr)).getURLs();</em> 
  &nbsp; 
  Here,&nbsp;<em>urlClsLdrField.get(clsLdr)</em>&nbsp;return an object of type ""__<em>java.base/jdk.internal.loader.URLClassPath</em>"" which can't be cast to ""<em>URLClassLoader"".</em> 
  &nbsp; 
  The fix seems to be to introduce another reflection call to invoke&nbsp;<em>getURLs</em>&nbsp; method of the internal&nbsp;<em>URLClassPath</em>&nbsp;class.",no,,[],,
IGNITE-8241,280,"Docs: Triggering automatic rebalancing if the whole baseline topology is not recovered.The ticket is created as a result of the following discussion: 
   <URL> 
  The rebalancing doesn't happen if one of the nodes goes down,  thus, shrinking the baseline topology.It complies with our assumption that  the node should be recovered soon and there is no need to waste  CPU/memory/networking resources of the cluster shifting the data around.However, there are always edge cases.I was reasonably asked how to trigger  the rebalancing within the baseline topology manually or on timeout if:  
   
   It's not expected that the failed node would be resurrected in the nearest time and  
   It's not likely that that node will be replaced by the other one.Until we embedd special facilities in the baseline topology that would consider such situations we can document the following workaround.A user application/tool/script has to subscribe to node_left events and remove the failed node from the baseline topology in some time.Once the node is removed, the baseline topology will be changed, and the rebalancing will be kicked off.",no,,"[sk8_timeout_expiration, ss11_rework]",,
IGNITE-8368,281,"Reimplement queries notebooks list table wth pc-items-table'.Currently notebooks list table is implemented with pure ui-grid directive.But now we have a component 'pc-items-table', which enfolds higher abstractions.For better managability we should reimplement this table with&nbsp;pc-items-table.",no,,[],,
IGNITE-8440,282,"Transaction may hangs on node in PREPARED state.In some specific cases we can to see when transaction hangs on one node in <tt>PREPARED state, but does not hang in others.That unhappy node waiting to get <tt>TxFinishRequest, but never got it and continue to print <em>long running transaction message</em>.Should to check other nodes, when transaction hang on PREPARED state without progress.",yes,transaction hangs,[sk2_block_hang_crash],,
IGNITE-8497,283,"Ignite always stops the node in the middle of checkpointing upon receiving a SIGINT.Steps Start Ignite server node with enabled PDS (see the attached <span class=""nobr"">
    <URL></span> config file) Activate the cluster with <em>./bin/control.sh --activate</em> Put some data into cluster (with <em>CachePutGetExample.java</em> for example) Stop Ignite server node with SIGINT 
  Actual result Ignite server node invokes the shutdown hook, checkpoint procedure starts, but Ignite node does not wait for checkpoint to finish and terminates the node.An excerpt from <span class=""nobr"">
    <URL></span> : 
  <SOURCECODE> 
  When one starts the node again, the following warning appears in the log ( <span class=""nobr"">
    <URL></span> ): 
  <SOURCECODE> 
  Note Same behavior when stopping a server node via ""Ignition.stop(true)"" call.",no,,[],,
IGNITE-8549,284,Include SSL related parameters for JDBC/ODBC.DRIVER={Apache Ignite};ADDRESS=13.56.76.37:9954;USER=ignite;PASSWORD=1mSBcc8loX;SSL_MODE=require;SSL_KEY_FILE=client.pem;SSL_CERT_FILE=client.pem;SSL_CA_FILE=client.pem,no,,[],,
IGNITE-8559,285,"WAL rollOver can be blocked by WAL iterator reservation.I've got the following thread dump from one of the Ignite nodes (only meaningful threads are kept for simplicity) WAL archiver is waiting for locked segment release TX commit is waiting for WAL rollover WAL rollover is blocked by the archiver Exchange is blocked by TX commit 
  <SOURCECODE> 
  On the first glance, the most robust fix would be to fix iterator not to lock segments, but prefetch the necessary amount of data from files.",yes,prefetch unnecessary,"[sk2_block_hang_crash, sk1_negative_necessary, sk4_prefetch, tk6_iteration]",,
IGNITE-8562,286,"Turn system-critical Ignite threads into GridWorkers.The goal of the change is to make system-critical threads (in terms of IEP-14, 
   <ERROR></ERROR>) available to <tt>WorkersControlMXBean.<ERROR></ERROR> 
   <URL>",no,,[],,
IGNITE-8651,287,VisorTxTask fails then printing transactions having implicit single type..org.apache.ignite.internal.processors.cache.distributed.near.GridNearTxLocal#mappings returns null for IgniteTxMappingsSingleImpl,no,,[],,
IGNITE-8689,288,"SQL query execution may lead to NullPointerException while node is stopped.Let's consider the following scenario: 
   
   Start a new node (node 'A') and create a new partitioned cache that resides on that node 
   
  <SOURCECODE> 
   
   Start the second node (node 'B')&nbsp;with a custom connector configuration: 
   
  <SOURCECODE> 
   
   Execute simple SQL query using sqlline&nbsp;for example (JDBC driver should be connected to the&nbsp;node 'B') 
   
  <SOURCECODE> 
  In that case, <tt>IgniteH2Indexing.prepareStatement() throws <tt>SQLException(Table not found) and the implementation (see <tt>IgniteH2Indexing.prepareStatementAndCaches()) tries to start caches that are not started yet by sending <tt>ClientCacheChangeDummyDiscoveryMessage to 'discovery-worker' thread, which in turn posts that message to 'exchange-worker' thread.Assume that while processing of <tt>ClientCacheChangeDummyDiscoveryMessage by the 'exchange-worker', the discovery thread receives <tt>EVT_NODE_FAILED (as a result of segmentation) and so <tt>DiscoCache history is updated by removing the failed node from the list of alive nodes.At the same time, 'exchange-worker' detects that there is only one alive node (node 'B' in our case) and mistakenly believes that node 'B' is a coordinator: 
  <SOURCECODE> 
  and results in the following <tt>NullPointerException: 
  <SOURCECODE> 
  As a result, the node cannot be stopped due to the following reasons: 
   
   'exchange' thread throws <tt>NullPointerException and therefore does not complete <tt>DynamicCacheStartFuture 
   'Client connector' thread is blocked on <tt>DynamicCacheStartFuture.get() method which never returns control 
   the thread which performs node stopping process is blocked on <tt>busyLock 
   
  &nbsp;Please see the following thread dump: 
  <SOURCECODE>",no,,"[sk2_block_hang_crash, ss3_in_one_go]",,
IGNITE-8700,289,"Support Uid/Pwd fields for authentication in DSN connection strings.I think Ignite should support Uid connection string field in addition to existing User, and Pwd in addition to Password.They should be interchangeable.This will improve user experience of tool users trying our ODBC.I have reviewed several ODBC implementation for popular data bases WRT authentication.<URL> uses Uid for username and Pwd for password 
  
   <URL> seems to prefer User Id and Password but also supports Uid and Pwd.<URL> uses UID and PWD.<URL> User and Password, but 
   <URL> Uid and Pwd.<URL> uses Uid and Pwd.In short, everybody seems to support Uid and Pwd even if they have their own field names.Many tools (such as Informatica) rely on this convention, translating GUI inputs into Uid and Pwd connection parameters.",no,,[],,
IGNITE-8805,290,"Web console: use TypeScript for E2E tests.TestCafe 
   <URL>.Writing E2E fixtures in TS will be a good stepping stone towards TS migration, so I suggest to investigate how this will work out with our setup and, if no major issues occur, convert current set of fixtures to TS.Who knows, it might be as easy as renaming the files.",no,,[],,
IGNITE-8880,291,"Add setIgnite() in SpringCacheManager and SpringTransactionManager.Neet to add setIgnite() in SpringCacheManager and SpringTransactionManager to make explicit injection of Ignite instance.For more details refer: 
   <URL>",no,,[],,
IGNITE-8926,292,Deadlock in meta data registration.Please file the attached jstack&nbsp;file with a deadlock.,no,,[],,
IGNITE-9057,293,"Update JCraft dependency version in Apache Ignite.Update ignite-ssh dependency version of com.jcraft:jsch to 0.1.54 
  <SOURCECODE> 
  It is required to run TC tartget RunAll to check all tests passed, it is required to build fabric using DEVNOTES.txt to make sure full build will be successfull 
  Probably it is required to run release step to make sure release candidate can be prepared.",no,,[],,
IGNITE-9077,294,"Metastorage content examination to ensure binary compatibility.Content of metastorage needs examination: all classes that get saved there should be reviewed to make sure binary compatibility between versions.As part of 
   <URL> was made a change to fix scenario of switching from ZookeeperDiscoverySpi to TcpDiscoverySpi.This fix has broken binary compatibility of metastorage when upgrading from 2.5 version to 2.6 version.Work-around is to clean up metastorage of old version, start cluster of new version and activate it back.So all classes that go to metastorage should be reviewed to prevent getting unexpected classes into metastorage.",no,,[],,
IGNITE-9213,295,"CacheLockReleaseNodeLeaveTest.testLockTopologyChange hangs sometimes, leading to TC timeout.Probability is quite low, &lt; 5%.One thread gets stuck in GridCacheAdapter.lockAll(...), holding gw readlock and waiting for future that never completes.Another one cannot acquire gw writelock.<SOURCECODE>",yes,unnecessary timeout expiration due to low probability,"[pf1_percentage, sk8_timeout_expiration, sk2_block_hang_crash]",,
IGNITE-9224,296,MVCC SQL: Cache metrics.We need to support <tt>CacheMetrics API for MVCC caches.Semantics should conform JCache specification.Only committed stores and removes should be counted.,no,,[],,
IGNITE-9228,297,"Spark SQL Table Schema Specification.The Ignite Spark SQL interface currently takes just ????????????le name???????????? as a parameter which it uses to supply a Spark dataset with data from the underlying Ignite SQL table with that name.To do this it loops through each cache and finds the first one with the given table name 
   <ERROR></ERROR>.This causes issues if there are multiple tables registered in different schema with the same table name as you can only access one of those from Spark.We could either: 
  1.Pass an extra parameter through the Ignite Spark data source which optionally specifies the schema name.2Support namespacing in the existing table name parameter, ie ????????????emaName.tableName???????????? 
  
   <ERROR></ERROR>https://github.com/apache/ignite/blob/ca973ad99c6112160a305df05be9458e29f88307/modules/spark/src/main/scala/org/apache/ignite/spark/impl/package.scala#L119",no,,"[ss15_each_one, tk6_iteration]",,
IGNITE-9283,298,"[ML] Add Discrete Cosine preprocessor.Add 
   <URL> 
  Please look at the MinMaxScaler or Normalization packages in preprocessing package.Add classes if required 
  1) Preprocessor 
  2) Trainer 
  3) custom PartitionData if shuffling is a step of algorithm 
  &nbsp; 
  Requirements for successful PR: 
  <ol> 
   PartitionedDataset usage 
   Trainer-Model paradigm support 
   Tests for Model and for Trainer (and other stuff) 
   Example of usage with small, but famous dataset like IRIS, Titanic or House Prices 
   Javadocs/codestyle according guidelines 
  </ol>",no,,[],,
IGNITE-931,299,Support user class loaders for PortableMarshaller.IgniteCacheAbstractExecutionContextTest.testUserClassLoader() fails with PortableMarshaller enabled.The reason is that the loader is not passed to required places when needed.Look for corresponding TODOs in the code.,no,,[sk1_negative_necessary],,
IGNITE-9382,300,Node.js fails to process large payloads.Looks like finalizing response is done too early.This can be easily checked with an SQL <em>limit</em> 1 and <em>limit 100</em>.,no,,[],,
IGNITE-9394,301,"MVCC: pds corrupted after grid restart..Ignite fails to apply unfinished checkpoint after restart.See stacktrace below.java.lang.AssertionError:  Assertion error on row: CacheObjectImpl 
   <ERROR></ERROR>CacheObjectImpl 
   <ERROR></ERROR>, expireT at org.apache.ignite.internal.processors.cache.persistence.tree.BPlusTree.doPut(BPlusTree.java:2286) at org.apache.ignite.internal.processors.cache.persistence.tree.BPlusTree.put(BPlusTree.java:2221) at org.apache.ignite.internal.processors.cache.IgniteCacheOffheapManagerImpl$CacheDataStoreImpl.update(IgniteCacheOffheapManagerImpl.java:23 at org.apache.ignite.internal.processors.cache.persistence.GridCacheOffheapManager$GridCacheDataStore.update(GridCacheOffheapManager.java:16 at org.apache.ignite.internal.processors.cache.IgniteCacheOffheapManagerImpl.update(IgniteCacheOffheapManagerImpl.java:438) at org.apache.ignite.internal.processors.cache.persistence.GridCacheDatabaseSharedManager.applyUpdate(GridCacheDatabaseSharedManager.java:24 at org.apache.ignite.internal.processors.cache.persistence.GridCacheDatabaseSharedManager.applyLastUpdates(GridCacheDatabaseSharedManager.ja at org.apache.ignite.internal.processors.cache.persistence.GridCacheDatabaseSharedManager.restoreState(GridCacheDatabaseSharedManager.java:1 at org.apache.ignite.internal.processors.cache.persistence.GridCacheDatabaseSharedManager.beforeExchange(GridCacheDatabaseSharedManager.java at org.apache.ignite.internal.processors.cache.distributed.dht.preloader.GridDhtPartitionsExchangeFuture.distributedExchange(GridDhtPartitio at org.apache.ignite.internal.processors.cache.distributed.dht.preloader.GridDhtPartitionsExchangeFuture.init(GridDhtPartitionsExchangeFutur at org.apache.ignite.internal.processors.cache.GridCachePartitionExchangeManager$ExchangeWorker.body0(GridCachePartitionExchangeManager.java at org.apache.ignite.internal.processors.cache.GridCachePartitionExchangeManager$ExchangeWorker.body(GridCachePartitionExchangeManager.java: at org.apache.ignite.internal.util.worker.GridWorker.run(GridWorker.java:110) at java.lang.Thread.run(Thread.java:745) Caused by: java.lang.AssertionError at org.apache.ignite.internal.processors.cache.tree.AbstractDataLeafIO.storeByOffset(AbstractDataLeafIO.java:67) at org.apache.ignite.internal.processors.cache.tree.AbstractDataLeafIO.storeByOffset(AbstractDataLeafIO.java:35) at org.apache.ignite.internal.processors.cache.persistence.tree.io.BPlusIO.store(BPlusIO.java:183) at org.apache.ignite.internal.processors.cache.persistence.tree.io.BPlusIO.insert(BPlusIO.java:270) at org.apache.ignite.internal.processors.cache.persistence.tree.BPlusTree$Put.insertSimple(BPlusTree.java:3395) at org.apache.ignite.internal.processors.cache.persistence.tree.BPlusTree$Put.insert(BPlusTree.java:3377) at org.apache.ignite.internal.processors.cache.persistence.tree.BPlusTree$Put.access$2500(BPlusTree.java:3257) at org.apache.ignite.internal.processors.cache.persistence.tree.BPlusTree$Insert.run0(BPlusTree.java:435) at org.apache.ignite.internal.processors.cache.persistence.tree.BPlusTree$Insert.run0(BPlusTree.java:416) at org.apache.ignite.internal.processors.cache.persistence.tree.BPlusTree$GetPageHandler.run(BPlusTree.java:5594) at org.apache.ignite.internal.processors.cache.persistence.tree.BPlusTree$GetPageHandler.run(BPlusTree.java:5579) at org.apache.ignite.internal.processors.cache.persistence.tree.util.PageHandler.writePage(PageHandler.java:346) at org.apache.ignite.internal.processors.cache.persistence.DataStructure.write(DataStructure.java:276) at org.apache.ignite.internal.processors.cache.persistence.tree.BPlusTree.access$12300(BPlusTree.java:87) at org.apache.ignite.internal.processors.cache.persistence.tree.BPlusTree$Put.tryInsert(BPlusTree.java:3569) at org.apache.ignite.internal.processors.cache.persistence.tree.BPlusTree$Put.access$7600(BPlusTree.java:3257) at org.apache.ignite.internal.processors.cache.persistence.tree.BPlusTree.putDown(BPlusTree.java:2531) at org.apache.ignite.internal.processors.cache.persistence.tree.BPlusTree.doPut(BPlusTree.java:2250) ... 14 more",no,,[ss11_rework],,
IGNITE-9402,302,Don't throw exception when flush was failed.IgnitePdsDiskErrorsRecoveringTest.testRecoveringOnWALWritingFail2 failed because it can lost last WAL data which have not flushed yet.,no,,[],,
IGNITE-9412,303,[ML] GDB convergence by error support..We need to support early training interruption when GDB has small error rate on learning sample,no,,[],,
IGNITE-9501,304,"Exclude newly joining nodes from exchange latch .Currently, we're waiting for latch completion from newly joining nodes.However, such nodes don't have any updates to be synced on wait partitions release.Newly joining nodes may start their caches before exchange latch creation and this can delay exchange process.We should explicitly ignore such nodes and don't include them into latch participants.",no,,[],,
IGNITE-9520,305,"Investigate fuzzy free lists.We have several data structures (free list, reuse list) associated with each partition.For these structures a major part of their state is maintained on-heap and persisted during checkpoints.This yields a lot of random disk accesses during checkpoints which significantly increases checkpoint mark phase (done under checkpoint write lock and essentially blocks all tx ops on the node).Need to investigate if we can implement some sort of a data structure which is updated lazily and may be out-of date, then we can update these data structures outside of checkpoint mark phases.",no,,[sk2_block_hang_crash],,
IGNITE-9530,306,MVCC TX: Local caches support..Mvcc support for local caches is turned off now.We need to consider implementing it in the future.,no,,[],,
IGNITE-9617,307,Different input possibilities for services page.We should improve usability.Currently branch field requieres full TeamCity branch name like on index page.It should accept PR number too.Same about JIRA ticket field.It should accept "IGNITE-XXX" and number only.,no,,[],,
IGNITE-9688,308,"MVCC: Implement out-of-order enlist optimization for bulk cache operations..For now, we always enlist updates&nbsp;in given order&nbsp;via setting ""GridNearTxEnlistFuture.sequential"" flag to true.This flag is always true for query updates as we do not know full data set at a time future has been created.For putAll (and other batch operations) full update map is known and we can&nbsp;make per-node batches full filled before send.E.g. we can send batches to nodes one by one regarding&nbsp;primary&nbsp;node order.This optimization has to be disscussed.",no,,"[ss13_cache, ss7_nn_by_nn]",,
IGNITE-971,309,"IgniteCacheQueryOffheapEvictsMultiThreadedSelfTest fails on TC.IgniteCacheQueryOffheapEvictsMultiThreadedSelfTest fails on TC.Do not forget to uncomment test in the suite when fixed.Example of error: 
  <SOURCECODE>",no,,[],,
IMPALA-1006,310,"Nested union all with an order by column that is not in the outer select list fails..The following query (using the functional.alltypesagg table) fails: explain select int_col from (select int_col, bigint_col from alltypesagg union all (select int_col, bigint_col from alltypesagg) order by bigint_col limit 10) A 
  
   <ERROR></ERROR> &gt; explain select int_col from (select int_col, bigint_col from alltypesagg union all (select int_col, bigint_col from alltypesagg) order by bigint_col limit 10) A; Query: explain select int_col from (select int_col, bigint_col from alltypesagg union all (select int_col, bigint_col from alltypesagg) order by bigint_col limit 10) A ERROR: IllegalStateException: null",no,,[],,
IMPALA-1016,311,Projecting only the column from adding a column causes the parquet scanner to return infinite values..1create table schema_test2 (string1 string) stored as parquet; 2.insert into schema_test2 values ('test'); --&gt; all queries work fine after this 3.alter table schema_test2 add columns (string2 string); 4.select * from schema_test2 --&gt; works fine 5.select string2 from schema.test2 -&gt; prints "NULL" unstoppedly 6.select distinct string2 from schema.test2 -&gt; query hangs,no,,"[sk11_infinity, sk2_block_hang_crash]",,
IMPALA-1113,312,Query with left semi join followed by inner join behaves non-deterministically.The following query sometimes returns 4 rows and sometimes does not return any results (0 rows):<SOURCECODE>.Returned 4 row(s) in 0.13 seconds.<ERROR>,no,,[pf2_duration],,
IMPALA-112,313,"Wrong result Double value using ""insert overwrite ~ select"".Wrong result of double value returns after executing ""insert overwrite ~ select"" in Impala while it doesn't run into the trouble using Hive.Correct value should be returned as well as doing with Hive.To reproduce this issue, follow the below steps: 
  <SOURCECODE> 
  This may return the correct value as ""12.34567"".<SOURCECODE> 
  This would returns the wronge value as ""12.3457"".It might be a bug.Meanwhile, we can get the correct value if ""insert overwrite ~ select"" using HiveQL: 
  <SOURCECODE> 
  This returns the correct value as ""12.34567"".",no,,[],,
IMPALA-1172,314,"run-hbase.sh continues even if wait-for-hbase-master.py timeouts.testdata/bin/run-hbase.sh calls testdata/bin/wait-for-hbase-master.py to avoid some race.However, wait-for-hbase-master.py can timeout after 30 seconds and does exit(1).But, run-hbase.sh doesn't check the exit code and just continues as if wait-for-hbase-master.py succeeded in all cases.I've seen this happen (somehow something was bound to my hbase master socket and so the hbase master was failing to start) and it leads to things being broken downstream that are hard to diagnose.It'd be better if we exit when the failure happens rather than continuing.That would make problems a bit easier to diagnose.Ishaan, I'll start with you but feel free to reassign as appropriate.",no,,"[tk2_bound, sk8_timeout_expiration, pf2_duration]",,
IMPALA-1183,315,"Crash: Analytic with GROUP BY.The query below crashes Impala 
  <SOURCECODE> 
  stack trace 
  <SOURCECODE> 
  Postgres result 
  <SOURCECODE> 
  The git hash of the build used is  
  commit 788b027439a03a1cc3378ff0191487577608e8b7",no,,[],,
IMPALA-1234,316,"Empty result set from a view in union fails planning or produces wrong results.When a stmt in a union has an inline view that has an empty result set, we produce an EmptySetNode for the wrong QueryStmt.In some cases we produce the wrong result, in others Planning fails with an IllegalStateException.For example, 
  <SOURCECODE> 
  Should produce: 
  <SOURCECODE> 
  (The 0 is the result of the count() and the 1s are from the second select stmt.)In the following example with an analytic expression in the second select stmt planning fails: 
  <SOURCECODE> 
  It seems that in this scenario with the inline view, we set hasEmptyResultSet_ on the analyzer for the UnionStmt but it isn't handled correctly with the inline view (which has a different analyzer).",no,,[],,
IMPALA-1243,317,Incorrect plan in analytic using inline view.The predicate "t1.smallint_col IS NULL" in the query below incorrectly gets pushed down to the scan.Also it looks like the "t1.month IS NOT NULL" predicate was dropped.<SOURCECODE>,no,,[],,
IMPALA-1292,318,"Incorrect result in analytic SUM when ORDER BY column is null.If multiple order by columns are used in an analytic SUM (and possibly other functions) and the left column has null values and the right columns does not, the result is not correct.The two queries below are the same except the top shows the first 10 rows and the bottom shows the last 10 rows.The results start off correct when the values are not null, but the bottom results are incorrect.<SOURCECODE> 
  Postgresql and Oracle both agree that the bottom result should be 
  <SOURCECODE>",no,,[],,
IMPALA-1310,319,"hdfs-parquet-table-writer.cc:818] Check failed: file_size_limit_ > DATA_PAGE_SIZE * columns_.size() (6291456 vs. 131072000).Decreasing the default parquet file size exposed a preexisting bug where very wide tables could get tripped up because the parquet writer code assumes there is at least a few DATA_PAGE_SIZE bytes available per column.F0925 05:07:48.850270 20556 hdfs-parquet-table-writer.cc:818] Check failed: file_size_limit_ &gt; DATA_PAGE_SIZE * columns_.size() (6291456 vs. 131072000)  
   
    
     
      
       
       Check failure stack trace: *** @ 0x23fdded google::LogMessage::Fail() @ 0x2401877 google::LogMessage::SendToLog() @ 0x2400dd6 google::LogMessage::Flush() @ 0x2401d0d google::LogMessageFatal::~LogMessageFatal() @ 0x195d116 impala::HdfsParquetTableWriter::InitNewFile() @ 0x18d6767 impala::HdfsTableSink::CreateNewTmpFile() @ 0x18d778a impala::HdfsTableSink::InitOutputPartition() @ 0x18daa3e impala::HdfsTableSink::GetOutputPartition() @ 0x18d7bdd impala::HdfsTableSink::Send() @ 0x181998a impala::PlanFragmentExecutor::OpenInternal() @ 0x1818a77 impala::PlanFragmentExecutor::Open() @ 0x17a69da impala::Coordinator::Wait() @ 0x11db6d4 impala::ImpalaServer::QueryExecState::WaitInternal() @ 0x11db0ce impala::ImpalaServer::QueryExecState::Wait()",no,,"[ss1_one_per, sk3_byte]",,
IMPALA-1325,320,Impala shell shows no error when an error message includes "Cancelled".I've seen this 3 times.Occasionally when impalad crashes the shell will give not show an error and instead return to the prompt as though nothing happened.There is no workaround.The first query below crashed an impalad on a different host but no error message was shown.<SOURCECODE>,no,,[],,
IMPALA-1326,321,"Codegen: Crash running agg with LEFT JOINS (incorrect result w/out codegen).With codegen disabled the query runs but the result is wrong.With codegen there is a crash.Let me know if a separate issue should be opened for the incorrect result.<SOURCECODE> 
  If no aggregation is done or the joins are INNER the query runs without crashing.Postgresql says the result should be 8 
  <SOURCECODE> 
  I tried getting a good stack trace several times but it is always 
  <SOURCECODE> 
  hs_err may be more helpful 
  <SOURCECODE>",no,,[],,
IMPALA-1337,322,"Incorrect results in aggregate query grouping by CHAR/VARCHAR.The top two queries have incorrect results.It looks like there is some sort of data corruption.The bottom query using CHAR(5) is fine.<SOURCECODE> 
  Postgresql 
  <SOURCECODE>",no,,[],,
IMPALA-1348,323,"Failde DCHECK(eos) running query with NOT IN subquery.If the WHERE clause is removed from the query below, there is no crash.As the query is, some data is returned then Impala crashes.I dont see the rewritten query in the INFO log.<SOURCECODE> 
  stack trace 
  <SOURCECODE> 
  code 
  <SOURCECODE>",no,,[ss11_rework],,
IMPALA-1365,324,"TPC-DS Queries (33,56,60) - FE Illegal State Exception null.The following queries fails with exception.The schema generation script for load-data.py is attached.Feel free to close or move if inappropriate.<SOURCECODE>",no,,[],,
IMPALA-1375,325,"typo in tests getting hdfs client.I noticed there was a typo in how the tests create an hdfs client when a user specifies an explicit namenode address, where the host and port were not being passed to the <tt>get_hdfs_client function.This patch fixes this typo.",no,,[],,
IMPALA-1411,326,"CREATE TABLE AS SELECT produces incorrect results.The following two queries should return identical results, but this is not the case.This query returns 1 row 
  <SOURCECODE> 
  This query returns 3 rows 
  <SOURCECODE>",no,,[],,
IMPALA-1422,327,"Allow constant exprs as the left-hand side of an IN subquery.The following two queries should produce identical results, but this is not the case.Both queries use the functional database.This query seems to produce correct results.<SOURCECODE> 
  In this query, we replace (smallint_col = double_col) with TRUE, and Impala throws AnalysisException.<SOURCECODE> 
  Workaround Wrap the TRUE constant in an inline view.To fix the example above: 
  <SOURCECODE>",no,,[],,
IMPALA-1427,328,"Improve ""unknown disk id"" warning messages.The warning that users see when volume mapping information is not available is not very clear, and can be repeated many, many times: 
  <blockquote> 
    ""Unknown disk id.This will negatively affect performance. """"Check your hdfs settings to enable block location metadata.</blockquote> 
  We should improve this to a) not print out so frequently b) point more directly to the setting to enable block data c) more clearly explain the problem.",yes,mapping repeated many many times is a waste of time,"[sk9_frequency, gk4_performance, sk2_block_hang_crash]",,
IMPALA-1431,329,"Add support for Partition Exchange.Hive supports partition exchange in a single statement: 
  alter table &lt;tablename&gt; exchange partition (&lt;(partial)?partition spec&gt;) with table &lt;tablename&gt; 
  
   <URL> 
  This is useful for Change Data Capture.",no,,[ss3_in_one_go],,
IMPALA-1454,330,"TPC-DS query 25 crashes.While running query 25 from TPC-DS it causes impala daemon to crash.The query, profile and jvm log is attached.Core file seems to be truncated.",yes,query crashes,[pf3_profiling],,
IMPALA-1467,331,"Query option parsing should support ""gb"" in addition to ""g"".set mem_limit=1g; set mem_limit=1gb; 
  Should both work.The second one does not currently.",no,,[tk3_memory_unit],,
IMPALA-1498,332,"Crash: impala::SlotRef::GetStringVal.QUERY: 
  <SOURCECODE> 
  STACK: 
  <SOURCECODE>",no,,[],,
IMPALA-1500,333,"Crash.QUERY: 
  <SOURCECODE> 
  STACK: 
  <SOURCECODE>",no,,[],,
IMPALA-1501,334,"Crash: impala::ThreadResourceMgr::ResourcePool::SetThreadAvailableCb.QUERY: 
  <SOURCECODE> 
  STACK: 
  <SOURCECODE> 
  DB: functional File Format: text/none",no,,[],,
IMPALA-1585,335,"Sorter constructor ignores returned status from function it calls.We should not have constructors that contain function calls that  return Status, which we ignore.For example, in sorter.cc: 
  <SOURCECODE>",no,,[],,
IMPALA-1618,336,"Impala server should always try to fulfill requested fetch size.The thrift fetch request specifies the number of rows that it would like but the Impala server may return fewer even though more results are available.For example, using the default row_batch size of 1024, if the client requests 1023 rows, the first response contains 1023 rows but the second response contains only 1 row.This is because the server internally uses row_batch (1024), returns the requested count (1023) and caches the remaining row, then the next time around only uses the cache.In general the end user should set both the row batch size and the thrift request size.In practice the query writer setting row_batch and the driver/programmer setting fetch size may often be different people.There is one case that works fine now though - setting the batch size to less than the thrift req size.In this case the thrift response is always the same as batch size.Code example: 
  <SOURCECODE>",no,,[],,
IMPALA-165,337,"add optimization for - select * from foo limit 5.In hive, the query ""select * from foo limit 5"" does not run a map reduce job and returns the results very quickly by reading the first block of the first file using the hive shell jvm.In contrast, impala will run this query in fully distributed mode.An optimization could be put in so the coordinator only runs a one-node fragment to return these results.",no,,"[sk2_block_hang_crash, gk7_speed, gk3_reduce]",,
IMPALA-1656,338,"Remove block_mgr_client_ !#NAME?In various places in BE's code there are checks like the one below that use the block_mgr_ only if this is set.This is because some tests do not setup the whole ExecEnv.But that adds complexity and branches.We should remove them, making sure that all tests have a properly setup environment (and DCHECK_NOTNULL(block_mgr_client_)).<SOURCECODE>",no,,"[sk2_block_hang_crash, gk2_complexity]",,
IMPALA-1703,339,"Date/Time functions are very slow.Without string function (to_date()): 
  <SOURCECODE> 
  With string function (to_date()): 
  <SOURCECODE>",yes,functions are very slow,[gk7_speed],,
IMPALA-1732,340,"Impala should have more generic functions DATEDIFF(datepart, startdate, enddate), DATEADD(datepart, number, date).I guess it should be like implementation in MS SQL: 
  DATEDIFF(datepart, startdate, enddate) DATEADD(datepart, number, date) 
   <URL> 
   <URL>",no,,[pf3_profiling],,
IMPALA-1737,341,"invalid tuple_idx when combining INSERT INTO with analytic subquery.The following query is producing invalid tuple_idx when executing.If the INSRET INTO is changed into a CREATE TABLE AS statement it executes fine.<SOURCECODE> 
  Workaround Force expression materialization via a UNION ALL as follows: 
  <SOURCECODE>",no,,[],,
IMPALA-1741,342,"Error when decimal value in WHERE clause is longer than 19 digits.When I try to run a query with WHERE clause and value for a column in DECIMAL data type is too long (more than 19 digits), then I get following error message: 
  <font color=""red""></font>
  <SOURCECODE>
   
  i.e. 
  Lets say we have table TTT (column VAL defined as DECIMAL(38,0)): 
  <div class=""table-wrap""> 
   <table class=""confluenceTable"">
    <tbody> 
     <tr> 
      <th class=""confluenceTh"">VAL</th> 
     </tr> 
     <tr> 
      <td class=""confluenceTd"">1234567890123456789</td> 
     </tr> 
     <tr> 
      <td class=""confluenceTd"">12345678901234567890</td> 
     </tr> 
    </tbody>
   </table> 
  </div> 
  Then if I will run: 
  <SOURCECODE> 
  Returns correct result 
  <br class=""atl-forced-newline""> 
  <SOURCECODE> 
  Gives: <font color=""red""></font>
  <SOURCECODE>
   
  <br class=""atl-forced-newline""> The only workaround I found is to cast the value: 
  <SOURCECODE> 
  But I think this is not good.<em>N.B. Hive performs casting on fly, but you need to put your value in quotes.</em>",no,,[],,
IMPALA-1782,343,CREATE_FUNCTION for unknown DB does not fail..CREATE_FUNCTION operation using the catalog service thrift API returns a success status code even when the DB for which the function is being created is unknown to the catalog service (but present in metastore).Note that no function is created though.,no,,[],,
IMPALA-1813,344,"CREATE TABLE LIKE ... STORED AS AVRO does not work..The create statement works fine but the resulting table is unusable.repro: 
  <SOURCECODE>",no,,[],,
IMPALA-1847,345,"Redact sensitive information from graphical representation of plan tree in Web UI..The graphical representation of the plan tree could contain sensitive information that also needs to be redacted.For example, the hash exprs of a partitioned join or a grouping aggregation are displayed.They could contain sensitive information.",no,,[],,
IMPALA-191,346,"Support /* */ comments.It is very common for command shells for Oracle or MySQL databases to support both - single-line and /* / multi-line comment notation.'impala-shell -f blah.sql' currently gives 'unrecognized command' for /* */ comments.It should silently ignore them.(It does  
  For example, the basic MySQL sample code here: 
   <URL> immediately gives a bunch of 'unrecognized command' errors due to a mix of - and /* */ comments at the top of the file, regardless of whether the actual SQL command syntax is compatible or not.Based on the pattern of error messages, it seems like Impala really does ignore whatever is in between the /* / delimiters, so it might just be a case of spurious messages when it actually encounters / and */.",no,,[],,
IMPALA-1910,347,"Significant memory leak in HashJoin Node.Here's the leak: 
  Log line format: 
   <ERROR></ERROR>mmdd hh:mm:ss.uuuuuu threadid 
   <URL>] msg F0320 20:41:54.770300 28819 exec-node.cc:175] Check failed: mem_tracker()-&gt;consumption() == 0 (1539309568 vs. 0) Leaked memory.Fragment af4cf08b2927df82:dc517e368ba78aa4: Consumption=1.43 GB AGGREGATION_NODE (id=3): Consumption=0 HASH_JOIN_NODE (id=2): Consumption=1.43 GB EXCHANGE_NODE (id=4): Consumption=0 EXCHANGE_NODE (id=5): Consumption=0 DataStreamSender: Consumption=16.00 KB 
  The leak is a lot more than what's reported in 
   <URL>.The query that caused the leak is in this form: 
  select * from small_tbl A, big_tbl B where A.id=B.id 
  The per query mem_limit is set to ~1.5 gb.We can use tpc-ds to try to repro it.",yes,significant memory leak,"[ss1_one_per, ss4_memory, tk3_memory_unit]",,
IMPALA-1946,348,"Outer joins on a view that have ON-clause predicates only referencing the view can cause a crash..This bug can happen in the following scenario: 1.There is an outer join on a catalog view or inline view 2.The outer join has a predicate in the On-clause only referencing the view 3.The view definition references multiple tables 4.The On-clause predicate returns a non-NULL value even when all its column values are NULL 
  A possible indicator of this bug is when there is a ""TupleIsNull"" inside a predicate of a scan node (see explain plan below).The stack of the crash will look similar to this: 
  <SOURCECODE> 
  Here's a query to repro: 
  <SOURCECODE> 
  The explain plan hints at the problem: 
  <SOURCECODE> 
  The problem here is that the TupleIsNullPredicate checks the tuples 1 and 2.However, scan node 01 does not materialize tuple 2.",no,,[],,
IMPALA-1988,349,"show column stats returns different results for beeswax and hs2.In the impala shell show column stats compute_stats_db.alltypes functions as expected; stats are returned.When the same command is executed through impyla using hs2, the second to last column, Max Size, is always of None type.To reproduce: in the Impala shell <SOURCECODE>.For those unfamiliar with impyla:
fetchall() return the query results; each tuple is a row.description returns the column labels and types, e.g. the first column is named Column and is of type string.",no,,[],,
IMPALA-1995,350,"Flaky test: PlannerTest.testHbase: splits for HBASE KEYRANGE not set up correctly..Looks like testdata/bin/split-hbase.sh does not always set up the ranges in the way we want.See failure below.Example run: 
   <URL> 
  <SOURCECODE>",no,,[],,
IMPALA-2018,351,"Where clause does not propagate to joins inside nested views.We have run into a situation where nested views will cause results to be inconsistent with the results of running the queries in the views directly.To reproduce... In Impala - Create tables and some data 
  <SOURCECODE> 
  In Bash - Create more data 
  <SOURCECODE> 
  In Impala - Create views and demonstrate bad query 
  <SOURCECODE> 
  In the version using the two views, we get null results.Broken Explain Results 
  <SOURCECODE> 
  Working Explain Results 
  <SOURCECODE> 
  The important difference there is that in the working version, ""other predicates: n.name = 'Customer A'"" shows up in the hash join.I believe that impala isn't pushing the predicates into the join the way it needs to to give proper results in this situation.The view works consistently in hive.",no,,[],,
IMPALA-2063,352,"impala 'cancelled' query status now has extra characters in it.When a query is cancelled (e.g. via Beeswax.cancel()), the query status has a trailing newline.This is visible in the query profile, see sample images.The trailing newline should be removed.",no,,[pf3_profiling],,
IMPALA-2069,353,"Cannot write string column to parquet with UTF-8 annotation.The Impala Parquet writer always creates an unannotated binary field for string columns.It should be possible to specify that the field include the UTF-8 annotation to indicate UTF-8 string data.CHAR and VARCHAR columns are written with the UTF-8 annotation, so a possible workaround is to cast string values to VARCHAR.",no,,[],,
IMPALA-211,354,"Running impala on 'localhost' causes log spew with GLOG_V=2+ @ simple-scheduler.cc:160 - ""Only localhost addresses found for ___"".Running impala on 'localhost' causes log spew with GLOG_V=2 @ simple-scheduler.cc:160: 
  <SOURCECODE>",no,,[],,
IMPALA-2142,355,"Type conversion slows down query reading ""null"" from column.Scanning data from table which has a DOUBLE column, and underlying data for that column has ""null"" , is very slow due to type conversion.We need to improve this.<SOURCECODE>",yes,slowness,[gk7_speed],,
IMPALA-2156,356,"Specify and test full lifecycle for expressions.<tt>expr-context.h does not fully specify what methods on an <tt>ExprContext are legal to call, when.For example, is it legal to <tt>Close() an <tt>ExprContext that has not yet been <tt>Prepare()'d?Can we call <tt>Open() twice?What state is copied when <tt>Clone() is called?Without this, it's hard to write robust error handling in objects that use <tt>ExprContext instances.It is also hard to write expressions that are correct in all uses.We should update the comments in <tt>expr-context.h to define the invariants required of and guarantees made to <tt>ExprContext clients.We can then add tests that exercise the legal lifecycles of all expressions to root out any bugs, and add invariant checks in the code that will catch bad clients.",no,,[],,
IMPALA-2187,357,"ASAN build fails in test_authorization.py with ""ValueError: bad marshal data"".Casey, assigning to you since Dimitris mentioned you were already looming into it.Feel to re-assign as appropriate.Failed build: 
   <URL> 
  Relevant snippet from the console output: 
  <SOURCECODE>",no,,[ss11_rework],,
IMPALA-2192,358,"Wrong results on TPCH-Q11 during nightly data load.Build: 
   <URL> 
  Relevant console snippet: 
  <SOURCECODE> 
  I strongly suspect this is what ASAN is telling us in 
   <URL>, but I'll leave this issue separate just to be sure.Dimitris, based on the timing and the query it looks like your nested-loop join patch may have something to do with this.Feel free to re-assign if you think otherwise.",no,,"[ss8_load_nn, ss11_rework]",,
IMPALA-2224,359,"Impala doesn't reject invalid timestamp format string.Impala only check if each character in the format string is valid or not, doesn't check if the whole string is invalid or not.for example, it will run the following query and produce result.<SOURCECODE>",no,,[],,
IMPALA-2248,360,"Make idle_session_timeout a query option.We currently have a process-wide session timeout (see --idle_session_timeout), but it would be useful to also have the ability to set per-session timeouts.We can do this by taking a parameter in the HS2 OpenSession() call (it already has a map of properties) and changing the session timeout logic in impala-server.cc.This could be set to a lower value by some clients, e.g. Hue, that do not close queries automatically and want to ensure queries aren't left open for a long time without requiring that all clients have the same session timeout.",no,,"[ss2_all_to_one, sk8_timeout_expiration]",,
IMPALA-2253,361,"setting request_pool query option should not override query to pool rules.Setting the query option request_pool should not override the query to queue rules specified in the fair-scheduler.xml config.For example, if the fair-scheduler.xml has only the rule ""user"" rule specified, the queue should always be ""root.&lt;user&gt;"", but if the ""request_pool"" query option is set, that will be used.",no,,[],,
IMPALA-2259,362,"Fn resolution requiring casting may cause wrong LAG()/LEAD() overload to be used.When using LAG() or LEAD() on a DECIMAL column and the offset index is explicitly specified but not a BIGINT, the planner chooses the wrong LAG/LEAD overload taking a FLOAT, i.e. the DECIMAL column is cast to a FLOAT.For example: 
  <SOURCECODE> 
  The registered lag() functions for DECIMAL are: 
  <SOURCECODE> 
  This is happening due to the way we resolve functions when casting is involved.In this case, when the 2-parameter function is used and the 2nd parameter needs to be cast from INT to BIGINT, we end up casting the first parameter as well.",no,,[],,
IMPALA-2276,363,"The Isilon, S3, and localFS builds should not attempt to do a full data load if presented with a stale snapshot..Currently, the builds which runs Impala tests against Isilon/S3 attempt to do a full data load if the available test data snapshot is stale.Since we don't run Hive on the Isilon and S3 builds, they should instead fail early with a clear message.",no,,[ss8_load_nn],,
IMPALA-2290,364,"BTrim() with non constant second argument is not thread safe..<SOURCECODE> 
  I think the fix is just to use THREAD_LOCAL.",no,,[],,
IMPALA-2321,365,"Impala should be more resilient to corrupted data.In lot of cases, scanner assumes the input data is in proper format and no corruption and use DCHECK to handle rare cases.This will cause Impala crash if the data file is corrupted.It's better to fail the query gracefully instead of using DCHECK, and provide details in error message.",no,,[],,
IMPALA-236,366,"impalad segfaults executing query that contains join and limit.impalad segfaults executing query that contains join and limit.It seems to be a race somewhere since the query actually completes successfully ~50% of the time.<SOURCECODE> 
  <SOURCECODE>",no,,[pf1_percentage],,
IMPALA-2439,367,"Crash in void boost::throw_exception<boost::lock_error>(boost::lock_error const&).The crash below happened while using the release build during the stress test (about 20 mins in).<SOURCECODE> 
  I'll post the log/core location when they are collected.",no,,[pf2_duration],,
IMPALA-2442,368,Document rebase process and cherry-pick sync from release process.Can put in branching process doc.That must also be updated.,no,,[],,
IMPALA-2484,369,"test_nested_types.py: test case for IMPALA-2376 sometimes fails.Occasionally, the test case for 
   <URL> fails.e.g.: 
  
   <URL> 
  The test is expecting a Memory Limit Exceeded failure, but occasionally the query apparently succeeds.To get the intended coverage, we don't actually need to query to always fail; it'd be good enough if it just usually fails.But I don't know if we have a way to express that.<URL>, you added this test case, so could you take a look?",no,,[ss4_memory],,
IMPALA-2541,370,"Implement ""group sessions"" operator in Impala.This would be great if we can implement ""group sessions"" feature in Impala.The feature is described in blow pages: 
   <URL> 
   <URL> For the first link, customer adds below clarification: Clarification #1 on above example would be the support for multiple/repeated start values.ie.1 A Aug 1, 2014  2 start Aug 2, 2014  2 start Aug 3, 2014  2 A Aug 4, 2014  2 A Aug 5, 2014  2 end Aug 6, 2014  3 B Aug 7, 2014  3 start Aug 8, 2014  3 end Aug 9, 2014  3 A Aug 10, 2014 Clarification #2 on above example - sequence could also be integer in form as well as date",no,,[],,
IMPALA-256,371,Add runtime profile counter for numbers of rows scanned in scan node.We currently only count the number of rows returned (after conjuncts).It would be very useful to know the rows scanned in the table before conjuncts.,no,,[pf3_profiling],,
IMPALA-2596,372,Mongo uploading failing in Jenkins jobs.<URL> Uploading build to reporting server.<ERROR>.,no,,[],,
IMPALA-2607,373,"S3: need to determine the right default for fs.s3a.connection.maximum.When Impala is scanning parquet files off S3, it appears we can get into a deadlock if fs.s3a.connection.maximum is not set high enough, because we can spread the connection use across files but unless we have a connection for all selected columns of a file, we can't make progress scanning that file.So, we need to figure out the right default for fs.s3a.connection.maximum for Impala, and also determine whether we need to put an upper bound on scanner concurrency given the number of connections.",no,,"[ss2_all_to_one, tk2_bound, tk7_concurrent]",,
IMPALA-263,374,Add sleep(int ms) builtin.We've talked about adding a sleep builtin that just sleeps for some amount of time.This would just be for testing.CM has asked for this to make it easier for them to generate longer running queries easily and in a controlled fashion.Should be easy enough to add.I think this should return a boolean type and return true when the sleep is over.,no,,[pf3_profiling],,
IMPALA-2684,375,"Fragment start latencies reported in wrong unit.The metric Fragment start latencies is reported in nanoseconds: 
   Fragment start latencies: count: 39, last: 0.078961ns, min: 0.011105ns, max: 0.541386ns, mean: 0.375923ns, stddev: 0.124721ns 
  According to my research, and I am probably wrong, but I think it should be reported in seconds (or code which divides it's value by 1000 * 1000 * 1000 should be removed).Coordinator uses MonotonicStopWatch here: 
  
   <URL> 
  which returns elapsed time in nanoseconds here: 
  
   <URL> 
  The problem occurs when elapsed time is converted to seconds here: 
  
   <URL> 
  despite the fact that the metric definition is already in nanoseconds here: 
  
   <URL>",no,,"[gk5_latency, pf2_duration]",,
IMPALA-2686,376,"Implement google BreakPad based crash handler for impala..At present, many crash analysis start with manual process of collecting gdb stack from crash core file.This jira is to request impala dev team to implement a Google BreakPad(or any other similar library) based crash handler for impala, so we get mini dumps when impala crashes.",no,,[sk2_block_hang_crash],,
IMPALA-2687,377,"Test failed due to read-your-writes metadata failure.See:  
  
   <URL> 
  <SOURCECODE> 
  But, from the Impala logs: 
  <SOURCECODE> 
  and the <tt>catalogd also saw the write: 
  <SOURCECODE>",no,,[],,
IMPALA-2689,378,"Log every time codegen is disabled due to NYI.Sometimes a function cannot be codegen'd, even with codegen enabled, because some functionality is not implemented (e.g. we cannot codegen the Avro scanner if the file schema does not match the table schema, certain types are not implemented for aggregates).We should always print a message to the log explaining why this is the case, otherwise it's very difficult to figure out why codegen couldn't be used for a certain query.We already log in some cases but not all.In addition, we could also put these reasons in the runtime profile.Currently we only log if codegen is enabled on an operator, but it would be also be useful to see if it was disabled and why.",no,,[pf3_profiling],,
IMPALA-2695,379,"Catalog stores URI in lowercase and cause sentry authorization fail.Create a directory with upper case then grant all URI permission to a role, then try to create table and set location to a subdir under the URI via Impala.create table will fail with ERROR: AuthorizationException: User 'systest@HALXG.CLOUDERA.COM' does not have privileges to access: hdfs://c2302.halxg.cloudera.com:8020/tmp/ABC/hh 
  step to reproduce: 
  <SOURCECODE> 
  Catalog is actually storing a lower cased version of uri when doing a grant.And also sending a lower cased version of uri when requesting privileges from Sentry.So looks like two things have to be fixed in Impala 1) Role.java -&gt; Change cache to case sensitive for privilege map.2) RolePrivilege.java -&gt; URIs should not be lower cased: 
   <URL>",no,,[ss2_all_to_one],,
IMPALA-2741,380,"Normalise view SQL in same way as Hive.Hive normalises view SQL in several ways for the VIEW_EXPANDED_TEXT metastore column.The column list, e.g. from select * is expanded and field names are normalised to include the database name and be enclosed in backticks.This does not appear to be documented and is essentially implementation-defined.Some DDL operations in Hive make assumptions about VIEW_EXPANDED_TEXT content and will crash if Impala generates something slightly different.Note that actually using the view in a query appears to work fine.We should consider modifying CREATE VIEW in Impala so that it generates VIEW_EXPANDED_TEXT that is normalised in a similar way to Hive to avoid some of these compatibility problems.The original bug report follows: 
  In Impala create table myTable (x int , y int); create view myView AS SELECT * FROM myTable; 
  in ""Hive"" SHOW TABLE EXTENDED FROM default LIKE 'myview'; --&gt; CRASH 
  To fix the issue we create the view in hive.We use aquadatastudio and the ""Hive"" tree browser crash because he try to run this query.SHOW TABLE EXTENDED FROM &lt;database&gt; LIKE `*` 
  Not very good option for us to ask all users to create all views only in ""Hive"" to avoid this crash.",no,,"[ss2_all_to_one, ss23_avoid_if]",,
IMPALA-2901,381,"Impala query using 100% CPU system state.I'm using impala query a table?????e nmon monitor cpu usage????? find the process impalad using 100% CPU system state????? then i use jstack attach the impalad process and use perf top tool monitor impalad process (reference attach files) ?????g's detail is below 
  Thread 139715: (state = IN_VM) 
  <ul class=""alternate"" type=""square""> 
   sun.misc.Unsafe.allocateMemory(long) @bci=0 (Interpreted frame) 
   java.nio.DirectByteBuffer.&lt;init&gt;(int) @bci=49, line=127 (Interpreted frame) 
   java.nio.ByteBuffer.allocateDirect(int) @bci=5, line=306 (Interpreted frame) 
   org.apache.hadoop.hdfs.util.DirectBufferPool.getBuffer(int) @bci=61, line=70 (Interpreted frame) 
   org.apache.hadoop.hdfs.BlockReaderLocal.createDataBufIfNeeded() @bci=20, line=244 (Interpreted frame) 
   org.apache.hadoop.hdfs.BlockReaderLocal.fillDataBuf(boolean) @bci=1, line=434 (Interpreted frame) 
   org.apache.hadoop.hdfs.BlockReaderLocal.readWithBounceBuffer(java.nio.ByteBuffer, boolean) @bci=147, line=504 (Interpreted frame) 
   org.apache.hadoop.hdfs.BlockReaderLocal.read(java.nio.ByteBuffer) @bci=135, line=386 (Interpreted frame) 
   org.apache.hadoop.hdfs.DFSInputStream$ByteBufferStrategy.doRead(org.apache.hadoop.hdfs.BlockReader, int, int, org.apache.hadoop.hdfs.DFSInputStream$ReadStatistics) @bci=26, line=706 (Interpreted frame) 
   org.apache.hadoop.hdfs.DFSInputStream.readBuffer(org.apache.hadoop.hdfs.DFSInputStream$ReaderStrategy, int, int, java.util.Map) @bci=14, line=740 (Interpreted frame) 
   org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(org.apache.hadoop.hdfs.DFSInputStream$ReaderStrategy, int, int) @bci=141, line=797 (Interpreted frame) 
   org.apache.hadoop.hdfs.DFSInputStream.read(java.nio.ByteBuffer) @bci=16, line=845 (Interpreted frame) 
   org.apache.hadoop.fs.FSDataInputStream.read(java.nio.ByteBuffer) @bci=18, line=144 (Interpreted frame) 
   
  Thread 139697: (state = IN_VM) 
  <ul class=""alternate"" type=""square""> 
   sun.misc.Unsafe.allocateMemory(long) @bci=0 (Interpreted frame) 
   java.nio.DirectByteBuffer.&lt;init&gt;(int) @bci=49, line=127 (Interpreted frame) 
   java.nio.ByteBuffer.allocateDirect(int) @bci=5, line=306 (Interpreted frame) 
   org.apache.hadoop.hdfs.util.DirectBufferPool.getBuffer(int) @bci=61, line=70 (Interpreted frame) 
   org.apache.hadoop.hdfs.BlockReaderLocal.createDataBufIfNeeded() @bci=20, line=244 (Interpreted frame) 
   org.apache.hadoop.hdfs.BlockReaderLocal.fillDataBuf(boolean) @bci=1, line=434 (Interpreted frame) 
   org.apache.hadoop.hdfs.BlockReaderLocal.readWithBounceBuffer(java.nio.ByteBuffer, boolean) @bci=147, line=504 (Interpreted frame) 
   org.apache.hadoop.hdfs.BlockReaderLocal.read(java.nio.ByteBuffer) @bci=135, line=386 (Interpreted frame) 
   org.apache.hadoop.hdfs.DFSInputStream$ByteBufferStrategy.doRead(org.apache.hadoop.hdfs.BlockReader, int, int, org.apache.hadoop.hdfs.DFSInputStream$ReadStatistics) @bci=26, line=706 (Interpreted frame) 
   org.apache.hadoop.hdfs.DFSInputStream.readBuffer(org.apache.hadoop.hdfs.DFSInputStream$ReaderStrategy, int, int, java.util.Map) @bci=14, line=740 (Interpreted frame) 
   org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(org.apache.hadoop.hdfs.DFSInputStream$ReaderStrategy, int, int) @bci=141, line=797 (Interpreted frame) 
   org.apache.hadoop.hdfs.DFSInputStream.read(java.nio.ByteBuffer) @bci=16, line=845 (Interpreted frame) 
   org.apache.hadoop.fs.FSDataInputStream.read(java.nio.ByteBuffer) @bci=18, line=144 (Interpreted frame)",no,,[pf1_percentage],,
IMPALA-2912,382,"buffered-block-mgr-test BE test failed.Tim, assigning to you since you touched that code/test last.<URL> 
  <SOURCECODE> 
  From the test logs: 
  <SOURCECODE>",no,,[],,
IMPALA-297,383,Remove distinction between value_expr and expr in parser..Our parser/analyzer requires some cleanup regarding the treatment of exprs and predicates.We need to unify all types of exprs including predicates under the expr production.This task will require moving some work from the parser into the analyzer with careful testing.,no,,[],,
IMPALA-3110,384,"Incorrect timing of scan node threads.Repro  
  Create household_demographics2 table  
  <SOURCECODE> 
  <SOURCECODE> 
  Profile snapshot, the scan ran with a single scanner thread and ScannerThreadsUserTime is 52s172ms while the scan node completed in 1.5s 
  <SOURCECODE>",no,,[pf3_profiling],,
IMPALA-3111,385,"Selecting collection types returns json output rather than analysis error.Today, an outermost select list may not return collection types-- Impala now throws an AnalysisException.E.g.<SOURCECODE> 
  Instead, we should support returning collection types in the outermost select list as json strings.",no,,[],,
IMPALA-3126,386,"Incorrect assignment of an inner join On-clause predicate through an outer join..Impala may return incorrect results for queries that have the following properties: 
   
   There is an INNER JOIN following a series of OUTER JOINs 
   The INNER JOIN has an On-clause with a predicate that references at least two tables that are on the nullable side of the preceding OUTER JOINs 
   
  Query to repro and its plan: 
  <SOURCECODE> 
  Workaround For some queries, this problem can be worked around by placing the problematic On-clause predicate in the WHERE clause instead, or changing the preceding OUTER JOINS to INNER JOINS (if the On-clause predicate would discard NULLs).To fix the example query above: 
  <SOURCECODE>",no,,[],,
IMPALA-3172,387,"No error message in Impala CLI for stale metadata.In the query profile I get: File 'hdfs://nameservice1/user/hive/warehouse/...' has an invalid version number: ????????????0 ???????????? This could be due to stale metadata.Try running ""refresh table_name"" 
  However the Impala-Shell does not print this error, just shows a new line.",no,,[pf3_profiling],,
IMPALA-3173,388,"Reduce catalog's memory footprint.An initial analysis of catalog's heap dumps shows that we can probably reduce it's memory footprint by: a) avoid storing redundant information about catalog entities such as partitions, b) using more compressed data structures.Currently, for a table with 2 int columns and 1 int partition column and without incremental stats, we use: 
   
   ~930B per partition out of which ~500B are used on hmsParameters_ (&lt;String, String&gt;Map), ~190B on cachedMsPartitionDescriptor_, and ~200B (depending on path) on location.~800B per file descriptor out of which ~530B go to file_blocks and the rest are used for storing the file_name.Every HdfsTable also uses two maps that replicate partition locations and file names (e.g. perPartitionFileDescMap_ and nameToPartitionMap_).A table like that with 100,000 partitions and 10 files per partition requires 1GB and 1.4GB of memory w and w/o incremental stats, respectively.This is a parent JIRA of 
   <URL>.",yes,algorithmic improvement for memory utilization,"[ss1_one_per, sk2_block_hang_crash, tk3_memory_unit, sk1_negative_necessary, gk3_reduce]",,
IMPALA-3205,389,"Validate and fix spilling performance and memory usage of new buffer pool.We want to ensure that performance doesn't regress: 
   
   Memory usage for the new buffer pool (with the old small buffers optimisation removed) should be similar or less than the old code for typical workloads.This will require reducing the buffer size - e.g. to 1MB or less.Spill-to-disk performance should not regress more than is acceptable.This may be tricky to get right since the I/O size will be reduced from 8MB.We may need to allocate logically sequential blocks in contiguous disk and do some kind of prefetching in the buffer pool to issue multiple writes at a time.",yes,performance and memory usage optimization,"[sk13_regression, sk7_buffer, gk4_performance, ss4_memory, tk3_memory_unit, sk2_block_hang_crash, gk8_spilling, sk4_prefetch, gk3_reduce]",,
IMPALA-3207,390,"Support Char type in codegen.There are multiple places in the backend which explicitly punt on codegen if there is CHAR type involved.Examples include: 
  <SOURCECODE> 
  <SOURCECODE> 
  The support for CHAR was disabled in f1c4e49905c57d91a8af455fe2537ae9cfb8110c because of various bugs.<URL> simplified the CHAR data type implementation and 
   <URL> added some support for codegen of a FIXED_UDA_INTERMEDIATE type with the same memory layout as CHAR.We've also fixed the other cases where codegen was disabled for specific data types, so enabling char codegen may actually be less error-prone than keeping it disabled and requiring all the requisite error-handling.",no,,[],,
IMPALA-3213,391,"Linking error while building Impala.Several Jenkins jobs started failing with linking errors while building Impala (e.g. 
   <URL>).The commit that seems to be causing this is 717dcb0.Tim, I assigned it to you.Feel free to reassign if it's not your commit.",no,,[],,
IMPALA-3275,392,"No artifacts collected after a failed Jenkins job build.The following job failed (
   <URL>) but there were no artifacts collected.In particular, it would be useful to have the data loading logs since this is where the build failed.",no,,[],,
IMPALA-3316,393,"convert_legacy_hive_parquet_utc_timestamps=true makes reading parquet tables 30x slower.Enabling convert_legacy_hive_parquet_utc_timestamps=true makes simple queries that don't even filter on a timestamp attribute perform really poorly.Parquet table.Impala 2.3 / CDH 5.5.2.convert_legacy_hive_parquet_utc_timestamps=true makes following simple query 30x slower (1.1minutes -&gt; over 30 minutes).<blockquote>
    select * from parquet_table_with_a_timestamp_attribute where bigint_attribute=1000771658169 
  </blockquote> 
  Notice I did not even filter on a timestamp attribute.Made multiple tests with and without convert_legacy_hive_parquet_utc_timestamps=true impalad present.Also, from 
   <URL> 
  <blockquote>
    Casey Ching added a comment - 15/Jun/15 5:12 PM Btw, a perf test showed enabling this flag was 10x slower.</blockquote>",yes,query 30x times slower,"[gk7_speed, ss14_perform, pf2_duration]",,
IMPALA-3331,394,"Improve ""Failed to parse file schema: Invalid record schema in avro_schema_record_field_append"" error.People can create their own avro files with API (python) and sometimes not follow the avro specifications at 
   <URL>.If names include '.'or '/' sometimes it may not match the avro schema file used to create the table, and you have to extract the schema from the avro file itself using avro-tools to locate the problem.Hive allows this for some reason, but Impala seems to be more sensitive.It would be nice if we displayed more than just: 
  ""Failed to parse file schema: Invalid record schema in avro_schema_record_field_append"" 
  and called out with the more detailed error: 
  In AVRO data file /path/file.avro, the schema does not follow avro guidelines at: ""name"" : ""/redacted/redacted/redacted/redacted/redacted/redacted"" Please see: 
   <URL> and correct",no,,[],,
IMPALA-3337,395,"Unexpected cancelled warning.The following query causes an unexpected warning: 
  <SOURCECODE> 
  Warning: 
  <SOURCECODE>",no,,[],,
IMPALA-3353,396,"Flaky test: TestRedaction.test_very_verbose_logging.<URL>, I picked you at random; feel free to reassign if you're swamped.<URL> 
  <SOURCECODE> 
  It looks flaky because build #2015, the next one, worked with a single change that looks unrelated.Like 
   <URL>, this could be because the test just before is failing.In this case, that test is custom_cluster/test_query_expiration.py.<URL>, 
   <URL>, this might be related to 
   <URL>.",no,,[],,
IMPALA-3355,397,"get query generation running against cdh5-trunk.<URL> means the current (cdh5-trunk) version of the query generator can't run.Since some test coverage is better than none, let's try an older version of the query generator first.While it may be missing some recent enhancements, it should at least run more reliably.It seems fine to run an older version of the query generator code against a current Impala.",no,,[],,
IMPALA-3393,398,"Report thread wake up interval is not well distributed.<URL> 
  We take an integer from 0 to 4.so if there are 200 nodes then ~40 of them will report at the same time every 5 seconds.This does not take into account fragment start time is basically random...",no,,[ss3_in_one_go],,
IMPALA-3454,399,Deletes may fail due to type mismatch in a query rewrite is needed.<SOURCECODE>.<PROFILING>.<ERROR>.,yes,has dynamic profiling data,"[pf3_profiling, ss11_rework]",,
IMPALA-3488,400,"test_ddl.py failure on LocalFS run.On LocalFS runs, we usually skip any test that uses the hdfs_client because the hdfs_client cannot access the local filesystem.However, we've had a bug in test_ddl.py for a long time where even though we skipped tests which used the hdfs_client, the setup_method() and teardown_method() used the hdfs_client.<SOURCECODE> 
  The only reason this never failed before was because of the way we used to check if files existed in hdfs_client.delete_file_dir(): 
  <SOURCECODE> 
  Since we only caught a generic exception, this function always returned 'True' and passed.It actually threw a ConnectionError exception.As a part of 
   <URL>, I added a hdfs_client.exists() function which did the right thing by only catching 'FileNotFound' Exception.This change exposed this bug on localFS runs causing them to fail.",no,,[sk10_skip],,
IMPALA-3506,401,"Impala/Kudu identifier casing.Kudu allows case sensitive names, e.g. ""t1"" and ""T1"" are separate table names.Impala should eventually follow the standard casing behavior (at least matching postgres), i.e. all identifiers get undercased unless they are quoted.For Kudu/Impala interactions, Kudu will support case sensitive naming and the behavior in Impala would be: 
  <ul class=""alternate"" type=""square""> 
   non-quoted identifiers are always lower-cased within Impala before calling any Kudu APIs 
   quoted identifiers are left as is 
   same goes for column names",no,,[],,
IMPALA-3620,402,"Consider implementing PrivilegeCache with listPrivileges based on authorization hierarchy.Impala's authentication can be slow on get_schema and show databases etc. due to Sentry's implementation.Once they have 
   <URL> in place.Impala should use their new api listPrivileges for authentication.According to Hao Hao from Sentry team, improvement is 15x with many roles/grants in Hive.",no,,[gk7_speed],,
IMPALA-3636,403,Regression in DecimalOperators::EQ with codegen disabled.Some of the decimal improvements that came in Impala 2.6 introduced a regression in the none-codegened path.This regression was cause by <URL>.<PROFILING>.This is a simplified version of the query which can be used as a repro <SOURCECODE>.,yes,performance regression,"[sk13_regression, pf3_profiling]",,
IMPALA-3672,404,"Build failed due to libkudu_client.so not found.Several cdh5-trunk core builds failed because of this.<URL> 
  <SOURCECODE>",no,,[],,
IMPALA-3697,405,"More reliable instruction set feature check.We check CPU features now by parsing /proc/cpuinfo.According to 
   <URL>, ec2 machines sometimes do no expose to /proc/cpuinfo all of the information about which instruction sets they support.We might be able to detect if instructions are supported by actually trying to run them (in a separate process, so we don't crash).However, beware of 
   <URL>",no,,[sk2_block_hang_crash],,
IMPALA-3701,406,"Evaluate compressing Runtime filters to save coordinator network bandwidth.When running complex queries on large clusters with lots of runtime filters the coordinator quickly becomes network bound due to the extra incoming and outgoing traffic for runtime filters, once the coordinator becomes network bound all other fragments in the cluster are negatively affected as they get blocked on shuffling/broadcasting data to the coordinator node.This bottleneck was identified when running large scale tests on EC2 nodes with less than ideal network throughput.In attached png is aggregate network throughput across the 32 nodes in the cluster with the coordinator in red.<span class=""image-wrap"" style="""">
    <URL></span>  
  Compression should alleviate this bottleneck but we should consider other solutions",yes,throughput performance optimization,"[sk15_network, tk2_bound, sk2_block_hang_crash, gk7_speed, gk9_throughput, gk2_complexity]",,
IMPALA-3772,407,"Failure in TestAdmissionControllerStress.I ran into this when running an exhaustive build for an unrelated scanner change.<URL> 
  <SOURCECODE>",no,,[],,
IMPALA-3777,408,"SqlParser parsed error for unicode.When I run query:create table unicode_parse_error(id int) row format delimited fields terminated by '\u0023##'; the field delimiter becomes to '\u0017##'.Logs: 
  <SOURCECODE> 
  After debugging, it seems that SqlParser.parse() goes wrong.As attachment shows, before calling SqlParse.parse() the statement is: fields terminated by '\u0023##' , but after parsing, it becomes '\u0017##'",no,,[],,
IMPALA-3783,409,"TestHS2.test_get_log failed for one local filesystem test execution.<URL> 
  Failed.The umbrella job running it is: 
   <URL> 
  Failure message: 
  <SOURCECODE> 
  It's looking for the error result in the log response but seeing a progress message instead, and further the progress message is at 0% complete.The query does execute; you can see the execution in the Impala logs.It doesn't show the error/result there.Note it's running in parallel test set, so there is other concurrent activity.The test itself first executes the query, checks response, fetch results and checks that response, and then gets the log.So, either the checks do not actually wait for the error and validate as it looks, or the progress messages are lagging somehow.If the former, it seems the test needs to be fixed, and the TGetLog is only to show current status, so if the query isn't done, you'd not expect to see the error.If the status is lagging in that the query is actually done but the TGetLog response doesn't see it, it may be a product bug../ee_tests/impalad.impala-boost-static-burst-slave-0967.vpc.cloudera.com.jenkins.log.INFO.20160621-193939.27395 
  <SOURCECODE>",no,,"[pf1_percentage, tk7_concurrent]",,
IMPALA-3786,410,"Replace ""cloudera""  in code and package names with ""impala"".Impala today includes many mentions of Cloudera, including the Java package names.It should move to using only ""Apache"".",no,,[],,
IMPALA-3797,411,"Relax sentry privileges required for creating or dropping functions.Impala requires an ALL privilege on the server level to create or drop functions.This is an unreasonable ask for sensitive deployments.Fix it and make it consistent with Hive.For Hive, 
  <ul class=""alternate"" type=""square""> 
   Create requires ALL on the hdfs URI 
   Drop doesn't require any specific privilege.",no,,[],,
IMPALA-3800,412,"collect_minidumps.py does not support python 2.6.Python 2.6 is the default python version shipped with CentOS 6.6 and the minidump collection script needs to run there, too.However, the tarfile library version shipped with python 2.6 does not support using TarFile objects as context managers.To fix this we should wrap tarfile.TarFile in contextlib.closing to support python 2.6, too.",no,,[],,
IMPALA-3814,413,Clean up and refactor large try blocks in impala-shell.One particular try block in impala_shell.py (in _execute_stmt()) is particularly large with a large chain of except statements.Not all of the except statements apply to all the code in the try block.This would be a lot cleaner if it's split up into multiple try blocks which each have only the required except statements.,no,,"[ss2_all_to_one, sk2_block_hang_crash]",,
IMPALA-3824,414,"TestHBaseQueries.test_hbase_inserts caused crash in impala::RuntimeProfile::total_time_counter.Over the weekend, there was an Impala crash in old aggs/joins job.Subsequent jobs succeeded.Unfortunately, we can't get the core because the machine that had it has been deprovisioned.Because this happened on 2.6.0, we didn't have the infra in place to save the cores to S3 (that has since been fixed).Failed run: 
  
   <URL> 
  Here is the backtrace 
  <SOURCECODE> 
  If I'm reading the console log, the failing test is  
  <SOURCECODE> 
  Here is the full failure: 
  <SOURCECODE> 
  A quick scan of the test code indicates ""executing against"" does happen before the actual query printed executes.Presumably then this last query listed above is the troublesome one.",no,,[gk7_speed],,
IMPALA-384,415,In the profile the DecompressionTime is always 0ns for RCfile format.In the query profile the DecompressionTime is always 0ns for RCfile format.There is no variable named decompress_timer_ in hdfs-rcfile-scanner.h for measuring the decompression time.<SOURCECODE>,no,,[pf3_profiling],,
IMPALA-3855,416,"small table left join  big table   Can be optimized for broadcast.I think small table broadcast to bigtable , then use inner join , and then the next node add some not join row in small table .which can be very valueable",no,,[],,
IMPALA-3859,417,"HdfsScanNode::LogRowParseError() will log table data.<tt>HdfsScanNode::LogRowParseError() is designed to log the data around a particular scanner parsing error.However, that log output may include sensitive table data, and should be avoided, particularly for uncompressed text formats.",no,,[ss23_avoid_if],,
IMPALA-3877,418,"Support unpatched LLVM.We currently depend on a patched version of LLVM to work around a feature/limitation/bug of LLVM's linker.It seems unlikely that the patch will make it into LLVM, so we should implement a workaround.The problem is that function call arguments may have different types to the called functions.We should implement a wrapper for the IRBuilder CreateCall() function that adds bitcasts to arguments as appropriate and use that consistently for codegen.I implemented a version of this as part of the larger 
   <URL>, but we could separate this out.",no,,[],,
IMPALA-3911,419,"custom_cluster/test_admission_controller.py.<SOURCECODE> 
  
   <URL>",no,,[],,
IMPALA-3923,420,"BufferedTupleStream::GetRows() can overflow 32-bit int.GetRows() tries to read an entire stream into a RowBatch.The stream num_rows is a 64-bit int but the RowBatch num_rows is a 32-bit int.If GetRows() is called with num_rows &gt; INT_MAX the likely result is a crash 
  The GetRows() API has other flaws (
   <URL>) but it would be good to avoid this particular crash.",no,,[ss23_avoid_if],,
IMPALA-3928,421,Calling get_value() from a TestVector object for a non-existent key raises a StopIteration exception.<SOURCECODE>,no,,[tk6_iteration],,
IMPALA-3939,422,"Data loading may fail on tpch kudu.Some gvms failed on data loading when tpch kudu tables already existed and load-data.py attempts to re-create the same tables.Ideally our data loading code would be smart enough to not re-create the same tables again, but it looks to me like we actually create tables for all other formats with ""IF NOT EXISTS"", so we should do the same for Kudu.",no,,[ss11_rework],,
IMPALA-3980,423,"Re-enable Hive as a target database for the Random Query Generator.As part of 
   <URL> some of the support for running the Random Query Generator against Hive was removed.This JIRA aims to re-introduce support for running the RQG against Hive.",no,,[ss11_rework],,
IMPALA-3982,424,"tests/common/impala_service.py fails to parse debug web page in non-English locales.I tried to run bin/start-impala-cluster.py and got the following error, repeated every second until timeout: 
  Error starting cluster: Expecting property name: line 72 column 19 (char 2264) 
  The error message contains no details about where it expects a property name, but I managed to track it down to tests/common/impala_service.py trying to parse 
   <URL>, which contained: 
   70 ""statestore-subscriber.heartbeat-interval-time"": { 71 ""count"": 14, 72 ""last"": 1,00036, 
  The number in line 72 uses a comma as the decimal separator, but the start script expects a dot.The reason for this is that I had LC_NUMERIC=hu_HU.UTF-8 set in my enviroment.It seems that the JSON metrics page respects the locale, while the start script ignores it.Possible solutions: 
  <ul class=""alternate"" type=""square""> 
   Ignore the locale on the metrics JSON page.Respect the locale in the start script when parsing the JSON.(This is probably not feasible, as a comma can not act both as a decimal separator and as an item separator at the same time, so the JSON page in its current form in unparseable.)Set the locale to en_US.UTF-8 as a first step in every script.(Additionally improve error handling in impala_service.py by making error reporting more verbose.)",no,,"[sk8_timeout_expiration, ss3_in_one_go]",,
IMPALA-4013,425,"Lzo text scanner can fail with corrupt index file with abort_on_error=1.I see a lot of errors like this when running the scanner fuzzing: 
  <SOURCECODE> 
  <SOURCECODE>",no,,[],,
IMPALA-4016,426,"Avoid materializing slots only referenced by Kudu conjuncts.Right now, when SlotRefs are serialized to Thrift, Impala makes sure the slots are materialized.For Kudu we only need to mark them as materialized to build the kudu predicates, meaning the slots might not be referenced in the projection at all.In this case we should avoid materializing the slots (and thus allocating memory).I believe that when we move to the scan token API we can avoid marking the SlotRefs as materialized.",no,,"[ss4_memory, ss23_avoid_if]",,
IMPALA-4032,427,"Run selected set of test suites against a remote cluster.Once we have test data loaded onto the cluster, we should pick a small set of core test functions from some test suite, and see if we can get them to run.",no,,[],,
IMPALA-4038,428,"RPC delays for single query can lead to ImpalaServer not making progress on any queries.We observed a phenomenon where all Impala queries submitted to an Impala daemon got stuck in the CREATED state.One of the causes was an RPC not timing out (
   <URL>), but this was greatly exacerbated by the locking in ImpalaServer, which meant that no queries could make progress.We saw threads in the following callstacks.<SOURCECODE> 
  The basic problem is a mismatch between the durations that two sets of locks are meant to be held for.ImpalaServer::query_exec_state_map_lock_ and QueryExecState::lock_ must be acquired at various points for a query to make progress, so it's important that they're only held for short critical sections, .Coordinator::lock_ is held for long durations during RPCs in the coordinator, but holding that lock can't prevent other queries from making progress, so this is ok.So it's bad if a thread tries to get Coordinator::lock_ while holding either of the other locks, since it may wait a long time while holding the other locks and block progress for a long time for all other queries in the system.",yes,it may wait for a long time,"[ss2_all_to_one, sk2_block_hang_crash]",,
IMPALA-4047,429,"Remove ""CDH"" from Impala.Impala is an Apache (incubating) project, and ""CDH""/""cdh"" should be removed from shell scripts and config files and so on.",no,,[],,
IMPALA-4062,430,"Create thread pool for HdfsScanNode::ScannerThread to limit Kernel contention.Servers with modern processors like E5-2698 can have up to 80 logical processors per server, as a result queries occasionally end up running with a significantly larger number of threads.Creating and destroying threads is expensive and wastes lots of resources, hence consider creating a thread pool for scanner threads to avoid resource contention during thread creation.For TPC-DS Q27 &gt;40% of CPU cycles are spent pthread_mutex_unlock and pthread_mutex_lock 
  Call stacks 
  <SOURCECODE> 
  <SOURCECODE> 
  <SOURCECODE> 
  Top functions from Perf 
  <SOURCECODE>",yes,reduce CPU cycles,"[ss1_one_per, ss12_spend_time, gk6_expensive, pf1_percentage, ss23_avoid_if]",,
IMPALA-4092,431,"Log the remote fragment error message on coordinator too.When a remote fragment fails, in the coordinator logs only the Cancel() message can be seen, not the real error message: 
  <SOURCECODE> 
  Of course the real error message is logged on the remote (backend) host, for example: 
  <SOURCECODE> 
  The error message is recorded in the query profile, but it is hard to track - trace - monitor why the query failed actually.",no,,[pf3_profiling],,
IMPALA-4114,432,"Port relevant BufferedBlockMgr unit tests for BufferPool.We have a bunch of unit tests for BufferedBlockMgr, some of which are regression tests for interesting scenarios.We should go through and evaluate which of them are applicable to the new BufferPool and port those.",no,,[sk13_regression],,
IMPALA-4127,433,different fragment should not authenticate to the same node again in a short period of time.This can happen(same host&lt;-&gt;host) hundreds of times in a row in couple seconds when there is a single node query that has many fragments.<SOURCECODE>,no,,[pf2_duration],,
IMPALA-4132,434,"Consider using -fno-omit-frame-pointer in release builds.We're currently building with -fomit-frame-pointer, which can lead to cryptic stack traces.Switching to -fno-omit-frame-pointer will produce better stack traces but cost some performance.In my past experience, on x86-64 the penalty was around 0.5% (though on different code and with a different register allocator) which I think is a reasonably tradeoff but in corner cases it can be higher.So we'd need to qualify this change through a perf regression run (both benchmarks and primitives).",yes,performance regression,"[sk13_regression, gk4_performance, pf1_percentage, pf4_benchmark]",,
IMPALA-416,435,"Impala and Hive's unescaping of octal escape sequences is flawed..Impala reuses Hive's code to do deal with escape sequences, so both Hive and Impala are affected.Octal values &gt; 127 fail to unescape properly.This may lead to problems if, e.g., a user has text data with exotic single-byte delimiters (UTF-8 single byte or ASCII extended).For example, 
  select ""\127"" returns ""W but select ""\128"" returns ""128""",no,,[sk3_byte],,
IMPALA-4170,436,"Incorrect double quoting of identifier in SQL generated by COMPUTE INCREMENTAL STATS..COMPUTE INCREMENTAL STATS fails to update the stats of new partitions if the name of a partition column is a reserved word.A syntax error is reported because a generated SQL statement incorrectly uses two sets of backticks for quoting the identifier.Repro: 
  <SOURCECODE> 
  The problem is in HdfsPartition.getConjunctSql() where the double quoting is pretty obvious: 
  <SOURCECODE>",no,,[],,
IMPALA-4188,437,"run query generator with KUDU_IS_SUPPORTED=true.Due to 
   <URL>, and the query generator running on Docker-on-AUFS, we should come up with a way for to run with <tt>KUDU_IS_SUPPORTED=true.There are some options here, including external docker volumes, configuring devicemapper as an alternative Docker storage driver, or getting the query generator execution completely out of Docker.",no,,[],,
IMPALA-4226,438,"Make breakpad dump_syms handle more than 4096 threads and memory regions.I recently tried to resolve a Minidump with more than 4096 threads.Apparently this is a hard limit compiled into Breakpad.After patching the sources I was able to proceed, so we should add a patch to our toolchain.",no,,[],,
IMPALA-4258,439,Clean up duplicated test macros.For some reason we have EXPECT_OK and ASSERT_OK defined in both testutil/test-macros.h and testutil/gtest-util.h.We should remove this duplication.,no,,[],,
IMPALA-4260,440,"Alter table add column drops all the column stats.Adding a column after compute stats drops the per column stats, repro below 
  <SOURCECODE>",no,,[ss1_one_per],,
IMPALA-4314,441,"Standardize on MT-related data structures in the coordinator and scheduler.<URL> introduced new control data structures to enable coordination and scheduling for multi-threaded execution.However, the old data structures were left in place, so we need to remove them and standardize on the new ones.",no,,[],,
IMPALA-4337,442,"Wrap long lines in explain plans.Explain plans can have very long lines, particularly when printing lists of expressions.It should be possible to wrap, and still correctly indent, those lines.This is trickier than it sounds because they have to be wrapped in the context of their place in the plan (i.e. with appropriate prefixes etc).It's a good opportunity to split out explain plan generation from presentation, centralizing the logic so that this kind of change is easy to make.",no,,[],,
IMPALA-4412,443,Per operator timing in profile summary is incorrect when mt_dop > 0.From the summary <PROFILING>.The slowest fragment in the query <SOURCECODE>.,yes,has dynamic profiling data,"[ss1_one_per, gk7_speed, pf3_profiling]",,
IMPALA-4433,444,"timestamp_col NDV intermittently incorrect.I can reproduce this 2 out of 6 times.<URL> also saw it: 
  
   <URL> 
  From my Jenkins console: 
  <SOURCECODE> 
  To reproduce, run in an ec2 c3.4xlarge, using 
   <URL> from a stock ubuntu 14.04 AMI.Following Amos Bird's thread, I checked 
  <SOURCECODE>",no,,[],,
IMPALA-4437,445,"Occasional crash in buffered-block-mgr-test.I saw a failure in the exhaustive tests, but it is in a be test that is run as part of core: 
  <SOURCECODE>",no,,[],,
IMPALA-448,446,"Hive queries fail against table when Impala INSERT is in progress on same table.Hive queries fail if any unexpected subdirectories are found under a table/partition directory.When doing an insert via Impala, Impala will create a temp staging directory for the insert results.This causes any concurrent Hive queries on the table to fail.",no,,[tk7_concurrent],,
IMPALA-4535,447,"ASAN build fails to compile.Looks like clang doesn't like 'auto' in function prototype.This seems to be introduced by the following commit: 
   <URL> 
  <SOURCECODE>",no,,[],,
IMPALA-4544,448,"Data loading without snapshot fails with ASAN.Setting <tt>CMAKE_BUILD_TYPE=ADDRESS_SANITIZER and loading test data from scratch fails: 
  <SOURCECODE> 
  From <tt>logs/data_loading/catalogd.ERROR: 
  <SOURCECODE>",no,,[],,
IMPALA-4551,449,"Set limits on size of expression trees.Very large expression trees can cause havoc in various Impala components.I have been experimenting with the attached test that generates large case statements of varying depths and widths, and have been able to hit limits in the frontend (Java OOM) and caused various runaway memory usage problems in the backend (thrift structures, LLVM IR, codegen, etc).We should set some kind of limit here, either on the number of nodes in the expression trees, or on the size of the query text, and then make sure that we can execute queries of the maximum size end-to-end.",no,,[ss4_memory],,
IMPALA-4600,450,"qgen: refactor discrepancy_searcher.py.The query generator design could use improvement.One of the design principles I've heard is ""an object should have 1 responsibility"".That is not the case with the query generator, especially with the QueryResultComparator and QueryExecutor in disrepancy_searcher.py.They each have several responsibilities.Also, Leopard's Job() duplicates some of this work.There should be 1 thing that can compare two Query objects' results.There should be 1 thing that knows how to execute any AbstractStatement concurrently on both a test and reference database.There should be 1 thing to track so-called ""execution modes"".Really what we want here is a sequence of setup statements, a sequence of test statements, and a sequence of teardown statements.As it stands the above is all mingled together across the classes, make extensibility difficult.",no,,[tk7_concurrent],,
IMPALA-4602,451,"qgen: rename instances of query to statement.The random query generator uses the name ""query"" incorrectly.<ul class=""alternate"" type=""square""> 
   in class definitions 
   class attributes, local variables, formal parameters 
   module names 
   
  Really, it generates SELECT statements, and soon the whole beast will be generating DML statements.The name query is misleading.But to change this in every single place along with a patch set that introduces new functionality results in a very large diff, hard to read.It would be easier to read and test such a change in isolation.",no,,[ss19_lot_reading],,
IMPALA-4680,452,"Join disjuncts are not simplified and pushed to the scan nodes when possible.While looking at the plan for primitive_conjunct_ordering_1 noticed that the join condition can be simplified and pushed to the scan but it is not.<URL> 
  Query 
  <SOURCECODE> 
  <SOURCECODE>",no,,[],,
IMPALA-4693,453,"qgen: consider alternatives to casting when writing INSERTs.The query generator tends only to deal with so-called generic types, not of sized types.When I say that, I mean it only knows about Ints and Chars, but does not tend to distinguish among BigInts, Ints, SmallInts, etc.When writing random INSERT statements, we often see loss-of-precision errors in the random statements generated.To get around this we are casting.But there are may be other approaches instead.This Jira is about investigating such approaches.",no,,[],,
IMPALA-4722,454,"test_scratch_disk.py fails sporadically when asserting logfile content.In automated tests we're seeing sporadic failures of tests/custom_cluster/test_scratch_disk.py.It looks like the log contents may not have been flushed to disk before calling <tt>self.assert_impalad_log_contains(""INFO"", ""Using scratch directory "", expected_count=1).This can be fixed by adding <tt>--logbuflevel=-1 or <tt>--logbufsecs=0 to the startup flags.",no,,[],,
IMPALA-4743,455,"Evaluate constant arguments to ScalarFnCall once during initialization.Constant input arguments to a ScalarFnCall expression is evaluated in ScalarFnCall::Open().Ideally, this should be evaluated once during the ScalarFnCall::Prepare().This allows constant arguments to be constant folded into the body of the UDF's function and prevents repeated evaluation of the input arguments for each fragment instance.",no,,[],,
IMPALA-4765,456,"Catalog loading threads can be wasted waiting for a large table to load.When there are multiple requests to the catalogd to prioritize loading the same table, then several catalog loading threads may end up waiting for that single table to be loaded, effectively reducing the number of catalog loading threads.In extreme examples, this might degrade to serial loading of tables.Note that even a single query may issue multiple table-loading requests even for the same table if the table is very big.After issuing a load request, an impalad will wait 2m for the metadata to arrive, and then send the request again every 2m.So if a large table takes 20m to load, then a single query could issue 10 table-loading requests which ultimately hog 10 table-loading threads in the catalogd.The simplest way to diagnose the issue is to examine the jstack of the catalogd and then you might discover several stacks that look like this: 
  <SOURCECODE> 
  The buggy code can be found in TableLoadingMgr.java: 
  <SOURCECODE> 
  Notice that the first few lines are intended to avoid loading the same table multiple times.However, the code does not prevent multiple threads from entering Catalog.getTableOrLoad() which will block on the same future for the same table.Reproduction: The issue is easy to reproduce by simulating a long table load and doing several concurrent loads of the same table from an impalad.For example, you can first ""invalidate metadata t"" and then ""desc t"" several times concurrently.A slow table loading can be simulated by adding a sleep inside call() function of the FutureTask created in TableLoadingMgr.loadAsync().",yes,threads can be wasted for waiting...,"[ss8_load_nn, sk2_block_hang_crash, gk7_speed, tk7_concurrent, gk3_reduce]",,
IMPALA-477,457,"Unexpected LLVM Crash When Querying Doubles on CentOS 5.x.We have a very strange LLVM issue on our cluster which has been prohibiting us from using codegen for quite some time.<h4><a name=""Symptoms""></a>Symptoms</h4> 
  The symptoms are simple, execute a query which utilizing runtime code generation on a DOUBLE column, the query will not only fail, but crash all the daemons which took part in the query.The following behavior is observed when issuing the query from the shell 
  <SOURCECODE> 
  From an operations point of view, these nodes die off with no warning, or error.With GLOG set to 2, the last known message was that LLVM was going to be used.<h4><a name=""TheQueries""></a>The Queries</h4> 
  As mentioned in the title, the crashes seem to only be reproducible with doubles.Here is an example query with LLVM on and off, calculating the AVG of a column.The Table 
  <SOURCECODE> 
  LLVM ON 
  <SOURCECODE> 
  LLVM OFF 
  <SOURCECODE> 
  With GLOG = 2, here is the last observed line before the daemon has an unexpected crash.<SOURCECODE> 
  <h4><a name=""DoublesOnlyReasoning""></a>Doubles Only Reasoning</h4> 
  The issue seems to only affect doubles, not only can I use AVG, MAX, etc..with INTs, but I think this query shows it the best.LLVM ON With <em>INTS</em> 
  <SOURCECODE> 
  LVM ON With <em>INTS</em> &amp; <em>DOUBLES</em> <em>1 to 1.0</em> 
  <SOURCECODE> 
  <h4><a name=""ConfigurationTesting""></a>Configuration Testing</h4> 
  This bug only seems to occurr on this specific hardware and CentOS 5.x.I reformatted this node to CentOS 6.x and this issue effectively dissapeared, it seems to only affect this specific hardware on this family of Red Hat.In addition, I attempted to reproduce this using systest, virtual machines, etc on CentOS 5.x and was unsuccessful.<h4><a name=""Logs""></a>Logs </h4> 
  Attached is a GLOG 2 version of an LLVM query failing.I tried to analyze a core dump, but since there is compiled code in there, GDB is unable to look into it due to a Unsupported JIT Version error... 
  With <em>--module_output=/tmp/module.out</em> added, module output is written on ALL LLVM queries except the one's that crash.I'm assuming those queries are crashing before it gets to the point of writing module output.",no,,[],,
IMPALA-4867,458,"Sometimes set num_nodes=1 in random query generator.I just found a correctness issue - 
   <URL> - that the random query generator would have found very quickly if it set num_nodes=1 and added limits to join queries.It would be good if we added that query option to the search space.",no,,[gk7_speed],,
IMPALA-4919,459,"Jenkins jobs don't bubble up artifacts to parents.When the 'gerrit-verify-dryrun' job runs, it kicks of a chain of jobs underneath the covers.The final artifacts for the entire run (impalad logs, etc.) do not show up in the 'gerrit-verify-dryrun' job and only show up in final job in the chain of jobs that get kicked off.This gets hard to track down if we need to diagnose errors.Ideally, the job that the user interacts with should have the link to all the artifacts necessary.",no,,[ss2_all_to_one],,
IMPALA-4941,460,"Bump Zookeeper version to 3.4.10 to address ZOOKEEPER-2044.<URL> fixes the root cause of 
   <URL>.The fix is included in Zookeeper version 3.4.10, we need to bump the minicluster dependency.<URL> - Do you know how or where to change this version?",no,,[],,
IMPALA-4952,461,"Differentiate between resolved and unresolved IP addresses.KRPC (
   <URL>) requires resolved IP addresses.Since RPCs happen frequently, and IP resolution can be expensive, it makes sense to retain resolved addresses where possible.The scheduler already knows the resolved address for every backend.However, the logic is very complex, and there's not really a good way to know at compile time whether or not we're using a resolved address or not.We can address this by adding a new type - perhaps <tt>ResolvedAddress : public TNetworkAddress, and requiring that <tt>RpcMgr::GetProxy() etc take a <tt>ResolvedAddress.That way the compiler will complain if we don't prove to it that the address is resolved.We can make it hard to construct a <tt>ResolvedAddress without actually resolving some string into an IPV4 address.",no,,"[sk9_frequency, gk6_expensive, gk2_complexity]",,
IMPALA-4954,462,"rand() isn't random.<tt>rand() is not a very good random number generator.For some use cases, 
   <URL> would be a better choice.Places to audit: 
  <SOURCECODE> 
  <SOURCECODE>",no,,[],,
IMPALA-496,463,"INSERT VALUES does not write ESCAPED BY character into data files.For a table created with an ESCAPED BY clause: 
  create table pipes (s string) row format delimited fields terminated by '|' escaped by '#'; 
  one would expect that inserted string values would get the escape character prepended before each terminator character in the data file.But this does not happen, leading to the columns getting mixed up when the table is queried: 
  insert into pipes values ('string contains a | character', 'string contains a #| sequence'); 
  I expect the data file to look like: 
  string contains a #| character|string contains a ##| sequence 
  Instead, hdfs dfs -cat shows the pipes are unescaped: 
  string contains a | character|string contains a #| sequence 
  That produces wrong results when the same row is queried: 
  <SOURCECODE>.It's interpreting the 2 parts of the first string as pipe-separated columns, and ignoring the second string.Assigning to Alex, even if the cause is out of his area of responsibility, in case it is something specific to INSERT VALUES.BTW: the ESCAPED BY clause was missing from the CREATE TABLE doc in Impala 1.0, but it is there now in the 1.1 doc.Although I'm waiting to straighten out the behavior before adding doc examples.",no,,[ss15_each_one],,
IMPALA-5123,464,"ASAN failure: heap-use-after-free in timezone_db.cc:683.Looks like the <tt>char *filestr in line 674 points to a temporary object and the underlying memory is free'd right after it's initialization.This was introduced by this change: 
   <URL> 
  Here's the ASAN output: 
  <SOURCECODE>",no,,[],,
IMPALA-5128,465,Only run slow backend tests in 'exhaustive' mode.Several backend tests take a significant amount of time to run.Paring down the tests to more minimal coverage should help increase productivity and iteration speed.,yes,increase the iteration speed,"[ss12_spend_time, gk7_speed, tk6_iteration]",,
IMPALA-5160,466,"Queries with a large number of small joins regress in terms of memory usage due to memory reservation.Queries with a large number of small joins regress in terms of peak memory usage due to memory reservations.For the example below peak memory usage increased from 470MB to 8GB.Query used 
  <SOURCECODE>",yes,performance regression,"[sk13_regression, ss4_memory, tk3_memory_unit]",,
IMPALA-5182,467,"Explicitly close connection to impalad on error from shell.When using connections secured with SSL, a connection close comprises a bi-directional SSL_shutdown().The second part of the bi-directional shutdown requires that the client also close the socket explicitly, and the server blocks till it gets the close notification from the client.We need to make sure that the impala-shell does the explicit close on an error.",no,,[sk2_block_hang_crash],,
IMPALA-5199,468,"Impala may hang on empty row batch exchange.Hang reproduction: 
  Start impala with the following stress timeouts (the same as those in test_exchange_delays): 
  <SOURCECODE> 
  Do a join where 0 rows will be exchanged: 
  <SOURCECODE> 
  When the data-stream-sender sends 0 rows, it sends it with params.row_batch.num_rows=0 and params.eos=true.The receiver hits the following case: 
   <URL> It ultimately times out from FindRecvrOrWait() after datastream_sender_timeout_ms. 
   <URL> 
  Due to stress_datastream_recvr_delay_ms, the exchange node is Prepare()'d even later.<URL> 
  Although this is a testing flag, this behavior is reasonable to expect in a real world setting under heavy stress, since the stress_datastream_recvr_delay_ms flag was added into testing after some users experienced nodes receiving data from DataStreamSenders and timing out after datastream_sender_timeout_ms, and before the ExchangeNodes were created.Thus, the CloseSender() function is called before the exchange node is created.After stress_datastream_recvr_delay_ms, the ExchangeNode is created and it calls CreateRecvr().<URL> 
  Then ExchangeNode::Open() calls DataStreamRecvr::CreateMerger() which makes a few threads wait on DataStreamRecvr::SenderQueue::GetBatch().<URL> 
  However, since the DataStreamSender has already received the timeout error from earlier, the receiver threads in GetBatch() will wait indefinitely until the query is cancelled by the user.",yes,Inefficiency under special cases: Impala may hang on empty row batch exchange,"[sk2_block_hang_crash, sk8_timeout_expiration, tk12_stress_testing, pf3_profiling]",,
IMPALA-5223,469,"HBase/Zookeeper continues to be flaky on RHEL7.There were several failures on RHEL7 recently.<SOURCECODE> 
  From the HBase master log: 
  <SOURCECODE> 
  Here's what a successful HBase looks like: 
  <SOURCECODE> 
  This issue is similar to 
   <URL>.",no,,[],,
IMPALA-5224,470,Remove repository.codehaus.org from Maven pom's.The repository no longer exists.The packages are still available in Maven central (I believe).,no,,[],,
IMPALA-5265,471,"Impala still throws zookeeper connection errors on non-HBase cluster.Impala still continues to throw these zookeeper errors even when there is no HBase active on the cluster on version 2.7.0 
  This was supposed to have fixed by 
   <URL> on Impala 2.5.0.But I continue to see them in 2.7 
  I0427 09:00:35.069941 12407 ClientCnxn.java:1096] Client session timed out, have not heard from server in 46031ms for sessionid 0x0, closing socket connection and attempting reconnect I0427 09:00:35.170202 12407 ClientCnxn.java:975] Opening socket connection to server localhost/127.0.0.1:2181.Will not attempt to authenticate using SASL (unknown error) W0427 09:00:35.170598 12407 ClientCnxn.java:1102] Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect Java exception follows: java.net.ConnectException: Connection refused at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717) at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350) at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)",no,,[],,
IMPALA-5314,472,Rename single letter tables in FE tests.I frequently create test tables on my local system with names like "t" or "p".A couple of frontend tests use the same names and then fail with "Table already exists".,no,,[sk9_frequency],,
IMPALA-5340,473,"Query profile and debug webpage can disagree about 'Query State'.The problem is that even though both the query profile and the /queries debug web page set 'Query State' from the value ClientRequestState::query_state_, the debug web page always takes the current value, whereas the value in the profile is only updated at particular points during execution.This can lead to a situation where the profile's 'Query State' is stale and doesn't match the one in the debug web page, which is confusing.",no,,[pf3_profiling],,
IMPALA-5437,474,"Codegen for Trunc() of timestamp takes far too long.The following query takes longer to run than it should because of codegen time.<SOURCECODE> 
  The size of the codegen module is excessive compared to the complexity of the query: 
  <SOURCECODE> 
  I dug into what was happening and the module is full of boost timestamp functions.It looks like most of this is coming from our implementation of Trunc(), which switches between a large number of different implementations.I moved Trunc() out of the cross-compiled IR and codegen time was more sensible: 
  <SOURCECODE>",yes,Codegen for Trunc() of timestamp takes far too long.,"[ss12_spend_time, gk2_complexity]",,
IMPALA-5501,475,"Installation instructions are slim, but inaccurate.The installation instructions do not say how to install Impala.They do say, ""The Impala package installs these binaries"".This is incorrect - there is no Impala ""package"".All of Impala's Apache releases have been source tarballs.",no,,[],,
IMPALA-5536,476,"TCLIService thrift compilation is broken on Hive 2.There are a few build issues that are preventing Impala from compiling with Hive 2: 
  1.common/thrift/CMakeLists.txt needs to create hive-2-api before generating the Hive 2 version of TCLIService.thrift 2.common/thrift/CMakeLists.txt needs to create a dependency that generates hive-2-api/TCLIService.thrift before compiling all of the thrift files 3.CatalogOpExecutor::applyAlterTable needs to use the shim to get the parameter that marks whether a partition's stats were generated automatically.",no,,[],,
IMPALA-5544,477,"CTAS fails to open HDFS file for writing when NN HA is enabled.For CTAS queries, Impala has to do some gymnastics to figure out the path of the new table before the table has actually been created by the Hive Metastore.It looks like like that logic has a bug which prevents it from working with HDFS HA.This issue manifests itself as follows.When running a CTAS through the Impala shell you will see: 
  <SOURCECODE> 
  In the HDFS NN logs you may see something strange like this: 
  <SOURCECODE> 
  The problem is most likely in CreateTableAsSelectStmt.analyze() 
  <SOURCECODE> 
  Workaround Split the query into <tt>create table and <tt>insert into select.",no,,[],,
IMPALA-5640,478,"Enable test coverage for Parquet gzip inserts was disabled.There's a comment in the code saying that it was disabled because of 
   <URL>.This test coverage seems useful - we should add it back 
  <SOURCECODE>",no,,[],,
IMPALA-5711,479,"test_redacted failing in GVOs.<SOURCECODE> 
  Looks like a potential crash, 
  <SOURCECODE> 
  GVOs (unrelated to each other, on different patches): 
   <URL> 
   <URL> 
  Trying to repro it locally by running in a loop, no luck so far.",no,,[tk6_iteration],,
IMPALA-5731,480,"Build Impala 2.9.0 failed on CentOS 6.9.I'm building Impala 2.9.0 in CentOS 6.9 and encounter some error.I think there should be something wrong with openssl.I've upgrade to the latest 1.0.1e of openssl-devel, But still cannot compile.<SOURCECODE>",no,,[],,
IMPALA-5739,481,"sles12 SP2 is not correctly detected by bootstrap_toolchain.py.Hector at Cloudera reported that our OS detection isn't accurate in some cases: 
  <SOURCECODE>",no,,[],,
IMPALA-5760,482,"Flaky test: query_test/test_udfs.py.There were some random failures of the UDFs test in our builds.One of the errors: 
  <SOURCECODE>",no,,[],,
IMPALA-581,483,"Support setting TBLPROPERTIES in CREATE/ALTER table statements.Support setting TBLPROPERTIES in CREATE/ALTER table statements.Syntax: 
  <SOURCECODE>",no,,[],,
IMPALA-5810,484,"Consider reducing RESERVATION_MIN_MEM_REMAINING.Based on my findings in 
   <URL> reducing this value could help reduces the memory requirements of queries in the 100-200M range.",no,,"[pf2_duration, gk3_reduce]",,
IMPALA-5840,485,"Don't write page level statistics in Parquet files in anticipation of page indexes.The Parquet project is very likely to deprecate page level statistics in favor of page indexes (
   <URL>).We should stop writing the page level statistics now since we don't plan to add read support for them.",no,,[],,
IMPALA-5844,486,"Fix management of FunctionContext ""local"" allocations..FunctionContexts associated with expressions make two kind of allocations with very different lifecycles.One type of allocation is owned and managed by the expression, while the other ""local"" allocation is implicitly transferred to the Impala daemon after control flow returns from the expression.Both are currently allocated from the same pool.RowBatches returned from plan nodes may reference variable-length data stored in local allocations so this memory should be attached to the RowBatches.One approach here is: 
   
   Separate local and other allocations to allocate from different MemPools.Manage local allocations in bulk by clearing, freeing, or transferring data from that MemPool, similar to other memory that would be allocated from a MemPool.I think there are some potential wrinkles here and work to set up correct MemPools for all places that use expressions but I think the high-level approach should work.",no,,[ss4_memory],,
IMPALA-5850,487,"Partitioned hash join inside union may return wrong results.Impala may return wrong results for plans that have a partitioned join inside a union.Affected queries 
   
   plan has a partitioned join inside a union 
   tables must have stats - otherwise a partitioned join would not be chosen 
   for at least one equi-join condition, the left-hand side and right-hand side join keys have different types 
   
  Reproduction Setup: 
  <SOURCECODE> 
  Query that returns correct results: 
  <SOURCECODE> 
  Query that returns wrong results: 
  <SOURCECODE> 
  Analysis The bug is a missing implicit cast in the EXCHANGE 05.The id should be cast to BIGINT to be consistent with the left input of the join.We already have code to properly cast partition expressions in exchanges, but the code incorrectly assumes that we only need to do so for hash-partitioned fragments.The problem is that the UNION makes the fragment RANDOM partitioned (because the union children could be arbitrarily partitioned there is no guarantee on which partition is produced by the fragment).The buggy code is in PlanFragment#finalizeExchanges(): 
  <SOURCECODE> 
  Workaround 
   
   Use the broadcast and straight_join hints to force the join to use a broadcast distribution strategy 
   Reformulate the query to avoid the join inside a union 
   Write the join result into a separate table and use that table in the original query instead of",no,,[],,
IMPALA-5891,488,"PeriodicCounterUpdater should not rely on static initialisation and destruction order.The lifecycle of PeriodicCounterUpdater is weird and depends on static initialisers and destructors running.Its constructor spawns a thread and its destructor tests down data structures in some probably unsafe order.I've seen a crash in the code which could be caused by the data structures being torn down while the thread is still running: 
  <SOURCECODE> 
  Instead of the static instance object we should just allocate the state on the heap.",no,,[],,
IMPALA-5916,489,"Add truncating files to test_scanners_fuzz.py.We should add tests for truncated files to test_scanners_fuzz.py.Impala can see truncated files when users overwrite tables (e.g. using Hive) while Impala is scanning them.The fuzz test already truncates files already, however we need to add support for running a query on an untruncated table first, so that the metadata and the file diverge.See 
   <URL> for details.",no,,[],,
IMPALA-5959,490,"jenkins.impala.io tests pass but job fails deleting workspace.On jenkins.impala.io, we have seen several cases where ubuntu-16.04-from-scratch tests pass but the job ultimately fails when the workspace cannot be cleaned up.E.g.<SOURCECODE> 
  The issue and some proposed solutions were discussed on the dev@ mailing list: 
   <URL>",no,,[],,
IMPALA-5982,491,"String columns saved to Parquet files should be annotated with the UTF8 logical type.When creating Parquet files, Impala doesn't add the proper logical type corresponding to the string SQL type.String columns should be annotated with the 
   <URL> logical type.The lack of the <tt>UTF8 logical type annotation makes it harder to consume the data using other tools, or even by Impala itself if the files are manually moved around in the filesystem and a new table has to be created based on their metadata.",no,,[],,
IMPALA-6039,492,"BitReader::GetAligned() doesn't zero out trailing bytes.BitReader::GetAligned() only sets the initial bytes of the output value, leaving the remaining bytes set to whatever they were previously.It isn't clear if this is intentional and undocumented or a latent bug.The problem is non-obvious because the current callsites either call it with a single byte at a time (GetVlqInt()), or initialize the output value to zero and always call GetAligned() with the same num_bytes value (RleDecoder).",no,,[sk3_byte],,
IMPALA-6075,493,"Add Impala daemon metric for catalog version.<URL> was investigating a metadata inconsistency issue and it would have been useful to have a metric to that exposes the catalog version that the daemon has.You can see the oldest version on the topics page on the statestored, which is sometimes helpful, but that doesn't tell you whether a particular coordinator has a stale catalog.",no,,[],,
IMPALA-6105,494,"unix_timestamp() documentation doesn't mention unit.When reading through the docs I noticed that unix_timestamp() doesn't explicitly say what unit it returns (I believe it is seconds).I saw this create some confusion for a user: 
   <URL> 
  See 
   <URL> 
  <blockquote>
    Purpose: Returns an integer value representing the current date and time as a delta from the Unix epoch, or converts from a specified date and time value represented as a TIMESTAMP or STRING.</blockquote>",no,,[pf2_duration],,
IMPALA-6140,495,"Unexplained gap between ""All backends started"" and ""Rows available"".In the query timeline below there is a significant delay before rows are available.<SOURCECODE> 
  The exec summary doesn't have any evidence of slowness.I also looked at the rest of the profile and couldn't find any timers &gt; 5s.<SOURCECODE> 
  One interesting data point is that we have anecdotal evidence that there were orphaned query fragments sending reports on this cluster.",yes,query has a significant delay,"[gk7_speed, pf3_profiling]",,
IMPALA-6155,496,"TPC-DS Q77A is not deterministic.See 
   <URL> 
  The query is sorted by the first two columns but there are duplicate values in those columns, so the results would be valid if returned in either order.I'm surprised that this hasn't been flaky but it could hit us at any time.",no,,[],,
IMPALA-6213,497,"The partitioning compatibility check is wrong in consecutive outer join cases.Currently createAnalyticFragment() and createMergeAggregationFragment() uses child fragment partitioning info and refsNullableTupleId() to determine whether the child fragment partitioning can be directly adapted to the parent fragment without an extra exchange.It is wrong because: 
  <ol> 
   The output partition of an outer join node is always assigned its lhs input partition, which is not correct for full/right outer joins.refsNullableTupleId() seems to be designed to handle the outer join case, but can be broken by 2 consecutive joins.</ol> 
  Given the query 
  <SOURCECODE> 
  impala@3ddafcd29505614a01c8f4362396635c84ab4052 generates the following plan: 
  <SOURCECODE> 
  , which is wrong because the rows with t2.id=null can appear in any partition after the outer join.So it's incorrect to aggregate without an exchange.",no,,[],,
IMPALA-6226,498,"Analytic planner misses some opportunities to merge sort groups.<SOURCECODE> 
  The above analytical function query produces the following plan: 
  <SOURCECODE> 
  I believe we could avoid sorting twice by rewriting first_value() into last_value().I suspect there are a few other cases where the analytic functions can be ""flipped"" like this to handle a different sort order.The solution probably involves adding some extra cases to this logic: 
   <URL> to detect when the expressions are identical up to the ASC/DESC and when the function can be ""flipped"" to produce the correct result.",no,,[ss11_rework],,
IMPALA-6246,499,"Add timeline information to fragment instances in profile.We should add timeline information to each fragment instance in the profiles.First batch sent / received 
   Last batch sent / received",no,,[pf3_profiling],,
IMPALA-633,500,"enable metadata only operations for queries only against partition keys / virtual columns.Per 
   <URL> Impala could do a metadata only operation to answer this query.Might make sense to add this functionality.e.g:  select distinct partition_key_col from tab; This could simply query the Metastore for the LOV.",no,,[ss1_one_per],,
IMPALA-6367,501,"Compute stats do not update statistics for big tables.Table with at least 10000 partitions and 100 columns  The table is partitioned by day(bigint), string (this partition cardinality is no bigger than 100) 
  Executing compute incremental stats without dynnamic partitioning takes about 1 hour.So we use partitioning: 
  compute incremental stats table stats partition (some-condition) (I tried (day =X) ???????????? or (day = X , string_part = Y) or (day &lt; X and day &gt; X - 3days) ) 
  It finishes successfully but when I do show table stats for all the partitions in the range I get the following:  day string_part #Rows Incremental stats 
  1409529600 foo1 0 false 1409529600 foo2 0 false 
  The #Rows is 0 (the partition is not empty though) And ""Incremental stats"" column is set to false 
  Another case If I execute compute incremental stats table stats partition  
  and then show table stats  
  day string_part #Rows Incremental stats 
  1409529600 foo1 13 false 1409529600 foo2 13 false 
  The #Rows is updated but ""Incremental stats"" remains False.That's usually for smaller tables.Note that the same happens if I do not use partition clause  Note also that I ran compute stats (without incremental) only for the big table (on our test server) and it had the same effect 
  Note that on production intermittently(not always) it happens for small tables (#Rows is 0 after compute stats)  But for the biggest tables it's always 
  In Impala there are 2500 tables with almost 900.000 partitions (accross all tables) with average of 20 columns per table (or 90.000 columns accross all tables), The biggest table has about 35000 partitions We are using postgres provided by Cloudera as hive metastore backend 
  I am able to reproduce the issue in our testing setup - it has less than 100 tables and only one is big - 35000 partitions (which I copied from prod).",no,,[pf2_duration],,
IMPALA-6428,502,"Fix ALTER for tblproperty num_tablet_replicas.For Kudu tables created through Impala, we store a tblproperty called 'kudu.num_tablet_replicas', indicating the replication factor for the table.If this value is set in a create table, it is passed to Kudu and actually applied to the table.However, altering this property is allowed but has no effect on the underlying table, which could be confusing.We should either disallow altering the property, or we should pass the new value down to Kudu (if Kudu allows changing the replication factor after table creation)",no,,[],,
IMPALA-643,503,Update the query profile's QueryState and QueryStatus consistently..We currently update the QueryState and the QueryStatus info strings in the query profile completely independently which can result in counter-intuitive inconsistencies between them (due to the complexity and raciness of query cancellation in particular).Since the QueryState is functionally dependent on the QueryStatus we should always update them together to avoid such confusion.,no,,"[pf3_profiling, gk2_complexity]",,
IMPALA-6445,504,"Whitespace should be stripped or detected in kudu master addresses metadata.Currently the kudu master list metadata is split on ',' and directly fed through to Kudu.This means that if a user specifies a list such as ""m1, m2, m3"" with spaces after the commas, it will pass those hosts on to Kudu as ""m1"", "" m2"", and "" m3"".Two of those three hostnames are of course invalid and Kudu will only be able to connect when m1 is the active master.We should either strip those spaces or detect this case and throw an error on the bad metadata.(I prefer stripping)",no,,[],,
IMPALA-6492,505,"impalad ERROR log flooding in test runs.I noticed that our ERROR log files are being flooded in our build+test runs.I don't know if that is expected, but I suspect it is not.The impalad ERROR logs contain &gt;10MB of presumably useless/unintended output like this: 
  <SOURCECODE> 
  It's not clear whether this is a product or test infra issue.I'm marking this as a bug to be on the safe side",no,,[tk3_memory_unit],,
IMPALA-65,506,"Results from selecting string literals which have escaping include the escaping themselves.<SOURCECODE> 
  The escaping should have been removed.",no,,[],,
IMPALA-6544,507,"Lack of S3 consistency leads to rare test failures.Every now and then, we hit a flaky test on S3 runs due to files missing when they should be present, and vice versa.We could consider running our tests (or a subset of our tests) with S3Guard to avoid these problems, however rare they are.",no,,[],,
IMPALA-6569,508,"Admission Control: Query queued when pool is free.At times, observed that queries has been queued when pool is idle - no other queries are running in the pool.There are couple of incidents.Observation for one of the incidents is as follows: 
  Had come across a situation where in queue started building up on specific pool.On analysis, come to know that queuing happens only when query is hitting 1 specific node.Then executed a simple query directly (using impala shell) on that node has been queued, whereas, same query executed in other nodes are able to admit immediately and completes its execution even though both these runs are using the same pool.I think that&nbsp; problematic node had this problem and then it is clearing off automatically, but am not sure.But in this case, restarted daemon.I suspect somehow admission control accountability info (How much has used?How much left?etc) is not available in this node for some reasons.Please share your view on this.",no,,[],,
IMPALA-6574,509,"Update documentation with Kudu decimal support details.With&nbsp;
   <URL> committed the documentation should be updated to no longer say that the decimal type isn't supported in Kudu.",no,,[],,
IMPALA-6595,510,"Hit crash freeing buffer in NljBuilder::Close().I think this is related to the ExchangeNode's use of buffers.Repro: increase the mem_limit for this test: 
  <SOURCECODE> 
  Loop the test: 
  <SOURCECODE> 
  Boom: 
  <SOURCECODE> 
  
   <URL> I think you fixed a similar bug elsewhere in a join, so I'll assign to you initially.",no,,[tk6_iteration],,
IMPALA-6596,511,"Query failed with OOM on coordinator while remote fragments on other nodes continue to run.This&nbsp;is somewhat similar to 
   <URL>.Query&nbsp; 
  <SOURCECODE> 
  Failed on coordinator node which is also an executor with&nbsp; 
  <SOURCECODE> 
  Log on the coordinator has lots of entries with&nbsp; 
  <SOURCECODE> 
  From the memz tab on a different node.<h2><a name=""Memory Usage""></a>Memory Usage</h2> 
  Memory consumption / limit:&nbsp;142.87 GB&nbsp;/&nbsp;100.00 GB 
  <h3><a name=""Breakdown""></a>Breakdown</h3> 
  <SOURCECODE> 
  &nbsp; 
  It is strange that a consumption of&nbsp;143.63 GB is reported while the system has 128GB RAM.<SOURCECODE>",no,,"[ss4_memory, tk3_memory_unit]",,
IMPALA-6604,512,"rethink stress test binary search cutoff point.While working on 
   <URL> I see that during a binary search, a lot of queries that are meant to be tested with spilling are not set at any different mem_limit than they were for their non-spillable counterparts.This is due to both the default data scales of the stress test (TPCH 100, TPCDS 300) and the fact that <tt>MEM_LIMIT_EQ_THRESHOLD_MB = 50 in <tt>concurrent_select.py.For queries that only use roughly 200 MB, a 50MB threshold is a rather large percentage.This means the binary search is ""giving up"" sooner than it could.There are some options: 
  <ul class=""alternate"" type=""square""> 
   drop MEM_LIMIT_EQ_THRESHOLD_MB completely and only rely on MEM_LIMIT_EQ_THRESHOLD_PC, which is 0.975.For a 200 MB query, this would effectively be a threshold of 5MB.This would lead to a more precise binary search, but one that would take longer to complete.keep MEM_LIMIT_EQ_THRESHOLD_MB but only put it in play for either larger queries or larger data sets.do not test at these data scales at all and instead move toward testing at data scales of, say, TPCH 1000",yes,performance stressing test,"[tk3_memory_unit, gk8_spilling, tk10_binary_search, tk7_concurrent]",,
IMPALA-6627,513,"Document Hive-incompatible behavior with the serialization.null.format table property.Impala only respects the ""serialization.null.format"" table property for TEXT tables and ignores it for Parquet and other formats.Hive respects the ""serialization.null.format"" property even for other formats, converting matching values to NULL during the scan.There's is a separate discussion to be had about which behavior makes more sense, but let's document this as an incompatibility for now since it has come up several times already.",no,,[],,
IMPALA-6632,514,"Document compatibility of table and column stats between Impala and Hive.The question of compatibility between the table and column stats between Hive and Impala comes up quite often, so is worth documenting explicitly.Quoting myself from a recent discussion thread to get the docs effort started: 
  Commonalities: 
  <ul class=""alternate"" type=""square""> 
   Hive and Impala both store row counts at the table level and partition level.Hive also computes and stores additional stats like file counts which Impala does not need or use.Differences: 
  <ul class=""alternate"" type=""square""> 
   Impala computes and stores column-level stats like the number of distinct values (NDV) only at the table level, and not at the partition level.Hive computes and stores column-level stats at the partition level.Impala does not follow this approach because the per-partition NDVs cannot be sensibly combined for queries that access multiple partitions.In short, the column stats for partitioned tables are not compatible between Impala and Hive (because imo Hive's approach does not make sense).Impala uses a more modern and tuned algorithm (HyperLogLog++) for estimating the number of distinct values, so they tend to be more accurate than Hive's.Your mileage may vary.For unpartitioned tables, the Hive and Impala column stats are compatible.For partitioned tables, the table-level column stats that Impala writes in the Metastore are stored just like for unpartitioned tables.These statistics are ""available"" to Hive in the sense that the standard retrieval APIs will work as expected.My understanding is that for partitioned tables, Hive does not use the table-level column stats, but instead expects partition-level column stats.As I've said before, these partition-level column stats do not make any sense because it is not possible to sensibly combine them for multiple partitions.",no,,[],,
IMPALA-6673,515,"Be smarter about I/O patterns for Parquet scan ranges.Currently the Parquet scanner is somewhat naive about how it issues column scan ranges: it issues a separate scan range per column, in the order that the the column readers are organised internally.If the column ranges are large (i.e. multiple I/O buffers) or we're reading from SSDs where random access is fairly efficient, this may not matter very much.However, this approach is suboptimal when reading smaller columns (e.g. highly compressed) from spinning disks for two reasons: 
  <ol> 
   Some columns may be adjacent in the file.If we are reading each column into a single smaller I/O buffer but multiple columns would fit in a larger I/O buffer, we would probably be better off doing a single I/O for that column.We are reading the columns in a fairly random order, because the I/O mgr does round robin on the scan ranges in the order they were added.Sorting the scan ranges by file offset would improve the odds of being able to read each subsequent column without an additional seek and will also improve locality for the disk's internal cache.Based on some superficial googling, a lot of drives have 64M or 128M internal caches, which is large enough that it could be useful but small enough that, if we do I/O from a 256MB+ Parquet file in a completely random order, we're reducing the chances of getting cache hits significantly.</ol> 
  
   <URL> may help a lot here, since it will tell us upfront what the memory budget is for I/O.",no,,"[ss1_one_per, gk1_efficiency, sk7_buffer, ss13_cache, ss4_memory, tk3_memory_unit, gk3_reduce]",,
IMPALA-6693,516,"Crash in impala::LibCache::GetCacheEntryInternal().A crash with the following stack trace was observed in one of our nightly builds: 
  <SOURCECODE>",no,,[],,
IMPALA-6760,517,"Recent change to run-tests.py broke build on python2.6.Commit <tt>
    <URL> &amp; 
    <URL> Update run-tests.py to handle exit_code 5 introduced set literal syntax, which doesn't work with older pythons.The error you get is: 
  <SOURCECODE>",no,,[],,
IMPALA-6799,518,"Impala Doc: Document GRANT and REVOKE to Users.Update documents to reflect new GRANT, REVOKE, and SHOW statements.",no,,[],,
IMPALA-6818,519,"Rethink data-stream sender/receiver startup sequencing.<URL> introduced parallel fragment startup, which is good for startup latency.However, it meant that data-stream senders can start before receivers, and there is a timeout to handle the case when the receiver never shows up: 
  <SOURCECODE> 
  We see this timeout fairly regularly (e.g. when&nbsp;a host has a spike in load and does not process the exec rpc for a while).Let's rethink how this works to see if we can make it robust but being careful to not sacrifice startup time too much.",no,,"[ss8_load_nn, sk8_timeout_expiration, gk5_latency]",,
IMPALA-6846,520,"Impala does not retrieve Sentry roles after restart.Environment: 
  CDH 5.14.2 (upgraded from CDH 5.13.0 that was&nbsp;also affected), Sentry configured, Hadoop group mapping via Ldap (OpenLDAP).The cluster is NOT Kerberized.Issue description: 
  There are some roles setup on Impala+Sentry: 
  <SOURCECODE> 
  Just after permissions grant was made, al works fine and users are assigned correct roles, which is confirmed by SHOW CURRENT ROLES on Impala side.But after Impala restart, any user that is logged to Impala does not have any roles.So, SHOW CURRENT ROLES returns just nothing.And users do not have any permissions, even users who are in&nbsp;gn:ldap:admin (it is stated as sentry.service.admin.group in Sentry).hdfs groups &lt;username&gt; returns correct groups for a user, so group mapping obviously work fine.Also, I can see the correct roles for a user, if I log to Hue-&gt;Security-&gt; Roles.So that the issue seems to be somewhere between Impala and Sentry.The workaround for this that I am currently using is to create another set of roles as dba1, etl1, bi1 and grant them to&nbsp;appropriate groups.Another workaround is to stop Sentry service, log in to Sentry database, drop all the tables, recreate Sentry schema and then create roles and grant permissions again.At the same time, I see a lot of such logs in Sentry Server logs: 
  <SOURCECODE> 
  Also, a lot of these logs in Impala Catalog Server logs: 
  <SOURCECODE> 
  Please let me know what other information is needed.",no,,[ss3_in_one_go],,
IMPALA-6858,521,"Add type information to query profiles.If query profiles (or structured plans) had table metadata information like types, testing Impala on a pre-existing query profile would be easier, since we could automate generating the table scaffolding.",no,,[pf3_profiling],,
IMPALA-6873,522,"Crash in Expr::GetConstVal() due to NULL dereference.Log file crashing frame 
  <SOURCECODE> 
  Crashing stack, extracted from core dump 
  <SOURCECODE> 
  &nbsp; 
  Missing frames are from the JVM and are below (extracted from hs_err_pid file) 
  <SOURCECODE> 
  So the root cause seems to be in the <tt>memcpy() in the following piece of code in expr.cc 
  <SOURCECODE> 
  Few observations: 
  <ul class=""alternate"" type=""square""> 
   The query crashes the coordinator during the query compilation/analysis (as evident from the JVM stack trace) 
   The root cause seems to be due to a malformed <tt>StringVal (ptr = NULL and len &gt;0) returned by <tt>GetStringVal and it is unclear at this point which specific function/piece of code is generating that.In this particular case, I figured that the ScalarFn in the crashing stack that is calling <tt>GetConstVal is <tt>concat() and removing it doesn't crash the coordinator.Unable to reproduce it locally on my dev box 
   The problematic piece of code memcpy'ing the NULL ptr is introduced by 
    <URL> and removed by 
    <URL>.Hence only 2.9.0 and 2.10.0 are the affected versions 
   
  Next Steps: 
  <ul class=""alternate"" type=""square""> 
   Avoid the crash by having a stricter is_null check on the output StringVal 
   Figure out which possible builtins can generate such StringVals.",no,,[],,
IMPALA-6874,523,"Add more tests for mixed-format tables, including parquet.We only have a single very basic table with mixed formats that I can see.It would be good to have a larger table that includes parquet, so we can exercise some of the more interesting memory reservation code paths.I think the requirements are: 
   
   Files of at least multiple MBs in size 
   Includes Parquet and at least one row-based format 
   Ideally a mix of different file and partition sizes 
   
  We want queries that exercise ""interesting"" code paths: 
   
   Many columns returned: select * from table.Single column returned 
   No columns returned - e.g. select count<IMAGE> 
   Selective scan with predicates",no,,[],,
IMPALA-6941,524,"Allow loading more text scanner plugins.It would be nice if Impala supported loading plugins for scanning additional text formats aside from LZO - the current logic is fairly specialized but could easily be extended to load libraries for codecs like LZ4 and ZSTD if available.It's kind of weird that we only support that one format.This might help a bit with 
   <URL> and 
   <URL> since we could test the plugin-loading mechanism without relying on the external Impala-lzo codebase.",no,,[],,
IMPALA-6951,525,"java7 vs. java8 warning prevents installation of impala-python on test machines.Commit <tt>22c7ded07eb2710aba3e1aa07ed7ec1a448f7c61 now prevents the installation of impala-python on a test machine wanting to run tests against an Impala cluster.It was very convenient to be able to install impala-python and run tests like the stress test regardless of the Java version on the host.In order to install impala-python, one must source impala-config.sh.When the failpoint is there, then impala-python now MUST have Java 8.This is a burden to update.",no,,[],,
IMPALA-6963,526,"Consider allowing extension of the builtin list of compression types.Following on from 
   <URL>, we could make this more flexible by allowing extension of the list of supported compression types (e.g. the HdfsCompression and THdfsCompression enums) via configuration.",no,,[],,
IMPALA-6974,527,"Static and shared compilation don't have compatible ccache entries.In be/CMakeLists.txt, we add an -fPIC flag unconditionally: 
  <SOURCECODE> 
  When building in static mode, every command will have an -fPIC flag.However, when building in shared mode, CMake adds -fPIC (or -fPIE) automatically, so every command will have -fPIC plus another -fPIC added by CMake.These extra flags prevent ccache from sharing entries between static and shared builds even after preprocessing.",no,,[],,
IMPALA-7036,528,"TestKuduOperations.test_local_tz_conversion_ops() fails with missing row.A 2.x core dataload job failed with: 
  <SOURCECODE> 
  The test is executing these SQLs: 
  <SOURCECODE> 
  Only saw this once so far.",no,,[],,
IMPALA-7082,529,"Show Human Readable Size in Query Backend Page.Currently, the ????????????ak mem.consumption???????????? column in the Query Backends pages is in raw bytes (see attachment).Since we introduce a JS function to convert Long size value into human readable size in&nbsp;
   <URL>, it can be reused here.",no,,[sk3_byte],,
IMPALA-711,530,"Need Impala native datatype for currency for precision storing / calculations of amounts.Storage and calculation of amounts, conversion rates etc. cannot use float/double and need fixed precision datatypes.Lack of support for this in latest impala version (1.2.1) is issue.The data type should be natively implemented in impalad for performance as lot of number crunching happens on these types.It is not practical to implement this as Java UDF (for same reason).",yes,performance improvement,[gk4_performance],,
IMPALA-7136,531,"Set a default THREAD_RESERVATION_AGGREGATE_LIMIT value.Similar to 
   <URL>, we should consider setting a default value for this option that will reject queries that are likely to be problematic because of scalability concerns.The aggregate thread count really depends on scalability, so 
   <URL> probably has the best idea of what a realistic limit is here after the KRPC changes.",no,,[],,
IMPALA-72,532,"Partition keys with path separators erroneously result in multi-level partition directories.Repro: 
  in hive: ""create table string_part_test(id int) partitioned by(s1 string, s2 string);"" 
  in impala:  localhost:21000] &gt; insert into table string_part_test partition(s1, s2) select id,date_string_col,string_col from alltypessmall; Query: insert into table string_part_test partition(s1, s2) select id,date_string_col,string_col from alltypessmall Inserted 100 rows in 22.46s 
   <ERROR></ERROR> &gt; select * from string_part_test; Query: select * from string_part_test Query finished, fetching results ... 
  Returned 0 row(s) in 0.76s &lt;--- no rows 
  It looks to me like the files are written into hdfs correctly but the hive metastore partition mappings are not.select * from hive also returns no results.inserting the table from hive works for both hive and impala.",no,,[],,
IMPALA-7249,533,"Cancel shutdown of impalad.Following on from 
   <URL>, it could be useful to cancel shutdown for some use cases.An extension would be to allow extending the deadline.",no,,[],,
IMPALA-73,534,"JVM crash causes Impala to freeze.I'm running ""impalad version 0.5 RELEASE (build 61ac362913e8626e01833bfb1c083f9f7190a0e0)"" on a 7-node cluster.I've noticed that queries that are simple (""select count<img class=""emoticon"" src=""https://issues.apache.org/jira/images/icons/emoticons/star_yellow.png"" height=""16"" width=""16"" align=""absmiddle"" alt="""" border=""0""> from one_gigabyte_table"") are freezing.Digging in a bit deeper, I found that the JVM inside of the FE has crashed.<SOURCECODE> 
  I don't currently know how to reproduce this, but perhaps the crash dump is useful.",no,,[],,
IMPALA-733,535,"Improve Parquet error handling for low disk space.If HDFS has less than 1 GB free (or I presume whatever value is set in the PARQUET_FILE_SIZE query option), INSERT into a Parquet table fails even for tiny amounts of data.That might be unavoidable, but the error should be communicated more clearly to the user.INSERT ... VALUES reports that N rows were inserted (no error at all), but the expected data is missing when the table is queried.INSERT ... SELECT gives a cryptic error message but still reports that the rows were inserted, although they aren't.Repro: 
  About 400 MB free.(This is a VM that keeps getting filled up by Impala-related logs.)$ df -k .Filesystem 1K-blocks Used Available Use% Mounted on /dev/vda1 24607156 23961976 395184 99% / 
  I was going to answer a question on the mailing list by showing an INSERT going from an unpartitioned to a partitioned table.<ERROR></ERROR> &gt; create table unpart (year int, s string) stored as parquet; Query: create table unpart (year int, s string) stored as parquet 
  Returned 0 row(s) in 0.12 seconds
  INSERT ... VALUES looks like it succeeds, but the data isn't really there.<ERROR>
  Returned 0 row(s) in 0.22 seconds
  
   <ERROR></ERROR> &gt; select * from unpart; Query: select * from unpart 
  Returned 0 row(s) in 0.22s 
  Copying the data out of a text table, the error is reported but it doesn't say specifically ""out of space"".And the ""Inserted 2 rows"" message raises the hope the data made it in, but it didn't.<ERROR></ERROR> &gt; insert into unpart select * from t1; Query: insert into unpart select * from t1 
  ERRORS ENCOUNTERED DURING EXECUTION: Backend 0:Failed to close HDFS file: hdfs://127.0.0.1:8020/user/hive/warehouse/partitioning.db/unpart/.impala_insert_staging/284cf98f761aec95_5712ef093b357195//.2903970254304242837-6274340053807624598_1840160694_dir/2903970254304242837-6274340053807624598_1083629803_data.0 Error(255): Unknown error 255 
  Inserted 2 rows in 0.34s 
   <ERROR></ERROR> &gt; select * from unpart; Query: select * from unpart 
  Returned 0 row(s) in 0.22s 
  After all this, the data directory contains a leftover staging subdirectory (empty) and a zero-byte data file: 
  $ hdfs dfs -ls hdfs://127.0.0.1:8020/user/hive/warehouse/partitioning.db/unpart Found 2 items drwxrwxrwx - impala supergroup 0 2014-01-08 11:39 hdfs://127.0.0.1:8020/user/hive/warehouse/partitioning.db/unpart/.impala_insert_staging <del>rw-r</del><del>r</del>- 1 impala supergroup 0 2014-01-08 11:39 hdfs://127.0.0.1:8020/user/hive/warehouse/partitioning.db/unpart/3188829493227009611-3605612775229973420_1967882694_data.0 
  Suggestions: 
  <ul class=""alternate"" type=""square""> 
   Make INSERT ... VALUES detect/report the HDFS error trying to write the block.Don't report number of rows inserted.<ul class=""alternate"" type=""square""> 
   Make INSERT ... SELECT error clearer, either suggest it could be out-of-space or do some followup check for $(PARQUET_FILE_SIZE) space free.Don't report number of rows inserted.<ul class=""alternate"" type=""square""> 
   Be cleaner about leftover staging directories and empty data files.(Shouldn't the data file stay in the staging directory until it's successfully closed?)<ul class=""alternate"" type=""square""> 
   Whatever distributed is checking is needed so the error is handled if it's a remote node that runs out of space, rather than the coordinator node like in this case with a single VM.",yes,dynamic profiling data for performance,"[pf1_percentage, tk3_memory_unit, sk2_block_hang_crash, pf2_duration, sk3_byte]",,
IMPALA-7366,536,"Remove MarkNeedsDeepCopy() from UnionNode.This case is a bit trickier than the others.The issue is that some plan nodes, like HdfsScanNode, hold onto memory that is referenced by returned batches and don't necessarily attach the memory before returning *eos.E.g. if you have a scan with a limit feeding into a UnionNode, the scan may terminate earlier before it attaches all memory to its output batches, then HdfsScanNode::Close() will free that memory.Thus it's not safe for the UnionNode to close its child until all referenced data has been copied out.The fix is either to attach the memory to the final batch in Close() or to have a new method like FlushResources() that is called before Close().",no,,"[ss4_memory, ss2_all_to_one]",,
IMPALA-7393,537,"Test infra should log query IDs.I'm debugging 
   <URL>.It would be more convenient to correlate logs and the test failure if the test logged the query ID.",no,,[],,
IMPALA-7453,538,"Intern HdfsStorageDescriptors.Every partition currently has an HdfsStorageDescriptor attached.In most cases, the number of unique storage descriptors in a warehouse is pretty low (most partitions use the same escaping, file formats, etc).For example, in the functional test data load, we only have 24 unique SDs across ~10k partitions.Each object takes 32 bytes (with compressed oops) or 40 (without).So, we can get some small memory/object-count savings by interning these.",no,,"[ss8_load_nn, sk3_byte]",,
IMPALA-7474,539,Tool to identify CPU bottlenecks.We run into a bunch of issues where we run impala into hangs or impacts query performance issues due to a very high CPU usage.A tool which periodically collects stacks from impala (when enabled) and prints calls with high CPU usage would be very useful for debugging such issues.Running this tool should ideally incur a minimalistic&nbsp;overhead on impalad while collecting the stacks.,yes,CPU bottlenecks,"[gk4_performance, sk2_block_hang_crash, sk14_overhead, ss5_cpu]",,
IMPALA-7519,540,"Support elliptic curve ssl ciphers.Thrift's SSLSocketFactory class does not support setting ciphers that use ecdh.We already override this class for others reasons, it would be straightforward to add the necessary openssl calls to enable this.",no,,[],,
IMPALA-7523,541,"Planner Test failing with ""Failed to assign regions to servers after 60000 millis."".I've seen <tt>org.apache.impala.planner.PlannerTest.org.apache.impala.planner.PlannerTest fail with the following trace: 
  <SOURCECODE> 
  I think we've seen it before as indicated in 
   <URL>.",no,,[],,
IMPALA-7534,542,"Handle invalidation races in CatalogdMetaProvider cache.There is a well-known race in Guava's LoadingCache that we are using for CatalogdMetaProvider which we are not currently handling: 
  <ul class=""alternate"" type=""square""> 
   thread 1 gets a cache miss and makes a request to fetch some data from the catalogd.It fetches the catalog object with version 1 and then gets context switched out or otherwise slow 
   thread 2 receives an invalidation for the same object, because it has changed to v2.It calls 'invalidate' on the cache, but nothing is yet cached.thread 1 puts back v1 of the object into the cache 
   
  In essence we've ""missed"" an invalidation.This is also described in this nice post: 
   <URL> 
  The race is quite unlikely but could cause some unexpected results that are hard to reason about, so we should look into a fix.",no,,"[ss13_cache, gk7_speed]",,
IMPALA-7544,543,"Add a test for the GDB helpers.In a recent change we added some GDB helpers to enumerate fragment instances and query IDs.We should add a test to make it easier to add functionality in the future and prevent regressions, either by changing the script of by modifying Impala.Ideally the test would run a query and then pull a core, and we could also check in a compressed core file (if it is small enough) for faster iterations.The latter won't prevent regressions from changing Impala though.",no,,"[sk13_regression, gk7_speed, tk6_iteration]",,
IMPALA-7545,544,"Add admission control status to query log.We already include the query progress in the HS2 GetLog() response (although for some reason we don't do the same for beeswax) so we should include admission control progress.We should definitely include it if the query is currently queued, it's probably too noisy to include once the query has been admitted.We should also do the same for beeswax/impala-shell so that live_progress/live_summary is useful if the query is queued.We should look at the live_progress/live_summary mechanisms and extend those to include the required information to report admission control state.",no,,[],,
IMPALA-757,545,"Like predicate with ""%%"" pattern returns empty result set..<SOURCECODE> 
  Buggy query with ""%%"" 
  <SOURCECODE>",no,,[pf1_percentage],,
IMPALA-7571,546,"is_member() function to determine if the current user is in a group.A function which returns if a user is member of a group.Basically, depending on user groups membership, we&nbsp;could mask or hide certain columns&nbsp;or rows of data, similarly how it's implemented in Dremio.Similar concept is available in Oracle, it's called Oracle Virtual Private Database.It can be implemented for other user-to-group mappings too.For example Dremio, uses a cache for is_member() calls&nbsp;(defautls to 24h), so this group membership check can be cached and consequentially&nbsp;resolved very quickly.For reference: 
  
   <URL> 
  
   <URL> 
  
   <URL>",no,,[gk7_speed],,
IMPALA-7585,547,"Always set user credentials after creating a KRPC proxy.<tt>kudu::rpc::Proxy ctor may fail in <tt>GetLoggedInUser() for various reason: 
  <SOURCECODE> 
  This resulted in an empty user name being used in <tt>kudu::rpc::ConnectionId.With plaintext SASL (e.g. in an insecure Impala cluster), this may result in the following error: 
  <SOURCECODE> 
  While one can argue that Kudu should fall back to some default username (e.g. ""cpp-client"") when <tt>GetLoggedInUserName() fails, it may have non-trivial consequence (e.g. generating an authn token with some random username on one machine while using the real user name on another machine).Therefore, it's best for Impala to explicitly set the user credentials (impala/&lt;some-hostname&gt;) after creating the proxy.",no,,[],,
IMPALA-760,548,"Export query results directly to HDFS.When query results are large it is inefficient to fetch the results to a single client and store on local disc or even impossible due to limited ressources.The time to fetch the results and materialize to disc becomes the dominating factor of the overall query runtime.Currently, the only way to write the results to HDFS instead is inserting into a table which is fine if you want to query the results later on.But there should be a possibility to ""export"" the results directly to HDFS without inserting into a table or fetching the results to the client and push to HDFS.Especially for ETL transformations this would be very helpful.",no,,[gk1_efficiency],,
IMPALA-7704,549,"ASAN tests failing in HdfsParquetTableWriter.ASAN tests have been failing for the last few runs.Here is the output: 
  <SOURCECODE>",no,,[],,
IMPALA-7723,550,"Recognize int64 timestamps in CREATE TABLE LIKE PARQUET.<URL> adds support for reading int64 encoded Parquet timestamps.These columns have int64 physical type, and converted/logical types has to be used to differentiate them from BIGINTs.These columns can be read both as BIGINTs and TIMESTAMPs depending on the table's schema.CREATE TABLE LIKE PARQUET could also convert these columns to TIMESTAMP instead of BIGINT, but I decided to postpone adding this feature for two reasons: 
  1.It could break the following possible workflow: 
  <ul class=""alternate"" type=""square""> 
   generate Parquet files (that contain int64 timestamps) with some tool 
   use Impala's CREATE TABLE LIKE PARQUET + LOAD DATA to make it accessible as a table 
   run some queries that rely on interpreting these columns as integers 
   
  CAST (col as BIGINT) in the query would make this even worse, as it would convert timestamp to unix time in seconds instead of micros/millis without any warning.2Adding support for int64 timestamps with nanoseconds precision will need Impala's parquet-hadoop-bundle dependency to be bumped to a new major version, which may contain incompatible API changes.Note that parquet-hadoop-bundle is only used in CREATE TABLE LIKE PARQUET.The C++ parts of Impala only rely on parquet.thrift, which can be updated more easily.",no,,"[ss8_load_nn, pf2_duration]",,
IMPALA-774,551,"Udfs returning strings cannot be run from the FE..In FeSupport, we don't initialize the runtime state correctly to be able to run UDFs that return strings.",no,,[],,
IMPALA-7750,552,"Prune trivial ELSE clause in CASE simplification.A trivial optimization is to omit ELSE if it adds no value: 
  <SOURCECODE> 
  The <tt>ELSE case defaults to null if not provided, so the above can be rewritten to: 
  <SOURCECODE> 
  Also, the simplification can omit the trailing <tt>WHEN clause if it returns <tt>NULL.For example: 
  <SOURCECODE> 
  Should be simplified to: 
  <SOURCECODE>",no,,[ss11_rework],,
IMPALA-7784,553,"Partition pruning handles escaped strings incorrectly.Repro: 
  <SOURCECODE> 
  Hive returns the row for both queries.",no,,[],,
IMPALA-7787,554,"python26-incompatibility-check failed because of docker 503 Service Unavailable.<URL> 
   <URL> 
  <SOURCECODE> 
  This happened a couple of times.Looks like flakiness but unsure if it was just a transient infra issue or something we're doing wrong.",no,,[],,
IMPALA-7830,555,"Improve Scheduler Tracing.The behavior of the scheduler is often difficult to understand.We should add tracing of its inputs (scan range, block location metadata), its decisions, and the resulting schedule.This should be configurable by a query option and/or a query hint, and should optionally be written into the log and/or the profile.",no,,"[sk2_block_hang_crash, pf3_profiling]",,
IMPALA-7866,556,"Predicates, helpers for implicit casts, slot refs.A recent change introduced a few more <tt>Expr predicates.This ticket asks for a few more: for implicit casts and slot refs which are currently sprinkled throughout the code.",no,,[],,
IMPALA-7888,557,"Incorrect NumericLiteral overflow checks for FLOAT, DOUBLE.Consider the following (new) unit test: 
  <SOURCECODE> 
  This test fails (that is, the value zero, so the method claims, overflows a FLOAT.)The reason is a misunderstanding of the meaning of <tt>MIN_VALUE for Float: 
  <SOURCECODE> 
  For Float, <tt>MIN_VALUE is the smallest positive number that Float can represent: 
  <SOURCECODE> 
  The value that the Impala code wants to check it <tt>- Float.MAX_VALUE.The only reason that this is not marked as more serious is that the method appears to be used in only one place, and that place does not use <tt>FLOAT values.",no,,[pf2_duration],,
IMPALA-7902,558,"Revise NumericLiteral to avoid analysis, fix multiple issues.The <tt>NumericLiteral class is a leaf node in the Impala FE AST.In order to address a number of pending issues, we must adjust this class to improve its semantics and fix a number of bugs.See the linked JIRA tickets for the issues that this roll-up ticket addresses.The issues are combined because they are tightly related; makes more sense to offer them as a single patch than as a series of disjoint patches.",no,,[],,
IMPALA-7975,559,"Improve supportability of the automatic invalidate feature.Some of the things which can be done to improve supportability of this feature: 
   
   Add metrics to detect issues pertaining to this feature 
   
  <ol> 
   Time taken to fetch the notifications (Would be useful to have average, min, max) 
   Time taken to process a batch of events received 
   Number of times particular table was invalidated (would be useful to have some rate metric like number_of_invalidates/per_hour) 
  </ol> 
   
   Ability to turn ON/OFF for this feature (possibly without the need of a restart)",no,,[pf2_duration],,
IMPALA-8002,560,"Unstable join ordering for equivalent tables.Consider the following test: <tt>PlannerTest.testJoins(): 
  <SOURCECODE> 
  Despite no changes in the planner code, on one run the order flipped to the above.Previously, 01 was t1 and 00 was t2.Likely, the behavior when two tables are equivalent has some kind of non-determinism, perhaps storing candidates in a Java Set or Map with undefined order.",no,,[],,
IMPALA-8021,561,"Add estimated cardinality to EXPLAIN output.The EXPLAIN output provides much useful information in the plan tree.All our planning decisions are based on cardinality; but it appears in the EXPLAIN output only for the EXTENDED level.The profile only contains the plan from the STANDARD level.This change proposes to include row size and cardinality even in the STANDARD level.The nodes that have the information call it ""cardinality"", so continue to use that term.Add cardinality to each node so it appears something like this: 
  <SOURCECODE> 
  Cardinality should appear in all levels above MINIMAL.Cardinality is not needed for EXCHANGE since it can be inferred from other nodes.Also, the existing code prints large cardinalities in detail: 1234567890, which is hard to read.Use the abbreviated output, using metric (power of 1000) units, so 1.23G instead.",no,,"[sk1_negative_necessary, pf3_profiling]",,
IMPALA-8025,562,"End-to-end tests sometimes unhelpfully truncate error output.Made a change to the DESCRIBE output per 
   <URL>.This required adjustment to <tt>metadata/test_explain.py to account for the change.The test encodes a ""golden"" version in a .test file using a specialized syntax.But, when running the test, the output shows the first few lines (which do match), then elides the rest: 
  <SOURCECODE> 
  As it turns out, passing ""-vv"" to <tt>tests/run-tests.py does not seem to pass it to the test program, so that did not work.The .xml file for the test contains the same message: output is truncated.Same in the .log file.So, the question is, how is a developer to figure out the issue if we can't see the actual error lines?This is the kind of thing that converts a simple task into a multi-hour ordeal.Right now, the only solution is to rerun the tests with <tt>--update_results flag to <tt>run-tests.py, then hunt down the generated output file.Better would be to output the n lines before the error, rather than the first n lines.",no,,[ss1_one_per],,
IMPALA-8026,563,"Actual row counts for nested loop join are way too high while the query is executing.Consider this extract from a query plan: 
  <SOURCECODE> 
  If the above is to be believed, the 06 nested loop join produced 5 billion rows.But, the actual number is far too huge for that: joining 1 row with 10 million rows cannot produce 500 times that number of rows.It appears that the nested loop join actually processed and returned the 9.5 million rows, since that is the same number produced by the 10 hash join which joins a single row with the output of the nested loop join.Because this same bogus result appears across multiple plans, it is likely that the actual number is completely wrong and bears no relation to the number of rows actually returned.",no,,[tk6_iteration],,
IMPALA-8072,564,"Clean up config files in docker containers.Currently the docker containers include a bunch of config files copied indiscriminately from the dev environment.Mostly these aren't valid for a production container and it's expected that the real config files will be mounted at /opt/impala/conf.We should instead include a more reasonable set of default configs (e.g. for admission control), plus placeholders for other config files that may need to be overridden with site-specific configs.",no,,[],,
IMPALA-8080,565,"Improve planner to use disk attributes when applicable.The disk seek time can vary depending on the underlying storage (Magnetic Disk, SSD, Flash etc) and the scan performance can change depending on the presence of buffer cache.The plan generation, Degree of Parallelism, and scheduling should look at these aspects to decide on the appropriate course of action.",yes,disk seek time performance,"[ss13_cache, gk4_performance]",,
IMPALA-8081,566,"Use appropriate degree of parallelism.The Degree of Parallelism should be appropriate for the operation being performed.It looks like it is currently either serial, or use as many resources as possible.Sometimes over parallelizing can result in bad performance.",yes,over parallelizing can result in bad performance.,[gk4_performance],,
IMPALA-8147,567,"Merge make_impala.sh into CMake and buildall.sh.make_impala.sh seems to be an unnecessary layer in the build system between buildall.sh and the CMake scripts.Per discussion on dev@, I think we should: 
   
   Move any logic required for doing incremental builds into CMakeLists.E.g. gen_build_version.py 
   maybe consider moving the CMake invocation logic into a separate script that does one clear-cut thing 
   Move the make invocation logic into a function in buildall.sh 
   Add support to buildall.sh to build both the debug and release binaries.",no,,"[ss1_one_per, sk1_negative_necessary]",,
IMPALA-8209,568,"Fragment instance ID no longer displayed on /memz.The change for 
   <URL> dropped the fragment instance ID from the runtime-state profile.However, this also removed it from the /memz page.We should add it back there.<URL>",no,,[pf3_profiling],,
IMPALA-8232,569,"Custom cluster tests should allow setting dfs.client settings for impalads.Right now, custom cluster tests only allow specifying impalad startup options, however, it would be nice if the tests could specify arbitrary HDFS client configs as well (e.g. <tt>dfs.client options).This would allow us to increase our test integration coverage with different HDFS client setups such as (1) disabling short-circuit reads (thus triggering the code path for a remote read) (requires setting <tt>dfs.client.read.shortcircuit to false), (2) enabling hedged reads (requires setting <tt>dfs.client.hedged.read.threadpool.size&nbsp;to a value greater than 0).",no,,[],,
IMPALA-8244,570,"Toolchain build fails to publish binaries even if asked to do so.The recent commit to native-toolchain that enabled the publishing of binary artifacts from Docker-based toolchain builds (
   <URL>&nbsp;changed the logic that gates the publishing calls.Internal builds on Jenkins (using the same logic as before the change) now don't even attempt publishing even if parameterized to do so.Excerpt from the build log: 
  <SOURCECODE> 
  The&nbsp;happy case with the missing steps (copied from another job that published the bits successfully): 
  <SOURCECODE>",no,,[],,
IMPALA-843,571,"Impalad's crash while running join on string column + union.I only tried this on master, 542de067b97ee46e6293fcaa71d41bb52a377c83 
  <SOURCECODE> 
  the imapalad log says 
  <SOURCECODE> 
  I've tried other variations, such as no union, no join, join on int type, and they all succeeded.",no,,[],,
IMPALA-925,572,"JDBC driver returns so results from getTables() and getColumns with null tableNamePattern.In MetadataOp.java, getDbsMetadata() won't return any table if tableName is null.But null should be treated the same as ""%"".",no,,[pf1_percentage],,
IMPALA-978,573,"Admission control error messages should have more actionable details.From Alan: 
  <blockquote> 
   User who is new to impala doesn't know much about the semantics of admission control only got a sense that the query doesn't run.The error msg doesn't suggest the cause and how to resolve it.Here's some of my thought 
   1.Query rejected right away Due to insufficient memory, (such as the estimated memory usage is 100GB when the system only has 10GB), the query is rejected right away.The ""root cause"" is communicated in the error message, but we should also suggest a way to resolve it: either force it through, reduce the join, increase the memory, etc.2Timeout when waiting for resource The current message simply say timeout.We should also mention why it's put in the queue and be a bit more descriptive on why we couldn't run it (i.e. we can't allocate the required amount of memory, or a slot).</blockquote>",no,,"[ss4_memory, tk3_memory_unit, sk8_timeout_expiration, gk3_reduce]",,
IMPALA-992,574,"rerun past queries from history.With the history command in impala shell shows user's query history with numbers, it would be nice to allow user to rerun past queries specified by the command history number.One idea we can borrow from is linux/unix history command: user rerun the command by typing exclaimation mark and the history number, e.g. ""!100""",no,,[],,
KAFKA-1084,575,"Validate properties for custom serializers.We use specifics encoder/decoder for our producers/consumers, they get correctly initialized by the Producer/Consumer.The only downside is the validate() function of VerifiableProperties that pollutes our log stream.This patch allows custom serializers keys to validate correctly if they begin with the ""external"" prefix, for example: 
  external.my.encoder.param=true 
  will not raise a WARN.",no,,[],,
KAFKA-1389,576,"transient unit test failure in ProducerFailureHandlingTest.Saw the following transient unit test failure.kafka.api.ProducerFailureHandlingTest &gt; testTooLargeRecordWithAckZero FAILED junit.framework.AssertionFailedError: Partition 
   <ERROR></ERROR> metadata not propagated after timeout at junit.framework.Assert.fail(Assert.java:47) at junit.framework.Assert.assertTrue(Assert.java:20) at kafka.utils.TestUtils$.waitUntilMetadataIsPropagated(TestUtils.scala:532) at kafka.utils.TestUtils$$anonfun$createTopic$1.apply(TestUtils.scala:151) at kafka.utils.TestUtils$$anonfun$createTopic$1.apply(TestUtils.scala:150) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244) at scala.collection.immutable.Range.foreach(Range.scala:141) at scala.collection.TraversableLike$class.map(TraversableLike.scala:244) at scala.collection.AbstractTraversable.map(Traversable.scala:105) at kafka.utils.TestUtils$.createTopic(TestUtils.scala:150) at kafka.api.ProducerFailureHandlingTest.testTooLargeRecordWithAckZero(ProducerFailureHandlingTest.scala:115)",no,,[sk8_timeout_expiration],,
KAFKA-2742,577,"SourceTaskOffsetCommitter does not properly remove commit tasks when they are already in progress.The current implementation is relying on ScheduledExecutorService to cancel the task, but this doesn't handle in-progress tasks and can result in stopping source tasks not completing a final offset commit before considering the task fully stopped.This can allow rebalancing to proceed before offsets are fully committed.",no,,[],,
KAFKA-3422,578,Add overloading functions for each operator not requiring serialization.This allows users to not provide a serde instance but indicate to use default ones from streams config.,no,,[],,
KAFKA-3444,579,"Figure out when to bump the version on release-candidate artifacts.Currently we remove the ""-SNAPSHOT"" marker immediately upon branching.Which means that our release artifacts are all released to maven repos with the same version.Which is apparently challenging for projects that depend on Kafka to test with the release artifacts.We need to revisit, discuss and maybe improve the process for the next release.",no,,[],,
KAFKA-3624,580,"NullPointerException Thrown for MetadataRequest when Cluster Not Available.Have been seeing consistently a NPE when metadata request is sent against a cluster that isn't online: 
  <SOURCECODE> 
  Looks like this is from this line: 
  <SOURCECODE> 
  The returned struct doesn't have the brokers key, since the returned data is incomplete, due to an invalid request.Would expect to see a better exception / handling rather than NPE.",no,,[],,
KAFKA-4304,581,"Extend Interactive Queries for return latest update timestamp per key.Currently, when querying state store, it is not clear when the key was updated last.The ides of this JIRA is to make the latest update timestamp for each key-value-pair of the state store accessible.For example, this might be useful to 
   
   check if a value was update but did not changed (just compare the update TS) 
   if you want to consider only recently updated keys 
   
  Covered via&nbsp;KIP-258:&nbsp;
   <URL>",no,,[ss1_one_per],,
KAFKA-4356,582,o.a.k.common.utils.SystemTime.sleep() swallows InterruptedException.<tt>org.apache.kafka.common.utils.SystemTime.sleep() catches and ignores <tt>InterruptedException.When doing so normally the interruption state should still be restored with <tt>Thread.currentThread().interrupt().,no,,[],,
KAFKA-4559,583,"Add a site search bar on the Web site.As titled, as we are breaking the ""documentation"" html into sub spaces and sub pages, people cannot simply use `control + f` on that page, and a site-scope search bar would help in this case.",no,,[],,
KAFKA-5232,584,"Kafka broker fails to start if a topic containing dot in its name is marked for delete but hasn't been deleted during previous uptime.We are using 0.10.2.0 (but this is reproducible even with 0.10.2.1 and latest upstream) in our environments.Our topic names contain (one or more) dot characters in their name.So we have topics like <tt>foo.bar-testtopic.Topic deletion is enabled on the broker(s) and our application does delete the topics as and when necessary.We just ran into a case today where for some reason the Kafka broker had either to be taken down (or went down on its own).Some of the topics which were deleted (i.e. a deletion marker folder was created for them) were left around after Kafka broker had gone down.So the Kafka logs dir had directories like <tt>foo.bar-testtopic-0.bb7981c216b845648edfe6e2b0a5c050-delete.When we restarted the Kafka broker, it refused to start and kept shutting down and running into this exception: 
  <SOURCECODE> 
  The only way we could get past this is pointing Kafka broker to a different Kafka logs directory which effectively meant a lot our topics were no longer accessible to the application.",no,,[],,
KAFKA-5530,585,"Balancer is dancing with KStream all the time, and due to that Kafka cannot work :-).Dears, 
  There are problems with balancer in KStreams (v. 0.10.2.x), when <em>num.stream.threads</em> is bigger than 1 and the number of the input topics are bigger than 1.I am doing more less such a setup in the code: 
  <SOURCECODE> 
  And if there are <em>num.stream.threads=4</em> but there are 2 or more but less than num.stream.threads inTopicNames, then complete application startup is totally self-blocked, by writing endless starnge things in log and not starting.Even more problematic is when the nuber of topics is higher than <em>num.stream.threads</em> what I had commented in 
    <URL> streams task gets stuck after re-balance due to LockException.I am attaching logs for two scenarios: 
   
   when: 1 &lt; num.stream.threads &lt; numer of topics (
    <URL>) 
   when: 1 &lt; numer of topics &lt; num.stream.threads (this ticket).I can fully reproduce the behaviour.Even I found workaround for it, but is not desired.When <em>num.stream.threads=1</em> than all works fine <img class=""emoticon"" src=""https://issues.apache.org/jira/images/icons/emoticons/sad.png"" height=""16"" width=""16"" align=""absmiddle"" alt="""" border=""0""> (for K v. 0.10.2.x, v. 0.11.0.0 does not work at all).<SOURCECODE>",no,,[ss11_rework],,
KAFKA-5985,586,"Mention the need to close store iterators.Store iterators should be closed in all/most of the cases, but currently it is not consistently reflected in the documentation and javadocs.For instance 
   <URL> does not mention the need to close an iterator and provide an example that does not do that.Some of the fetch methods do mention the need to close an iterator returned (e.g. 
   <URL>)), but others do not: 
   <URL>) 
  It makes sense to:  
  <ul class=""alternate"" type=""square""> 
   update javadoc for all store methods that do return iterators to reflect that the iterator returned needs to be closed 
   mention it in the documentation and to update related examples.",no,,[tk6_iteration],,
KAFKA-6398,587,"Non-aggregation KTable generation operator does not construct value getter correctly.For any operator that generates a KTable, its <tt>valueGetterSupplier has three code path: 
  1.If the operator is a KTable source operator, using its materialized state store for value getter (note that currently we always materialize on KTable source).2If the operator is an aggregation operator, then its generated KTable should always be materialized so we just use its materialized state store.3Otherwise, we treat the value getter in a per-operator basis.For 3) above, what we SHOULD do is that, if the generated KTable is materialized, the value getter would just rely on its materialized state store to get the value; otherwise we just rely on the operator itself to define which parent's value getter to inherit and what computational logic to apply on-the-fly to get the value.For example, for <tt>KTable#filter() where the <tt>Materialized is not specified, in <tt>KTableFilterValueGetter we just get from parent's value getter and then apply the filter on the fly; and in addition we should let the future operators to be able to access its parent's materialized state store via <tt>connectProcessorAndStateStore.However, current code does not do this correctly: it 1) does not check if the result KTable is materialized or not, but always try to use its parent's value getter, and 2) it does not try to connect its parent's materialized store to the future operator.As a result, these operators such as <tt>KTable#filter, <tt>KTable#mapValues, and <tt>KTable#join(KTable) would result in TopologyException when building.The following is an example: 
  ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ 
  Using a non-materialized KTable in a stream-table join fails: 
  <SOURCECODE> 
  fails with 
  <SOURCECODE> 
  Adding a store name is not sufficient as workaround but fails differently: 
  <SOURCECODE> 
  error: 
  <SOURCECODE> 
  One can workaround by piping the result through a topic: 
  <SOURCECODE> 
  ------------------------------------------------------------------------------------------------------------ 
  Note that there is another minor orthogonal issue of <tt>KTable#filter itself that it does not include its parent's queryable store name when itself is not materialized (see <tt>KTable#mapValues for reference).",no,,[],,
KAFKA-6425,588,"Calculating cleanBytes in LogToClean might not be correct.In class `LogToClean`, the calculation for `cleanBytes` is as below: 
  <SOURCECODE> 
  Most of the time, the `firstDirtyOffset` is the base offset of active segment which works pretty well with log.logSegments, so we can calculate the cleanBytes by safely summing up the sizes of all log segments whose base offset is less than `firstDirtyOffset`.However, things changed after `firstUnstableOffset` was introduced.Users could indirectly change this offset to a non-base offset(changing log start offset for instance).In this case, it's not correct to sum up the total size for a log segment.Instead, we should only sum up the bytes between the base offset and `firstUnstableOffset`.Let me show an example: Say I have three log segments, shown as below: 0L --&gt; log segment1, size: 1000Bytes 1234L --&gt; log segment2, size: 1000Bytes 4567L --&gt; active log segment, current size: 500Bytes 
  Based on the current code, if `firstUnstableOffset` is deliberately set to 2000L(this could be possible, since it's lower bounded by the log start offset and user could explicitly change LSO), then `cleanBytes` is calculated as 2000Bytes which is wrong.The expected value should be 1000 + (bytes between offset 1234L and 2000L)  
  
   <URL> 
   <URL> Do all of these make sense?",no,,[sk3_byte],,
KAFKA-6827,589,"Messages stuck after broker's multiple restart in a row.Hello <img class=""emoticon"" src=""https://issues.apache.org/jira/images/icons/emoticons/smile.png"" height=""16"" width=""16"" align=""absmiddle"" alt="""" border=""0""> 
  Tried with v0.10.2 and 1.1.0.I start with brand new brokers with no old data.Created topic test 
  <SOURCECODE> 
  &nbsp; Logs from brokers available in attachment 
  I start producing with a verifiable producer 
  <SOURCECODE> 
  During the production, i stop, start, stop start broker 1.<SOURCECODE> 
  There is no data loss producer side: 
  <SOURCECODE> 
  I consume messages with my simple shell consumer: 
  <SOURCECODE> 
  I grep values ""1.""in the /tmp/kafka_data_back.txt 
  <SOURCECODE> 
  Got only 999 937 messages instead of 1 000 000 
   
   I can restart the consumer any time, i will still got 999937.Depending on the run, i will get more or less messages stuck.I can restart kafka1, wait and restart kafka2, messages are still stuck.I can produce more messages, this will not unlock the messages untill =~ 700 messages produced.Disabling compression did not solve the problem.Ack 1 or -1 got the same result.Each run reproduce the problem.Starting from a brand new broker or not.Can you help me understand why messages are stuck ?",no,,[],,
KAFKA-6969,590,"AdminClient should provide a way to get the version of a node.Currently adminClient returns a lot of info about the cluster, topics... 
  It would be nice if it could return either the result of ApiVersions or a simplified ""kafkaVersion"" number for a node.This would be useful for admin tools to save them from having to do the request with their own NetworkClient.",no,,[],,
KAFKA-7565,591,"NPE in KafkaConsumer.The stacktrace is 
  <SOURCECODE> 
  Couldn't find minimal reproducer, but it happens quite often in our system.We use <tt>pause() and <tt>wakeup() methods quite extensively, maybe it is somehow related.",no,,[],,
KAFKA-7571,592,Add system tests for downgrading Kafka.We have system tests which verify client behavior when upgrading Kafka.We should add similar tests for supported downgrades.,no,,[],,
KAFKA-7699,593,"Improve wall-clock time punctuations.Currently, wall-clock time punctuation allow to schedule periodic call backs based on wall-clock time progress.The punctuation time starts, when the punctuation is scheduled, thus, it's non-deterministic what is desired for many use cases (I want a call-back in 5 minutes from ""now"").It would be a nice improvement, to allow users to ""anchor"" wall-clock punctation, too, similar to a cron job: Thus, a punctuation would be triggered at ""fixed"" times like the beginning of the next hour, independent when the punctuation was registered.",no,,[pf2_duration],,
KAFKA-7701,594,"Revert kafka-trunk-jdk8 CI change to re-enable testAll-based builds.As of Apache Jenkins build #3235, the kafka-trunk-jdk8 job was configured to&nbsp;build and test&nbsp;independently against Scala 2.12, followed by building and testing against Scala 2.11.Previously, the configuration leveraged the testAll Gradle task.Build #3235 completed successfully.The previous series of kafka-trunk-jdk8 failures correspond to the introduction of Gradle 5.0 in 
   <URL>&nbsp;The consistent failure symptoms have been instances of java.lang.NoClassDefFoundError in Kafka Streams tests.After addressing the issues assumed to be caused by the Gradle change, the CI configuration should be reverted to its previous state.",no,,[ss11_rework],,
KAFKA-7933,595,KTableKTableLeftJoinTest takes an hour to finish.PRs might time out as <tt>KTableKTableLeftJoinTest.shouldNotThrowIllegalStateExceptionWhenMultiCacheEvictions took one hour to complete.<SOURCECODE>,no,,[pf2_duration],,
KAFKA-995,596,Enforce that the value for replica.fetch.max.bytes is always >= the value for message.max.bytes.replica.fetch.max.bytes must always be &gt;= message.max.bytes for replication to work correctly.This ticket adds enforcement of the constraint.,no,,[],,
LUCENE-1076,597,"Allow MergePolicy to select non-contiguous merges.I started work on this but with 
   <URL> I won't make much progress on it for a while, so I want to checkpoint my current state/patch.For backwards compatibility we must leave the default MergePolicy as selecting contiguous merges.This is necessary because some applications rely on ""temporal monotonicity"" of doc IDs, which means even though merges can re-number documents, the renumbering will always reflect the order in which the documents were added to the index.Still, for those apps that do not rely on this, we should offer a MergePolicy that is free to select the best merges regardless of whether they are continuguous.This requires fixing IndexWriter to accept such a merge, and, fixing LogMergePolicy to optionally allow it the freedom to do so.",no,,[ss11_rework],,
LUCENE-1079,598,"DocValues cleanup: constructor & getInnerArray().DocValues constructor taking a numDocs parameter is not very clean.Get rid of this.Also, it's optional getInnerArray() method is not very clean.This is necessary for better testing, but currently tests will fail if it is not implemented.Modify it to throw UnSupportedOp exception (rather than returning an empty array).Modify tests to not fail but just warn if the tested iml does not override it.These changes should make it easier to implement DocValues for other ValueSource's, e.g. above payloads, with or without caching.",no,,[sk1_negative_necessary],,
LUCENE-1874,599,"Further updates to the site scoring page.update the site scoring page - see Appendix: 
  <blockquote> 
   Class Diagrams Karl Wettin's UML on the Wiki
  </blockquote> 
  Karl's diagrams are outdated - I think this link should be pulled for 2.9 
  <blockquote> 
   Sequence Diagrams FILL IN HERE.Volunteers?</blockquote> 
  I think this should be pulled - I say put something like this as a task in JIRA - not the published site docs.",no,,[],,
LUCENE-1899,600,"Inefficient growth of OpenBitSet.Hi, I found a potentially serious efficiency problem with OpenBitSet.One typical (I think) way to build a bit set is to set() the bits one by one - e.g., have a HitCollector set() the bit for each matching document.The underlying array of longs needs to grow as more as more bits are set, of course.But looking at the code, it appears to me that the array grows very ineefficiently - in the worst case (when doc ids are sorted, as they would normally be in the HitCollector case for example), copying the array again and again for every added bit... The relevant code in OpenBitSet.java is: 
   public void set(long index)  { int wordNum = expandingWordNum(index); ... } 
   protected int expandingWordNum(long index) { int wordNum = (int)(index &gt;&gt; 6); if (wordNum&gt;=wlen)  { ensureCapacity(index+1); ... } 
   public void ensureCapacityWords(int numWords) { if (bits.length &lt; numWords)  { long[] newBits = new long[numWords]; System.arraycopy(bits,0,newBits,0,wlen); bits = newBits; } 
   } 
  As you can see, if the bits array is not long enough, a new one is allocated at exactly the right size - and in the worst case it can grow just one word every time... 
  Shouldn't the growth be more exponential in nature, e.g., grow to the maximum of index+1 and twice the existing size?Alternatively, if the growth is so inefficient, this should be documented, and it should be recommended to use the variant of the constructor with the correct initial size (e.g., in the HitCollector case, the number of documents in the index).and the fastSet() method instead of set().Thanks, Nadav.",no,,"[ss15_each_one, gk1_efficiency, sk12_over_again, ss7_nn_by_nn]",,
LUCENE-1980,601,Fix javadocs after deprecation removal.There are a lot of @links in Javadocs to methods/classes that no longer exist.javadoc target prints tons of warnings.We should fix that.,no,,[],,
LUCENE-2014,602,"position increment bug: smartcn.If i use LUCENE_VERSION &gt;= 2.9 with smart chinese analyzer, it will crash indexwriter with any reasonable amount of chinese text.its especially annoying because it happens in 2.9.1 RC as well.this is because the position increments for tokens after stopwords are bogus: 
  Here's an example (from test case), where the position increment should be 2, but is instead 91975314!<SOURCECODE> 
  junit.framework.AssertionFailedError: posIncrement 1 expected:&lt;2&gt; but was:&lt;91975314&gt; at junit.framework.Assert.fail(Assert.java:47) at junit.framework.Assert.failNotEquals(Assert.java:280) at junit.framework.Assert.assertEquals(Assert.java:64) at junit.framework.Assert.assertEquals(Assert.java:198) at org.apache.lucene.analysis.BaseTokenStreamTestCase.assertTokenStreamContents(BaseTokenStreamTestCase.java:83) ...",no,,[],,
LUCENE-2474,603,"Allow to plug in a Cache Eviction Listener to IndexReader to eagerly clean custom caches that use the IndexReader (getFieldCacheKey).Allow to plug in a Cache Eviction Listener to IndexReader to eagerly clean custom caches that use the IndexReader (getFieldCacheKey).A spin of: 
   <URL>.Basically, its make a lot of sense to cache things based on IndexReader#getFieldCacheKey, even Lucene itself uses it, for example, with the CachingWrapperFilter.FieldCache enjoys being called explicitly to purge its cache when possible (which is tricky to know from the ""outside"", especially when using NRT - reader attack of the clones).The provided patch allows to plug a CacheEvictionListener which will be called when the cache should be purged for an IndexReader.",no,,[ss10_called_frequently],,
LUCENE-3194,604,"remove pdf files from website.While working on 
   <URL>, I started thinking we should remove the PDF files from the docs/website.just another thing to simplify life.",no,,[],,
LUCENE-3597,605,"grouping documentation is incorrect in 3.x.The grouping example in package.html doesn't look like it works for 3.x, because it has stuff like TopGroups&lt;BytesRef&gt; groupsResult (but in 3.x the method returns TopGroups&lt;String&gt;).So i think it needs to be updated for 3.x",no,,[],,
LUCENE-3603,606,"jar-src fails if ${build.dir} does not exist.Simple fix ???????????? make jar-src depend on a target which creates the build.dir.Also, I noticed that build.dir is set in multiple places across our build.xmls, so I'd like to improve that a bit (minor fixes as well).",no,,[],,
LUCENE-4067,607,I'm having 2 strings string 1) "abc" string 2) "xya" .Both are indexed to a string called "name" Then i'm trying to search for a* .It's giving both the strings.but i need the sting abc since it starts with a.This is what i'm expecting.I'm having 2 strings string 1) "abc" string 2) "xya" .Both are indexed to a string called "name" Then i'm trying to search for a* .It's giving both the strings.but i need the sting abc since it starts with a.This is what i'm expecting since i gave the search sting as a*,no,,[],,
LUCENE-4377,608,"consolidate various copyBytes() methods.Spinoff of 
   <URL>: 
  <blockquote> 
   I don't think the default impl (SlicedIndexInput) should overrided BII's copyBytes?Seems ... spooky.</blockquote> 
  There are copyBytes everywhere, mostly not really being used.Particularly DataOutput.copyBytes(DataInput) versus IndexInput.copyBytes(IndexOutput).Bulk merging already uses DataOutput.copyBytes(DataInput), its the most general (as it works on DataInput/Output), and its in dst, src order.I think we should remove IndexInput.copyBytes, its not necessary.",no,,[sk1_negative_necessary],,
LUCENE-515,609,"Using ConstantScoreQuery on a RemoteSearchable throws java.io.NotSerializableException.Using a ConstantScoreQuery through a MultiSearcher on a Searchable obtained through RMI (RemoteSearchable) will throw a java.io.NotSerializableException 
  The problem seems to be the fact that the ConstantScoreQuery.ConstantWeight has a Searcher member variable which is not serializable.Keeping a reference to the Searcher does not seem to be required: the fix seems trivial.I've created the TestCase to reproduce the issue and the patch to fix it.",no,,[],,
LUCENE-5327,610,Expose getNumericDocValues and getBinaryDocValues at toplevel reader and searcher levels.Expose NumericDocValues and BinaryDocValues in both IndexReader and IndexSearcher apis.,no,,[],,
LUCENE-580,611,"Pre-analyzed fields.Adds the possibility to set a TokenStream at Field constrution time, available as tokenStreamValue in addition to stringValue, readerValue and binaryValue.There might be some problems with mixing stored fields with the same name as a field with tokenStreamValue.",no,,[],,
LUCENE-6044,612,"Add backcompat for TokenFilters with posInc=false before 4.4.In Lucene 4.4, a number of token filters supporting the enablePositionIncrements=false setting were changed to default to true.However, with Lucene 5.0, the setting was removed altogether.We should have backcompat for this setting, as well as work when used with a TokenFilterFactory and match version &lt; 4.4.",no,,[],,
LUCENE-6046,613,"RegExp.toAutomaton high memory use.When creating an automaton from an org.apache.lucene.util.automaton.RegExp, it's possible for the automaton to use so much memory it exceeds the maximum array size for java.The following caused an OutOfMemoryError with a 32gb heap: 
  <SOURCECODE> 
  When increased to a 60gb heap, the following exception is thrown: 
  <SOURCECODE>",yes,use so much memory -- out of memory error,[ss4_memory],,
LUCENE-6678,614,"jdk1.9.0-ea-b72 JVM failures.A placeholder for JDK1.9.0-ea-b72 failures.Linked issues: 
   <ERROR></ERROR> 
   <URL>",no,,[],,
LUCENE-6795,615,"Remove all accessClassInPackage permissions / remove uses AccessibleObject#setAccessible() to make ready for Java 9 Jigsaw.With jigsaw builds this stuff is not allowed, its no longer an option of security manager or not.So we should remove these permissions and fix the test leaks, crappy code, remove hacks, etc.If the hack is really somehow needed for some special reason (e.g. well known case like mmap), it needs to gracefully handle not being able to do this, the code and tests should still ""work"" if it cannot do the hack.Otherwise there will be problems for java 9.",no,,[sk1_negative_necessary],,
LUCENE-7230,616,"IntPoint, LongPoint etc. need a ctor with FieldType.Previously IntField, LongField etc. had constructors that also take a FieldType.Such a constructor would also be required for IntPoint, LongPoint etc. so that a 'stored' Point can easily be created.",no,,[],,
LUCENE-7565,617,"UnifiedHighlighter: add ability to delineate passes by max char size.The Highlighter and FastVectorHighlighter can be configured to delineate passages using a target character length, that is then typically adjusted for the word boundary.This would be a good option to add to the UnifiedHighlighter (UH) in it's own right, as well as for better back-wards compatibility to those using this highlighter.",no,,[],,
LUCENE-8780,618,"Improve ByteBufferGuard in Java 11.In 
   <URL> we added <tt>ByteBufferGuard to protect MMapDirectory from crushing the JVM with SIGSEGV when you close and unmap the mmapped buffers of an IndexInput, while another thread is accessing it.The idea was to do a volatile write access to flush the caches (to trigger a full fence) and set a non-volatile boolean to true.All accesses would check the boolean and stop the caller from accessing the underlying ByteBuffer.This worked most of the time, until the JVM optimized away the plain read access to the boolean (you can easily see this after some runtime of our by-default ignored testcase).With master on Java 11, we can improve the whole thing.Using VarHandles you can use any access type when reading or writing the boolean.After reading Doug Lea's expanation &lt;
   <URL>&gt; and some testing, I was no longer able to crush my JDK (even after running for minutes unmapping bytebuffers).The apraoch is the same, we do a full-fenced write (standard volatile write) when we unmap, then we yield the thread (to finish in-flight reads in other threads) and then unmap all byte buffers.On the test side (read access), instead of using a plain read, we use the new ""opaque read"".Opaque reads are the same as plain reads, there are only different order requirements.Actually the main difference is explained by Doug like this: ""For example in constructions in which the only modification of some variable x is for one thread to write in Opaque (or stronger) mode, X.setOpaque(this, 1), any other thread spinning in while(X.getOpaque(this)!=1){} will eventually terminate.Note that this guarantee does NOT hold in Plain mode, in which spin loops may (and usually do) infinitely loop ???????????? they are not required to notice that a write ever occurred in another thread if it was not seen on first encounter.""- And that's waht we want to have: We don't want to do volatile reads, but we want to prevent the compiler from optimizing away our read to the boolean.So we want it to ""eventually"" see the change.By the much stronger volatile write, the cache effects should be visible even faster (like in our Java 8 approach, just now we improved our read side).The new code is much slimmer (theoretically we could also use a AtomicBoolean for that and use the new method <tt>getOpaque(), but I wanted to prevent extra method calls, so I used a VarHandle directly).It's setup like this: 
  <ul class=""alternate"" type=""square""> 
   The underlying boolean field is a private member (with unused SuppressWarnings, as its unused by the java compiler), marked as volatile (that's the recommendation, but in reality it does not matter at all).We create a VarHandle to access this boolean, we never do this directly (this is why the volatile marking does not affect us).We use VarHandle.setVolatile() to change our ""invalidated"" boolean to ""true"", so enforcing a full fence 
   On the read side we use VarHandle.getOpaque() instead of VarHandle.get() (like in our old code for Java 8).I had to tune our test a bit, as the VarHandles make it take longer until it actually crushes (as optimizations jump in later).I also used a random for the reads to prevent the optimizer from removing all the bytebuffer reads.When we commit this, we can disable the test again (it takes approx 50 secs on my machine).I'd still like to see the differences between the plain read and the opaque read in production, so maybe 
   <URL> or 
   <URL> can do a comparison with nightly benchmarker?Have fun, maybe 
   <URL> has some ideas, too.",yes,buffering to solve performance issue,"[sk7_buffer, ss13_cache, ss19_lot_reading, gk7_speed, pf2_duration, sk3_byte, tk6_iteration, ss10_called_frequently, pf4_benchmark]",,
MESOS-1185,619,"Reorganize cgroup layout.The proposed layout has the following advantages: 1.Easier breakdown of accounting (and isolation) of resources 2.Support multiple slaves (
   <URL>) 3.Support nesting slaves (
   <URL>) 
  Using the default values of the relevant flags and the cpu controller MESOS_CGROUPS_HIERARCHY = /sys/fs/cgroup MESOS_CGROUPS_ROOT = mesos 
  each slave would have a separate cgroup: 
  <SOURCECODE> 
  Under the slave's cgroup there would be a 'slave' cgroup which would contain the slave process (and child processes like mesos-fetcher) 
  <SOURCECODE> 
  And a 'containers' cgroup which would contain all containers as child cgroups 
  <SOURCECODE> 
  Multiple slaves would be differentiated by their SlaveIDs.Nested slaves would have an extended MESOS_CGROUP_ROOT which contained the parent's SlaveID.Such a layout would enable answering resource usage questions like: 1.Total usage of all slaves 2.Total usage of a single slave 3.Usage of a slave process 4.Total usage of all containers of a slave 5.Usage of a particular container (what we have now) 
  And also setting limits to any of the above.",no,,[],,
MESOS-1212,620,"Use maven to compile and package Mesos' Java files..We currently package our own jars which means we can't easily take advantage of maven plugins such as the shade plugin which we'd like to do for 
   <URL>.We might as well leverage maven for building Java.It would be great to check for maven during configure then too.",no,,[],,
MESOS-1246,621,"Convert stout hashmap and hashset to use std:: or std::tr1 instead of boost.Currently, parts of Mesos are failing to build with older compilers (g++ 4.1.2) due to duplicate definitions of 'ref' from boost and std::tr1.We're going to hit these issues more regularly so wherever we can replace boost with std:: or std::tr1 we should.This also sets us up nicely for the move to C++11.",no,,[],,
MESOS-1675,622,"Decouple version of the mesos library from the package release version.This discussion should be rolled into the larger discussion around how to version Mesos (APIs, packages, libraries etc).Some notes from libtool docs.<URL> 
  
   <URL>",no,,[],,
MESOS-2131,623,"Add a reverse proxy endpoint to mesos.A new libprocess Process inside mesos which allows attaching/detaching known endpoints at a specific path.Ideally I want to be able to do things like attach 'slave-id' and pass HTTP requests on to that slave: 
  Sample endpoint actions: 
  C++ api: attach(std::string name, Node target): Add a new reverse proxy path detach(std::string name): Remove an established reverse proxy path 
  HTTP endpoints: /proxy/go/ {name} 
  <ul class=""alternate"" type=""square""> 
   Prefix matches a path, forwards the remaining path onto the remote endpoin /proxy/debug.json 
   Prints out all attached endpoints.",no,,[],,
MESOS-3401,624,"Add labels to Resources.Similar to how we have added labels to tasks/executors (
   <URL>), and even FrameworkInfo (
   <URL>), we should extend Resource to allow arbitrary key/value pairs.This could be used to specify that a cpu resource has a certain speed, that a disk resource is SSD, or express any other metadata about a built-in or custom resource type.Only the scalar quantity will be used for determining fair share in the Mesos allocator.The rest will be passed onto frameworks as info they can use for scheduling decisions.This would require changes to how the slave specifies its `--resources` (probably as json), how the slave/master reports resources in its web/json API, and how resources are offered to frameworks.",no,,[],,
MESOS-3535,625,"Expose info about the container image associated each container through an HTTP endpoint..Currently no such information is exposed and it's difficult for the operator to know the state of the cluster in terms of which images are used vs. which can be cleaned up.We have tickets for both Appc and Docker store to automatically GC old images based on the cache size but before it's implemented, the operator needs this information to decide what to clean up.Plus, even after auto GC is implemented, images cannot be GCed when they are still in use, so this will still be very useful.The current thought is that this could be exposed as a <tt>monitor/status endpoint and we can require each isolator and the containerizer to expose a method akin to <tt>usage().Note that here this status is semantically different than resource statistics (which fluctuates) and the user configuration (which is specified by the user and can be ambiguous if the user doesn't care about preciseness or the specific config at all), but rather this is the runtime info/state/status of the container which is likely static for the lifecycle of the container.For reference, OCI defines a similar ""state"" endpoint: 
   <URL>",no,,[],,
MESOS-3998,626,"resource test failure.Encountered this test failure when building mesos on CentOS 7 via 
   <URL>.<SOURCECODE> 
  Comparison code 
  <SOURCECODE>",no,,[],,
MESOS-4011,627,"Allow build phase independent platform integration tests..Many of the tests in Mesos could be described as integration tests, since they have external dependencies on kernel features, installed tools, permissions, etc.I'd like to be able to generate a mesos-tests RPM along with my mesos RPM so that I can run the same tests in different deployment environments.",no,,[],,
MESOS-442,628,"Command executor should not sleep before shutdown.Currently we do a sleep after sending terminal status update, so that the status update manager can reach the slave before the executor exits.There has to be a better way.",no,,[],,
MESOS-4488,629,"Define a CgroupInfo protobuf to expose cgroup isolator configuration..Within `MesosContainerizer` we have an isolator associated with each linux cgroup subsystem.The isolators apply subsystem specific configuration on the containers before launching the containers.For e.g cgroup/net_cls isolator applies net_cls handles, cgroup/mem isolator applies memory quotas, cgroups/cpu-share isolator configures cpu shares.Currently, there is no message structure defined to capture the configuration information of the container, for each cgroup isolator that has been applied to the container.We therefore need to define a protobuf that can capture the cgroup configuration of each cgroup isolator that has been applied to the container.This protobuf will be filled in by the cgroup isolator and will be stored as part of `ContainerConfig` in the containerizer.",no,,[],,
MESOS-4945,630,"Garbage collect unused docker layers in the store..Right now, we don't have any garbage collection in place for docker layers.It's not straightforward to implement because we don't know what container is currently using the layer.We probably need a way to track the current usage of layers.",no,,[],,
MESOS-5055,631,"Slave/Agent Rename Phase I - Update strings in the log message and standard output.This is a sub ticket of 
   <URL>.In this ticket, we will rename all the slave to agent in the log messages and standard output.",no,,[ss2_all_to_one],,
MESOS-555,632,"Jenkins scheduler should reuse a Jenkins slave.Currently a Jenkins slave is killed as soon as the job launched on it is finished.It would be nice to have a timeout before killing the slave, so that other jobs could be scheduled on it.",no,,[sk8_timeout_expiration],,
MESOS-5760,633,MAC OS Build failed.<SOURCECODE>,no,,[],,
MESOS-579,634,"Refactor the Isolator to return Futures and wrap a libprocess Process..Currently the Isolator interface <em>is</em> a libprocess Process: 
  class Isolator : public process::Process&lt;Isolator&gt; { ... virtual void killExecutor( const FrameworkID&amp; frameworkId, const ExecutorID&amp; executorId) = 0; ... } 
  This has led to issues where we've accidentally called directly into Isolator (instead of dispatching).We should make Isolator a wrapper around a libprocess Process.Second, the Isolator should return Futures rather than calling back into the slave.We currently have to construct the ProcessIsolator / CgroupsIsolator by passing in the Slave PID, so that they can dispatch back into the slave to notify when the executor has exited (for example).We can eliminate the need for this by returning Futures!class Isolator { ... // Returns a Future that becomes ready when the executor has been killed.// Exited contains the status and message.virtual Future&lt;Exited&gt; killExecutor( const FrameworkID&amp; frameworkId, const ExecutorID&amp; executorId); { return dispatch(isolator, &amp;IsolatorProcess::killExecutor, frameworkId, executorId); } 
   ... 
  private: IsolatorProcess* isolator; }",no,,[],,
MESOS-5946,635,"Consider supporting compression for the event stream..Currently, we support GZIP compression for HTTP based responses in Libprocess.However, for events streamed on the persistent connection (RecordIO encoded) for schedulers/executors/subscribers, we don't yet have support for compression.We would need an implementation of GZIP/other compression technique that would work efficiently for streaming data and is not fixed length based.",yes,change a compression technique to improve performance,[gk1_efficiency],,
MESOS-6568,636,"JSON serialization should not omit empty arrays in HTTP APIs.When using the JSON content type with the HTTP APIs, a <tt>repeated protobuf field is omitted entirely from the JSON serialization of the message.For example, this is a response to the <tt>GetTasks call: 
  <SOURCECODE> 
  I think it would be better to include empty arrays for the other fields of the message (<tt>pending_tasks, <tt>completed_tasks, etc.).Advantages: 
  <ol> 
   Consistency with the old HTTP endpoints, e.g., /state 
   Semantically, an empty array is more accurate.The master's response should be interpreted as saying it doesn't know about any pending/completed tasks; that is more accurately conveyed by explicitly including an empty array, not by omitting the key entirely.</ol>",no,,[],,
MESOS-6738,637,"Mesos master help message gives unformatted documents..build mesos from the release tarball and running the following command: 
  <SOURCECODE> 
  it gives unformatted docs, but the slave/agent's help message is OK.",no,,[],,
MESOS-6785,638,"CHECK failure on duplicate task IDs.The master crashes with a CHECK failure in the following scenario: 
  <ol> 
   Framework launches task X on agent A1.The framework may or may not be partition-aware; let's assume it is not partition-aware.A1 becomes partitioned from the master.Framework launches task X on agent A2.Master fails over.Agents A1 and A2 both re-register with the master.Because the master has failed over, the task on A1 is <em>not</em> terminated (""non-strict registry semantics"").</ol> 
  This results in two running tasks with the same ID, which causes a master <tt>CHECK failure among other badness: 
  <SOURCECODE>",no,,[ss11_rework],,
MESOS-6904,639,"Perform batching of allocations to reduce allocator queue backlogging..Per 
   <URL>: 
  <blockquote> 
   Our deployment environments have a lot of churn, with many short-live frameworks that often revive offers.Running the allocator takes a long time (from seconds up to minutes).In this situation, event-triggered allocation causes the event queue in the allocator process to get very long, and the allocator effectively becomes unresponsive (eg.a revive offers message takes too long to come to the head of the queue).</blockquote> 
  To remedy the above scenario, it is proposed to perform batching of the enqueued allocation operations so that a single allocation operation can satisfy N enqueued allocations.This should reduce the potential for backlogging in the allocator.See the discussion 
   <URL> in 
   <URL>.",yes,Running the allocator takes a long time,"[ss1_one_per, ss12_spend_time, pf2_duration, gk3_reduce]",,
MESOS-7112,640,Ensure Mesos can be built and tests successfully on Debian8.This is a tracking bug collecting issues in building or running Mesos on Debian8.,no,,[],,
MESOS-713,641,"Support for adding subsystems to existing cgroup hierarchies..Currently if a slave is restarted with additional subsystems, it will refuse to proceed if those subsystems are not attached to the existing hierarchy.It's possible to add subsystems to existing hierarchies via re-mounting: 
   <URL> 
  We can add support for this by calling mount with the MS_REMOUNT option.",no,,"[pf3_profiling, ss11_rework]",,
MESOS-7299,642,Develop a generic solution for merging environments..Recent bugfixes have shown that we should come up with a generic solution for inheriting and merging environments.,no,,[],,
MESOS-7596,643,"Multiple registration attempts might result in agent shutdown..This sequence of events is possible: 
  <ol> 
   Agent sends register message M1 to master.Agent register timer expires, sends register message M2 to master.Master sees M1 and adds agent with ID A1.Agent gets SlaveRegisteredMessage with ID A1.The master &lt;-&gt; agent socket breaks; the master marks the agent as disconnected.Master sees M2; since the agent is currently disconnected, the master removes A1 and adds the agent with ID A2.Agent gets SlaveRegisteredMessage with ID A2.Since this is unexpected, the agent exits (""Registered but got wrong id"").</ol> 
  Shutting down the agent is unfortunate, although arguably not catastrophic.",no,,[sk8_timeout_expiration],,
MESOS-7619,644,"Framework Upgrade Resulting in Jan 1, 1070 Date.In the process of upgrading Apache Mesos and Marathon (in HA mode)..marathon ended up with a new framework ID and the older framework ID is listed as being from Jan 1, 1970 (47 years ago).The issue with Marathon getting a new framework Id is understood and was worked out with mesosphere's marathon team.Must of the detail is in the #marathon channel of Apache Mesos slack.",no,,[],,
MESOS-7741,645,"SlaveRecoveryTest/0.MultipleSlaves has double free corruption.Observed this on ASF CI 
  <SOURCECODE>",no,,[],,
MESOS-788,646,"Test AllocatorZooKeeperTest/0.FrameworkReregistersFirst goes into deadlock..Mesos Commit 13b8a8d2c852acb0968a55663af6e7e6a62d59d9 
  When running AllocatorZooKeeperTest/0.FrameworkReregistersFirst the test goes into deadlock and stalls keeping the cpu spinning.Logtrace: DEBUG: [ RUN ] AllocatorZooKeeperTest/0.FrameworkReregistersFirst DEBUG: 2013-10-31 05:20:00,568:6119(0x7fc285d97700):ZOO_DEBUG@do_completion@326: started completion thread DEBUG: **** DEADLOCK DETECTED!**** DEBUG: You are waiting on process authenticatee(13334)@127.0.0.1:49807 that it is currently executing.DEBUG: tests/allocator_zookeeper_tests.cpp:147: Failure DEBUG: Failed to wait 10secs for status DEBUG: **** DEADLOCK DETECTED!**** DEBUG: You are waiting on process authenticatee(22990)@127.0.0.1:49807 that it is currently executing.DEBUG: **** DEADLOCK DETECTED!**** DEBUG: You are waiting on process authenticatee(59121)@127.0.0.1:49807 that it is currently executing.",no,,[],,
MESOS-8020,647,"ssl-client crashes a lot during the libprocess tests (without being noticed).The libprocess test-suite, when running on my machine causes plenty of crash reports in <tt>~/Library/Logs/DiagnosticReports.The test-suite runs through without issues in those cases.One test that 100% reproducibly triggers this is ""SSLTest.ProtocolMismatch"".After a single run of that test which ends ""OK"", I got anything from 6 to 8 new crash reports in that folder.Example: 
  <SOURCECODE>",no,,[pf1_percentage],,
MESOS-8150,648,"Attributes documentation indicates that sets are valid attribute types; code disagrees.On the 
   <URL> page, it says: 
  <blockquote>
   The types of values that are supported by Attributes and Resources in Mesos are scalar, ranges, sets and text.</blockquote> 
  However, the code for 1.4.x disagrees.Sets are not supported for attribute types: 
  
   <URL> 
  
   <URL>",no,,[],,
MESOS-8324,649,"Add succeeded metric to container launch in Mesos agent.Only metric on agent related to stability of containerizer is ""slave/container_launch_errors"" and it does not track standalone/nested containers.I propose we add a container_launch_succeeded counter to track all container launches in containerizer, and also add make sure `error` counter tracks standalone and nested containers.",no,,[],,
MESOS-8615,650,"Webui should display agent unreachable agents..Using the webui, it's currently only possible to see the count of unreachable agents.The webui should include a table of unreachable agents.",no,,[],,
MESOS-9019,651,"Validate that container paths are unique in `ContainerInfo.volumes`..Currently we allow two volumes to have the same <tt>container_path.The bind-mount of a later volume would overwrite that of an earlier one.However, if the two volumes are file-based secrets, the containerizer will generate pre-exec commands similar to the following: 
  <SOURCECODE> 
  The second <tt>mv would rename <tt>secret2 to <tt>source, but <tt>target remains bounded to the gone <tt>secret1, and this would make the last <tt>mount result in an <tt>ENOENT.In general, allowing multiple with the same <tt>container_path is not useful, so we should disallow it and validate that in advance instead of getting a failure during container launch.",no,,[],,
MESOS-9195,652,"Publish native Mesos eggs to pypi..We currently publish only mesos.interface to pypi.Would it also be possible to publish the binary / platform dependent eggs to pypi?Namely <tt>mesos.scheduler and <tt>mesos.executor bundled via <tt>mesos.native?!are needed for our users to run the examples without having to build Mesos.For some extra karma, we could maybe publish them for both, linux and macOS.",no,,[],,
MESOS-9389,653,"Cannot build python support using clang 8.Trying to compile latest mesos master with python support enabled on a Fedora 28 machine leads to the following configuration error: 
  <SOURCECODE>",no,,[],,
MESOS-963,654,"Compile fails on 10.9.stout-tests(94041,0x7fff7e691310) malloc: *** error for object 0x18f00000062: pointer being freed was not allocated 
   
    
     
      
       
       set a breakpoint in malloc_error_break to debug make
        <ERROR></ERROR>: *** 
        <ERROR></ERROR> Abort trap: 6 make
        <ERROR></ERROR>: *** 
        <ERROR></ERROR> Error 2 make
        <ERROR></ERROR>: *** 
        <ERROR></ERROR> Error 1 make
        <ERROR></ERROR>: *** 
        <ERROR></ERROR> Error 2 make
        <ERROR></ERROR>: *** 
        <ERROR></ERROR> Error 1 make
        <ERROR></ERROR>: *** 
        <ERROR></ERROR> Error 1 make
        <ERROR></ERROR>: *** 
        <ERROR></ERROR> Error 2 make: *** 
        <ERROR></ERROR> Error 1",no,,[],,
OAK-1053,655,Deadlock between ChangeProcessor and Session logout.Happend while running <tt>org.apache.jackrabbit.oak.jcr.observation.ObservationJcrTest.I will attach the thread dump.,no,,[],,
OAK-1077,656,XPath failures for escaped quotes in full-text search.It seems that the query doesn't match when a property is <em>exact\"quote</em> or <em>exact\'quote</em>,no,,[],,
OAK-1168,657,"Invalid JCR paths not caught.<tt>NamePathMapper.getOakPath should return <tt>null when called with an invalid JCR path like <tt>foo:bar]baz, but it doesn't.",no,,[],,
OAK-1192,658,"NPE in Versioning when a Node has a hidden child node.For 
   <URL> I had to add a hidden child node on a versionable node and I see some test failures, which would point to the fact that the hidden node will be pulled into the version store 
   <ERROR></ERROR>.Browsing through the code is seems that the VersionEditor is in fact based on a VisibleEditor so it doesn't descend into hidden child nodes, but if a versionable node has such a child node, this will be included 
   <ERROR></ERROR> .I'm assuming a simple 'isHidden' check on the child name is what's missing here.<ERROR></ERROR> 
   <URL> 
  
   <ERROR></ERROR> 
  <SOURCECODE>",no,,[],,
OAK-1222,659,"Migration of Group Members stored in tree structure.as discussed in 
   <URL> we will change the way how 'many' group members are stored in the content and discontinue the former node structure which used to have residual weak reference properties.consequently we have to adjust the group member structure upon migrating from jackrabbit 2.x to oak.",no,,[],,
OAK-1248,660,Return early in Commit.markChanged() when parent is already marked.Minor improvement when the parent is already marked changed.There is no need to further move up to the root node when a node is detected that is already marked modified.,no,,[],,
OAK-1362,661,"Add package-info.java files instead of using the project version for the package exports.the current code lacks of package-info.java with BND information for the exported java packages.the default bundle plugin builds therefor bundles with package exports that use the project version, which is not best practice.we should add package-info.java with bnd information for all exported packages.",no,,[],,
OAK-1644,662,Has Binary flag should also be copied to split documents.Currently the binary flag <tt>_bin is only set on the main document and is not copied over to split document.This would cause issue in Blob GC as binary references present in old version in split document are not accounted for,no,,[],,
OAK-1674,663,"Node isNew() is false in case the node is removed and added in same commit.When you remove a Node /path/a transiently and add one add /path/a again.The transiently added Node isNew() check will be false.<SOURCECODE> 
  The API says 
  <blockquote> 
   Returns true if this is a new item, meaning that it exists only in transient storage on the Session and has not yet been saved.Within a transaction, isNew on an Item may return false (because the item has been saved) even if that Item is not in persistent storage (because the transaction has not yet been committed)....
  </blockquote>",no,,[],,
OAK-1710,664,"Extend authentication with intelligent loginid->userid mapping.use cases: 
   
   login with windows ""DOMAIN\userid"" 
   login with case insensitive userid 
   login with login id (e.g.) that is not equal to the user id 
   login with ldap DN 
   
  the logical steps to resolve the users are: 1.select the correct IDP for the given credentials 2.find the user in the IDP based on the credentials 3.authenticate the user 4.find the oak-user via user manager 5.setup subject based on the oak-user 6.allow login modules to add more principals 7.set AuthInfo to correctly identify the userid that corresponds to the user that was logged in 
  question: 
   
   different credentials for different use cases?how much must each login module implement itself?",no,,[],,
OAK-1745,665,"OrderedIndex should serve range queries regardless of direction.Make the ordered index serve all kind of range queries (<tt>&gt;, &gt;=, &lt;, &lt;= and <tt>between regardless of the direction",no,,[],,
OAK-1756,666,Remove export package directive from oak-solr-*.Since no one is currently using (or planned to use) the oak-solr exported packages it'd be good to avoid exporting them at all.,no,,[ss2_all_to_one],,
OAK-1825,667,"Randomized test to compare storage engines.Testing for the storage implementations (Segment, Mongo, RDB) should be improved.I propose a test that runs random operations against two (or more) storage implementations, and compares the result.The limits / thresholds / cache size should be low, so that the code coverage is better.",no,,[],,
OAK-1869,668,"TarMK: Incorrect tar entry verification in recovery mode.When recovering segments from a forcibly closed tar file (i.e. one without a proper tar index), the TarMK will scan all tar entries and verify their checksums.Unfortunately that checksum verification is incorrect, which leads to a lot of errors like the one below, and to the affected segments being discarded.<SOURCECODE> 
  In practice this leads to the TarMK undoing a lot of recent changes until it finds a segment that hasn't been discarded because of such an incorrect checksum verification.",no,,[],,
OAK-1936,669,TarMK compaction map check should switch comparison sides.This issue affects the SegmentNodeState#equals call as it makes the compaction map useless on account of not properly identifying compacted states.,no,,[],,
OAK-1988,670,"Confusing debug message about old revision access.There is a confusing debug message about accessing nodes at an old revision.This only makes sense for the root node, because other nodes may not have changed and reading them on an old revision is expected.",no,,[],,
OAK-2028,671,"Configurable date precision for ordered index.Currently the ordered index stores the full date as key of the index resulting very often in a key for each node as it will be highly improbable to have two nodes added at the same millisecond.Provide the ordered index with a configuration that will allow the end user to decide what precision for date fields to adopt.Something like 
  <SOURCECODE> 
  where the available options will be  
  <SOURCECODE> 
  with <tt>second as default having therefore the milliseconds always ignored.By specifying for example <tt>minute it will make the index to ignore seconds and milliseconds.The timezone aspect will always be considered.This will result in smaller and therefore faster indexes with a loss in sorting precision depending on the value specified.By having seconds for example it will mean that all the nodes added within the same second will result in a non-deterministic order.See full discussion at 
   <URL> and related issues in the ticket.",no,,"[gk7_speed, pf2_duration]",,
OAK-2124,672,"Trigger compaction after restart to workaround issue due to long running JCR session.Due to 
   <URL> we cannot perform efficient compaction on a running system as of now.To workaround that an alternative approach can be used 
  <ol> 
   Regular maintainence job (invoked via RevisionGC MBean) if determines that compaction would reclaim space above certain threshold then it sets a boolean flag under ""compactionStatus"" to true along with timestamp 
   Post restart the maintainence job on detecting such a flag can trigger compaction and cleanup 
  </ol> 
  Or in terms of pseudo code (by 
   <URL> ) 
  <SOURCECODE>",no,,"[gk1_efficiency, sk2_block_hang_crash]",,
OAK-2131,673,"Reduce usage of _lastRev.As described in 
   <URL> the usage of _lastRev must be reduced to better handle large transaction.The short term solution implemented for 
   <URL> relies on MapDB to temporarily store the pending modifications to a temp file.This solution prevents an OOME when there is a large commit, but can take quite a while to update the _lastRev fields after the branch is merged.This has an impact on the visibility of changes in a cluster because any further changes done after the merge only become visible when the large update of the _lastRev fields went through.Delayed propagation of changes can become a problem in a cluster when some components rely on a timely visibility of changes.One example is the Apache Sling Topology, which sends heartbeats through the cluster by updating properties in the repository.The topology implementation will consider a cluster node dead if those updates are not propagated in the cluster within a given timeout.Ultimately this may lead to a cluster with multiple leaders.",no,,"[ss4_memory, sk8_timeout_expiration, gk3_reduce]",,
OAK-2168,674,"Make SolrIndex implement AdvanceQueryIndex.Make SolrIndex implement AdvanceQueryIndex to allow it to make use of  
   
   Sorting 
   AggregateIndex which would only work with AdvanceQueryIndex post 
    <URL>",no,,[],,
OAK-2180,675,"Solr default rows number is too high.Currently the Solr query index retrieves 100k items at a time from the Solr native result set, this was set high to try to keep the no.of calls to Solr (going on the wire) at 1, however in most of the cases it seems too high, causing the Solr instance to retrieve many docs (and consume memory), the repository and Solr servers to exchange such significant amount of results over the wire, the Solr index to cache (for a short time) a huge no.of docs.Something like Lucene current default (50) should be better.",no,,[ss4_memory],,
OAK-2267,676,"Expose checkpoints through JMX.Revision clean works best when stale checkpoints have been removed.Currently the repository has to be taken offline for this.I suggest we expose Oak's checkpoint functionality via JMX, making it possible to list, create and delete checkpoints.",no,,[],,
OAK-2272,677,"Compatibility support for pre index rule configuration.With change in index configuration as part of 
   <URL> existing index configuration would not work.For compatibility purpose existing configuration should be adapted to IndexRule based configuration",no,,[],,
OAK-2463,678,Provide support for providing custom Tika config.Currently the Oak Lucene uses the default Tika Config while extracting text content from binary properties.To provide better control the tika config should be made configurable,no,,[],,
OAK-2529,679,"Index usage for ""not equals"" conditions.Queries with ""not equals"" conditions don't currently use an index.Example: 
  <SOURCECODE> 
  Such conditions should result in a ""not null"" property restrictions (so that property indexes can be used if available, and if there are only few entries for this property).Also, ""not equal"" need to be used and supported in the filter.Currently, in FilterImpl, this is not supported and used, and would throw a IllegalArgumentException if used.This is for indexes that natively support this feature (such as Apache Lucene and Solr).Also, this needs to be supported in the Lucene and Solr index.",no,,[],,
OAK-255,680,Implement Node#getReferences() both for REFERENCE and WEAKREFERENCE.,no,,[],,
OAK-2638,681,Use message from causing exception in DocumentStoreException.convert().The method with just the causing exception currently uses 'null' as the message for the DocumentStoreException.Instead it should use the message from the given exception.,no,,[],,
OAK-2681,682,"Update lease without holding lock.A lease update of the DocumentNodeStore on MongoDB will acquire a lock in MongoDocumentStore to perform the changes.The locking is only necessary for changes in the 'nodes' collection, because only those documents are cached and the locking makes sure the cache is consistent.The MongoDocumentStore must be changed to only acquire a lock when changes are done in the 'nodes' collection.",no,,"[ss13_cache, sk1_negative_necessary]",,
OAK-2709,683,"Misleading log message from IndexCopier.The lucene IndexCopier logs the following message on my Mac: 
  <SOURCECODE> 
  The message should be more general if this is expected to happen on a Mac as well.I'm also wondering if INFO is appropriate.I see this kind of message every 30 seconds in my log.Maybe we should lower it to DEBUG?",no,,[],,
OAK-2712,684,Possible null-dereference when calling ItemImpl#perform.FindBugs complains about usages of ItemImpl#perform that is annotated with <tt>@CheckForNull but callers expecting a non-null return value....most prominently in NodeImpl,no,,[],,
OAK-2738,685,"XPath: Possible StackOverflowException with many ""or"" conditions.For XPath queries with many ""or"" conditions of the form ""@x = 1 or @x = 2 or @x = 3"", the converter could throw a StackOverflowException (during the optimization phase).Such conditions are converted to ""x in (1, 2, 3)"", however this conversion is recursive and relatively slow.We need to make sure at least 10'000 conditions can be processed efficiently.",yes,conditions can be processed efficiently.,"[gk1_efficiency, gk7_speed, tk6_iteration]",,
OAK-2741,686,"oak shutdown, unstopped threads.I run oak in a non-osgi environment (tomcat) and am seeing these on webapp reload; 
  SEVERE: The web application 
   <ERROR></ERROR> appears to have started a thread named 
   <ERROR></ERROR> but has failed to stop it.This is very likely to create a memory leak.Apr 07, 2015 2:27:20 PM org.apache.catalina.loader.WebappClassLoader clearReferencesThreads 
   <ERROR></ERROR> 
  There's 32 of them, which roughly corresponds to the number of indexes I've configured (27 custom indexes + 5).The repository is constructed as 
  private Repository oakRepository; private SegmentStore segmentStore; 
  segmentStore = new FileStore(new File(oakRepositoryPath), 256); NodeStore nodeStore = new SegmentNodeStore(segmentStore); oakRepository = new Jcr(nodeStore) .with(new LocalInitialContent()) .withAsyncIndexing() .createRepository(); 
  and shut down using 
  ((RepositoryImpl)oakRepository).shutdown(); segmentStore.close(); 
  Chetan Mehrotra says: 
  This looks like the pool of 32 threads created in Oak.defaultScheduledExecutor which is not shutdown when content repository is closed (L587 in Oak class).In many cases the executor is provided from outside so close logic needs to take care if the executor is created by itself then it should be closed also.",no,,"[ss4_memory, ss11_rework]",,
OAK-2757,687,"Failed to read from tar file .Under some rare circumstances there is a warning in the logs: 
  <SOURCECODE> 
  This happens due to a race between <tt>FileStore#readSegment reading from tar files and already removed by <tt>FileStore#flush.This isn't a problem as the tar file in question is still present at a newer generation and the <tt>FileStore will eventually read from that one.However the warning looks rather scaring and somewhat implies a defect.We should either lower the log level or remove the race.",no,,[],,
OAK-2811,688,"Oak + data store: NPE in SegmentNodeStoreService.deactivate() leads to data store not shutting down properly.For some reason, on shutdown of a system, the <tt>observerTracker in the SegmentNodeStoreService became null before deactivate() was called, leading to this NPE on 
   <URL>: 
  <SOURCECODE> 
  This in turn lead to the datastore not being closed (
   <URL>).In our case we used the CachingDataStore, which relies on close() being called, if not it triggers 
   <URL>, which prevents the system from starting up again.In any case, the NPE should be guarded.",no,,[],,
OAK-2831,689,"Test classes extending AbstractImportTest do not always shut down repository instances properly.In <tt>AbstractImportTest a content repository instance is unconditionally created, see 
   <URL> .However, the repository is shutdown only if the import behaviour != null: 
   <URL> .This leads to executor instances not being closed and a large number of threads being leaked.I actually get consistent build failures due to this - see 
   <URL>",no,,[],,
OAK-2835,690,"TARMK Cold Standby inefficient cleanup.Following 
   <URL>, it turns out that patching the data corruption issue revealed an inefficiency of the cleanup method.similar to the online compaction situation, the standby has issues clearing some of the in-memory references to old revisions.",no,,[gk1_efficiency],,
OAK-2922,691,"enhance LastRevRecoveryAgent diagnostics.Depending on persistence type and repo size, the LastRevRecoveryAgent might run for a long time.Enhance the logging so that the admin knows what's going on (say, once per minute?).",no,,"[ss1_one_per, pf2_duration]",,
OAK-2978,692,ContentSession: Populate AuthInfo from Subject .currently the <tt>ContentSession implementation expects the <tt>AuthInfo to be already present on the <tt>Subject.<URL> suggested that this could be improved by creating the <tt>AuthInfo based on the information present in the <tt>Subject as fallback instead of returning an EMPTY info.That makes a lot of sense and would also simplify the creation of custom {{LoginModule}}s.,no,,[],,
OAK-3104,693,"Version garbage collector doesn't collect a rolled back document if it was never deleted.If a commit gets rolled back it can leave (in case the document was never deleted explicitly) a document in a state like: 
  <SOURCECODE> 
  If the path is fairly busy, the document can get created naturally later and then follow the usual cycle.But, at times, such documents are ephemeral in nature and never re-used.In those cases, such documents can remain silently without getting collected.",no,,[ss11_rework],,
OAK-3159,694,"Extend documentation for SegmentNodeStoreService in http://jackrabbit.apache.org/oak/docs/osgi_config.html#SegmentNodeStore.Currently the documentation at 
   <URL> only documents the properties 
  <ol> 
   repository.home and 
   tarmk.size All the other properties like customBlobStore, tarmk.mode, ....are not documented.Please extend that.Also it would be good, if the table could be extended with what type is supported for the individual properties.</ol>",no,,[],,
OAK-3229,695,"Log initialization of the cache and the persistent cache.Currently, the persistent cache initialization just logs ""start"".It should log the config used, to more easily analyze problems.The in-memory (in-heap) cache should also log the configuration used.",no,,[ss13_cache],,
OAK-3236,696,"integration test that simulates influence of clock drift.Spin-off of 
   <URL> 
   <URL> - ie there should be an integration test that show cases the issues with clock drift and why it is a good idea to have a lease-check (that refuses to let the document store be used any further once the lease times out locally)",no,,[],,
OAK-3251,697,"speeding up the build time.Running the build with a  {mvn clean install} 
   takes a considerable amount of time: 15 minutes on my local.The top 10 slowest unit test are the following 
  <SOURCECODE> 
  Is there anything we could do to speed-up these?sorted times obtained with 
   <URL>",yes,speedup building time,"[gk7_speed, pf2_duration]",,
OAK-3261,698,"consider existing locks when creating new ones.When creating new locks, existing locks need to be checked: 
  <ul class=""alternate"" type=""square""> 
   on ancestor nodes, when deep locks 
   on descendant nodes 
   
  (Note that the check on descendant nodes might be costly as long as we have to walk to whole subtree)",no,,[],,
OAK-3286,699,Persistent Cache improvements.Issue for collecting various improvements to the persistent cache.,no,,[ss13_cache],,
OAK-3312,700,"[Blob GC] Test case for GC / OAK-3167.<URL> fixes the bug ""Wrong time units for blobGcMaxAge are passed from SegmentNodeStoreService"".But there is no test case.Without a good test case, we risk running into the same (or worse) problems again, with each change of Oak in that area.And this is a really important problem area: nobody wants to lose data.",no,,[],,
OAK-3313,701,"Many tests leak DocumentNodeStore instances.Many tests use the <tt>DocumentMK.Builder to create <tt>DocumentNodeStore instances in tests.The cleanup is manual, either in an <tt>@After method or in the test method.The problems arise when the cleanup is forgotten or not done in a finally block.The problem appears when too many threads are started but not stopped and hit machine resource limitations.To solve this problem I propose using a JUnit <tt>@Rule which returns a custom <tt>DocumentMK.Builder instance which shuts down the <tt>DocumentNodeStore that it has created when the test method is finished.I was able to replace most of the leaks by using the <tt>DocumentMkBuilderProvider rule, as follows: 
  Before: 
  <SOURCECODE> 
  After: 
  <SOURCECODE> 
  I haven't touched tests which did not leak DocumentNodeStore instances.",no,,[sk2_block_hang_crash],,
OAK-3347,702,"Ineffective cleanup after compaction due to references to root.The cleanup phase after a compaction run is currently not able to remove the pre compacted segments as the previous (pre-compacted) root is still being referenced.Those references are coming from: 
   
   The <tt>before 
    <URL> in <tt>FileStore.flush.The 
    <URL> of the <tt>SegmentNodeStore.",no,,[],,
OAK-345,703,"Split NameMapper into session aware and session less parts.As described on the @oak-dev list 
   <ERROR></ERROR> it would make sense to use the same approach we used for the NamespaceRegistry and the NodeTypeManager also for the NameMapper.<ERROR></ERROR> 
   <URL>",no,,[],,
OAK-3473,704,"CliUtils#handleSigInt uses classes from sun.misc.*.<tt>CliUtils#handleSigInt() introduced with 
   <URL> uses <tt>sun.misc.Signal and <tt>sun.misc.SignalHandler to gracefully shut down after e.g. <tt>Ctrl+C. 
  Instead of using internal and sun specific APIs, this should be implemented using a 
   <URL>.",no,,[],,
OAK-3623,705,"backport DocumentStore time diff API back.This API was introduced as part of 
   <URL>.Not having it in older branches makes backporting unrelated changes harder; so just include it without using it (yet).",no,,[],,
OAK-3679,706,"Rollback to timestamp.We should have a feature to roll back to a certain point in time.The use case are:  
   
   undo a failed, large operation (for example upgrade, migration, installing a package), 
   on a copy of the repository, switch to an old state for reading old content 
   recover from a corruption (for example corruption due to incorrect ""discovery"" state, such as concurrent async index updates).",no,,[tk7_concurrent],,
OAK-3694,707,"As a user, I want to know when a node that I've created has been indexed..As a user, I want to know when a node that I've created has been indexed so that I can start using it.Generalization: As a user, I want to know when everything created before timestamp T0 has been indexed.Ideal solution: MBean operation <tt>bool isIndexedBefore(long timestamp) that returns <tt>true if everything created before <tt>timestamp has been indexed (by all indexers).Current options:  
  <ol> 
   check IndexStatsMBean for Start and Done to determine if a cycle has started after adding the node and has finished.Issue: there can be multiple async indexers.<URL> proposes a solution.add a node and search for it until it is retrieved.Issue: must add different nodes for different indexers and wait for all.</ol> 
  These options are not precise (give false positives) and are not resilient to changes in indexing strategy (e.g. adding one more indexer breaks the checks).",no,,[],,
OAK-3737,708,"Compactor should log revisions acting upon.For post mortem analysis it would be helpful to have the revisions that where involved in a compaction run.I.e. the revision that was compacted, the revisions of the cycles (if any) and the revision that is ultimately applied?",no,,[],,
OAK-3753,709,"Test failure: HeavyWriteIT.<tt>org.apache.jackrabbit.oak.plugins.segment.HeavyWriteIT failed on Jenkins: 
  <SOURCECODE> 
  Seen at build 597",no,,[],,
OAK-3785,710,"IndexDefinition should expose underlying node state.For 
   <URL>, we want to pass index definition's <tt>NodeState to registered callbacks.Thus, we want to expose the underlying <tt>NodeState for currently used <tt>IndexDefinition",no,,[],,
OAK-3794,711,"The Cold Standby should expect loops in the segment graph.The fix for 
   <URL> introduces the possibility of loops in the segment graph.The Cold Standby client currently traverses the segment graph without keeping track of the visited segments.This makes the Cold Standby prone to infinite loops during one transfer operation.",yes,infinite loop,"[sk11_infinity, tk6_iteration]",,
OAK-3849,712,"After partial migration versions are not restorable.After migrating a content subtree with referenced versions and starting the destination repository, the versions are not available.The reason is that the new version histories UUIDs hasn't been indexed.We should set the <tt>/oak:index/uuid/reindex to <tt>true every time we copy a version history, so the new identifiers will be indexed during the merge.",no,,[ss11_rework],,
OAK-387,713,"Clarify behavior/state of Root and Tree after calling ContentSession#close().quickly discussed this topic with jukka today in the office.as far as i know the API contract does currently not specify what happens to (the state of) a Root or Tree once the ContentSession has been closed.if i am not mistaken, the current implementation would just loose  the permissions that were granted to the original subject... but that's rather a coincidence (and i didn't test it to verify that's really the case) 
  possible solutions could be: 
  <ul class=""alternate"" type=""square""> 
   upon session closure the root/tree becomes invalid (invalidstate) and throws 
   the root/tree are still valid but doesn't have the original permissions any more -&gt; default permissions for empty-subject would apply 
   ... 
   
  whatever solution we may prefer in the end, i think that API contract should state the expected behavior (even if it was ""undefined"") and we should have tests verifying the current implementation does what we think it should do.",no,,[gk7_speed],,
OAK-3899,714,"Extend TokenLoginModule to respect shared key javax.security.auth.login.name.The <tt>TokenLoginModule and specifically TokenProviderImpl 
   <URL> when creating a token.However, in certain situations, such as with the ExternalLoginModule and non-username/password credentials, the SimpleCredentials are used but don't have a user id as the real user id is determined not by the caller of <tt>Repository.login(), but by the external identity provider inside the ExternalLoginModule (and the credentials might not include any kind of user id, say an opaque token from an external service).In this case, <tt>SimpleCredentials.getUserID() returns null and the token implementation fails to create a token and does not return it in the <tt>.token attribute of the credentials.Instead, the TokenLoginModule should look at the shared <tt>javax.security.auth.login.name attribute, which can de-facto override a <tt>SimpleCredentials.getUserID(), as it happens in the ExternalLoginModule.",no,,[],,
OAK-3903,715,"Commit fails even though change made it to the DocumentStore.In some rare cases it may happen that the DocumentNodeStore considers a commit as failed even though the changes were applied entirely to the DocumentStore.The issue happens when the update of the commit root is applied to the storage of a DocumentStore but then shortly after the communication between Oak the the storage system fails.On the Oak side the call will be considered as failed, but the change was actually applied.The issue can be reproduced with the test attached to 
   <URL> and a replica-set with 3 nodes.Killing the primary node and restarting it a after a while in a loop will eventually lead to a commit that conflicts itself.",no,,[tk6_iteration],,
OAK-3904,716,"Compaction Map predicate should use cached state for evaluation.In the case of offline compaction, the Compactor predicate would try to evaluate if a specific node is candidate for the map of not based on a set of conditions.To evaluate said conditions, the predicate currently uses the compacted state, the one that was just written by the SegmentWriter 
   <ERROR></ERROR>, but this offers very poor performance as this NodeState will be accessed from the TarWriter directly, a very IO intensive call (no memory mapping, no caching of the segment) 
   <ERROR></ERROR>.A much better thing is to use the cached nodestate, in my local test (on a SSD) this accounts for 10% of perf loss, I would imagine the gains are more significant on a non-SSD disk.<ERROR></ERROR> 
   <URL> 
   <ERROR></ERROR> 
   <URL>",yes,10% performance loss,"[gk4_performance, pf1_percentage]",,
OAK-3911,717,"Integer overflow causing incorrect file handling in OakDirectory for file size more than 2 GB.In couple of cases we have seen strange error related to invalid seek.In such cases it was seen that file sizes are greater than 2GB.A close inspection of OakDirectory 
   <ERROR></ERROR> shows that following calls in loadBlob and flushBlob are prone to integer overflow (Thanks 
   <URL>) 
   
   <tt>int n = (int) Math.min(blobSize, length - index * blobSize); 
   <tt>int n = (int) Math.min(blobSize, length - i * blobSize); 
   
  Above both <tt>blobSize and <tt>index and <tt>i are <tt>int.And multiplication of 2 int would be int that can cause overflow.<SOURCECODE> 
  
   <ERROR></ERROR> 
   <URL>",no,,[tk3_memory_unit],,
OAK-3949,718,"Deadlock with bulk acquire of TreeLocks.As reported in 
   <URL>, the new bulk lock acquire method in TreeNodeDocumentLocks may lead to a deadlock.The feature was introduce with 
   <URL>.",no,,[],,
OAK-4035,719,AsyncIndexUpdate should not log exception when its forcibly stopped.While shutdown when an index is forced to stop then it logs a exception message indicating that indexing has been stopped.This is done at warn level.As this is an expected scenario the exception should not be logged at warn level and just a info level message indicating that indexing is interrupted should be logged,no,,[],,
OAK-4037,720,"AccessControlValidator: include paths in AccessControlExceptions.I had a situation where a system was throwing exceptions about duplicate ACEs during installation of multiple nested content packages.It was rather difficult to identify the culprit of the issue, because the exception's message does not contain any paths.It would be nice to have paths in the exceptions thrown by <tt>AccessControlValidator to make analysis of such issues easier.",no,,[],,
OAK-4074,721,"LengthCachingDataStore should be enabled by default in oak-upgrade.<URL> introduces <tt>LengthCachingDataStore which may increase the performance of repeated upgrades (
   <URL>), especially if a slow blob store is used (eg.s3).Let's have it enabled by default.Also, let's add a new CLI parameter to change the default location of the <tt>mappingFilePath.",yes,increase the performance of repeated upgrades,"[gk4_performance, gk7_speed]",,
OAK-4285,722,"Align cleanup of segment id tables with the new cleanup strategy .We need to align cleanup of the segment id tables with the new ""brutal"" strategy introduced with 
   <URL>.That is, we need to remove those segment id's from the segment id tables whose segment have actually been gc'ed.",no,,[],,
OAK-4287,723,"Disable / remove SegmentBufferWriter#checkGCGen.<tt>SegmentBufferWriter#checkGCGen is an after the fact check for back references (see 
   <URL>), logging a warning if detects any.As this check loads the segment it checks the reference for, it is somewhat expensive.We should either come up with a cheaper way for this check or remove it (at least disable it by default).",yes,checking process is somewhat expensive,"[ss8_load_nn, gk6_expensive]",,
OAK-4347,724,"Use the indexPath from hidden property instead of taking this as input as part of index config.With 
   <URL> the index path is made accessible to the index editors as hidden property.Currently LuceneIndexEditor relies on user to provide the indexPath as index config.Instead of that LuceneIndexEditor should make use of the new hidden property to determine the indexPath.",no,,[],,
OAK-4406,725,"Add easy to use commands to maintain search indices (for Property, Lucene and Solr) .Currently only the console within <tt>oak-run can be used to remove search index files.But for that you have to know where the index definition is stored (because the actual index is stored below those nodes).and you have to use low-level commands to manually modify the repository.It would be really nice if there would be dedicated commands in oak-run which would allow to 
  <ol> 
   list all existing search indices 
   remove specific indices 
   verify them for validity (not sure if that is possible though) 
  </ol> 
  In addition those commands probably need to consider the copy on the local filesystem of indices (
   <URL>).",no,,[],,
OAK-441,726,"Release oak-core-tests.I'd like to be able to use the test classes from oak-core in the oak-lucene project.Similar to jackrabbit-core, I'd like to have oak-core tests released.",no,,[],,
OAK-4442,727,"S3DataStoreService should initialize SharedS3DataStore.Since, the S3SharedDataStore extends the S3DataStore it's better that the S3DataStoreService initializes it to gain the shared storage features.",no,,[],,
OAK-4450,728,"Properly split the FileStore into read-only and r/w variants .The <tt>ReadOnlyFileStore class currently simply overrides the <tt>FileStore class replacing all mutator methods with a trivial implementation.This approach however leaks into its ancestor as the read only store needs to pass a flag to the constructor of its super class so some fields can be instantiated properly for the read only case.We should clean this up to properly separate the read only and the r/w store.Most likely we should factor the commonalities into a common, abstract base class.",no,,[],,
OAK-4476,729,Option to check datastore consistency in oak-run.Add an option to check data store consistency in oak-run.Along with the consistency check it makes sense to have the option to dump all blob ids and/or all the blob references available.,no,,[],,
OAK-4524,730,"LucenePropertyIndexTest#longRepExcerpt sometimes failing.As reported by Julian on oak-dev@&nbsp;it seems <em>longRepExcerpt</em> is still failing sometimes when query takes more than 10s e.g. see this 
   <URL>.",no,,[],,
OAK-4608,731,"Unify background operation impls.I unified the 2 background operation implementations and made then extend <tt>Closeable, this reduces the code footprint a bit and makes it easier for me to add a new operation.<URL>, 
   <URL> change is in this commit, please review!<URL>",no,,[gk3_reduce],,
OAK-4644,732,"Support multiple readers in LuceneIndexMBean.With 
   <URL> its possible to have multiple readers and thus multiple directory for a singe index.LuceneIndexMBean should be adapted to support this kind of setup",no,,[],,
OAK-4659,733,"Address records by logic identifiers instead of offsets.Records are currently identified by their offsets inside the segments, This prevents records from being relocatable inside a segment and, as a consequence, this prevents segments to be shrunk down.This limitation forced us into a very limited solution space when working on garbage collection.Logic record IDs and their impact on the current implementation should be investigated.",no,,[],,
OAK-4691,734,"Use utility backends from oak-segment-tar in oak-run.Once oak-segment-tar is released, oak-run should be updated to use the utility backend code from oak-segment-tar instead of maintaining its own copy of utility code.",no,,[],,
OAK-47,735,"Wrong results and NPE with copy operation.The following code either results in an NPE or in a wrong result depending on which Microkernel instance is used.<SOURCECODE> 
  The wrong result is  
  <SOURCECODE> 
  The expected result is 
  <SOURCECODE> 
  simple:fs:target/temp: wrong result fs: {homeDir}/target: NPE
   http-bridge:fs:{homeDir} 
  /target: NPE simple: wrong result",no,,[],,
OAK-4736,736,Fix integration test in StandbyTestIT.,no,,[],,
OAK-4926,737,"o.a.j.o.s.s.c.StandbyClientSyncExecution#isLocal should expect missing segment.Currently the method <tt>org.apache.jackrabbit.oak.segment.standby.client.StandbyClientSyncExecution#isLocal does invoke 
  <SOURCECODE> 
  in order to read the referenced segment.In case of missing segment, the <tt>ReferenceId.getSegment does throw a SNFE and logs an ERROR level statement.The SNFE is needed but not the log statement in this case.",no,,[sk1_negative_necessary],,
OAK-4930,738,"External Principal Management:  DynamicSyncContext makes redundant calls to IdentityProvider.getIdentity().When recursively collecting principal names associated with the declared group memberships of a given principal, the method DynamicSyncContext.collectPrincipalNames() unnecessarily calls IdentityProvider.getIdentity() for every declared group reference, though it is only necessary when the actual depth of the remaining recursion is &gt;1.",no,,"[sk1_negative_necessary, tk6_iteration]",,
OAK-4966,739,"Re-introduce a blocker for compaction based on available heap.As seen in a local test, running compaction on a tight heap can lead to OOMEs.There used to be a best effort barrier against this situation 'not enough heap for compaction', but we removed it with the compaction maps.I think it makes sense to add it again based on the max size of some of the caches: segment cache <tt>256MB by default 
   <ERROR></ERROR> and some writer caches which can go up to <tt>2GB all combined 
   <ERROR></ERROR> and probably others I missed.<ERROR></ERROR> 
   <URL> 
   <ERROR></ERROR> 
   <URL>",no,,"[ss13_cache, ss2_all_to_one, tk3_memory_unit, ss11_rework]",,
OAK-5004,740,"Offline compaction explodes checkpoints .Running offline compaction on a repository with checkpoints will explode those into full copies.Observed e.g. with 
   <URL>.I think we should consider improving this by compacting checkpoints on top of each other in the proper order (<tt>oak-upgrade does this successfully).<ERROR></ERROR>, WDYT?What was our take on this in the previous Oak versions?",no,,[],,
OAK-5019,741,"Support glob patterns through OakEventFilter.(Originally reported as 
   <URL>, but moved to Oak as a result of introducing the OakEventFilter in 
   <URL>.From the original description: ) 
  In the Sling project, we would like to register JCR listeners based on glob patterns as defined in 
   <URL> 
  So basically instead (or in addition) to specifying an absolute path, defining patterns.<URL> /cc 
   <URL>",no,,[],,
OAK-5073,742,"Bug in JcrPathParser.I seem to have found a bug in the org.apache.jackrabbit.oak.namepath.JcrPathParser, when looking up following property through the session, it returns me false, while the property does exist.I have debugged the code and found following results.A file.scss exists at /etc/shared/mixins/file.scss 
  The api that I am using:  
   session.propertyExists(""/etc/shared/mixins/shared/../file.scss/jcr:content/jcr:data""); returns false  =&gt; PathListener elements 
   <ERROR></ERROR> 
   session.propertyExists(""/etc/shared/mixins/something/../file.scss/jcr:content/jcr:data""); returns true  =&gt; PathListener elements 
   <ERROR></ERROR> 
   session.propertyExists(""/etc/shared/mixins/file.scss/jcr:content/jcr:data""); returns true  =&gt; PathListener elements 
   <ERROR></ERROR> 
  So it seems that when using the same word shared to go back on a second time after other words in between, results in an error: /etc/shared/mixins/shared/../file.scss/jcr:content/jcr:data I tried this with other examples (/etc/anything/mixins/anything/../file.scss/jcr:content/jcr:data) and always came to the same result that the api is not working correctly.",no,,[pf2_duration],,
OAK-5093,743,"Failed compaction should return the number of the incomplete generation.The <tt>compact() method in <tt>GarbageCollector doesn't always return the new generation to the caller when the compaction operation fails.This prevents the caller to react to a failed or interrupted compaction - e.g. by cleaning up the new, invalid segments.",no,,[],,
OAK-5149,744,Set root revision for child nodes in DocumentNodeStore.getChildNodes().The method relies on the caller to set the root revision of the returned node states.It would be better if the method itself ensures the returned node states have the correct root revision set.,no,,[],,
OAK-5154,745,"Checkpoints should only be migrated if no custom paths are defined.When running a tar -&gt; tar sidegrade, the oak-upgrade tool migrates the checkpoints as well.However, this should only happen for the full migration, eg.without <tt>--{include,merge,exclude}-paths options.",no,,[],,
OAK-5199,746,"Test coverage for ExternalGroupRef.<URL>, in addition to the missing documentation, i just noticed that there is also no tests for the new <tt>ExternalGroupRef you added recently.IMHO its crucial that we don't introduce new features/improvements/fixes without tests... this has been one of the big issues in the past and I really want the old bad habits that of the <tt>oak-auth-external module to be continue.The lack of test and documentation coverage has just been so troublesome in the past.If that is not possible by mutual agreement among all devs working on this module, I would suggest that we introduce automatic control into the pom.xml that fails the build if the test-coverage drops with a given change.",no,,[],,
OAK-531,747,"NodeBuilder deleted child nodes can come back.While working on 
   <URL>, I've noticed a problem with the NodeBuilder: when we delete an entire hierarchy of nodes and then recreate a part of it, some of the previously deleted nodes can come back.This only happens when there are more than 3 levels of nodes.So given a hierarchy of nodes: /x/y/z deleted 'x' and simply use the NodeBuilder to traverse down on the same path: .child('x').child('y').At this point the 'z' child reappears even though it was deleted before.I'll attach a test case shortly.",no,,[],,
OAK-5340,748,"Build Apache Jackrabbit Oak matrix/Ubuntu Slaves=ubuntu,jdk=JDK 1.8 (latest),nsfixtures=DOCUMENT_NS,profile=unittesting #1337 failed.Jenkins CI failure: 
   <URL> 
  The build Apache Jackrabbit Oak matrix/Ubuntu Slaves=ubuntu,jdk=JDK 1.8 (latest),nsfixtures=DOCUMENT_NS,profile=unittesting #1337 has failed.First failed run: 
   <URL> 
   <URL>",no,,[pf3_profiling],,
OAK-5450,749,"Documented example for relativeNode in index aggregation does not work..The documentation contains the following example query: 
  select * from 
   <ERROR></ERROR> where contains(renditions/original/*, ""pluto"") 
  This query does not work.The parser identifies the pattern /* as begin of a comment and does not find the end of the comment.The following query works: 
  select * from 
   <ERROR></ERROR> where contains(
   <ERROR></ERROR>, ""pluto"")",no,,[],,
OAK-5590,750,"The check command doesn't do any check when ""deep"" option is not provided.When the <tt>check command is used without <tt>--deep option, there is no check/traversal being done against the repository.First relevant line in code is 
   <ERROR></ERROR>, where a check is supposed to happen, but due to a mismatch between argument expected/argument provided, <tt>null is always returned without checking anything.The method which should do the actual check 
   <ERROR></ERROR> expects a set of paths to be traversed, but this set is always empty.Therefore, relevant code for running the check is never executed 
   <ERROR>",no,,[],,
OAK-5634,751,"Expose IOMonitor stats via JMX.Followup of 
   <URL> and 
   <URL>, to expose the collected data via JMX for external use.",no,,[],,
OAK-5639,752,"Build Oak-Win/Windows slaves=Windows,jdk=JDK 1.8 (unlimited security) 64-bit Windows only,nsfixtures=DOCUMENT_NS,profile=unittesting #449 failed.Jenkins Windows CI failure: 
   <URL> 
  The build Oak-Win/Windows slaves=Windows,jdk=JDK 1.8 (unlimited security) 64-bit Windows only,nsfixtures=DOCUMENT_NS,profile=unittesting #449 has failed.First failed run: 
   <URL> 
   <URL>",no,,[pf3_profiling],,
OAK-5650,753,"RDBDocumentStore on Oracle: ArrayIndexOutOfBoundsException in Oracle JDBC driver.Seen running <tt>SetPropertyTest: 
  <SOURCECODE>",no,,[],,
OAK-5656,754,"InitialContent depends on document.bundlor.BundlingConfigInitializer.<URL>, in the light of 
   <URL> a dependency to the document nodestore code got introduced in <tt>org.apache.jackrabbit.oak.plugins.nodetype.write.InitialContent by adding the following line: 
  <SOURCECODE> 
  the <tt>BundlingConfigInitializer is defined in the <tt>org.apache.jackrabbit.oak.plugins.document.bundlor.To me that looks quite troublesome and I don't think the generic JCR-InitialContent should have any dependency on the document nodestore code base.Why not defining a dedicated <tt>RepositoryInitializer for that kind of init an making sure it is listed in the (default) setup scenarios (or at least in those that actually have a document store and thus require this)?",no,,[],,
OAK-5667,755,RDBDocumentStore: remove support for DBs without support for CASE statements in SELECT.It appears that special case isn't needed (was there for Derby) and can be removed.,no,,[],,
OAK-5723,756,"Build Apache Jackrabbit Oak matrix/Ubuntu Slaves=ubuntu,jdk=JDK 1.7 (latest),nsfixtures=SEGMENT_MK,profile=integrationTesting #1435 failed.Jenkins CI failure: 
   <URL> 
  The build Apache Jackrabbit Oak matrix/Ubuntu Slaves=ubuntu,jdk=JDK 1.7 (latest),nsfixtures=SEGMENT_MK,profile=integrationTesting #1435 has failed.First failed run: 
   <URL> 
   <URL>",no,,[pf3_profiling],,
OAK-5752,757,Remove duplicate code for background operation timing log.There are multiple places in DocumentNodeStore where background operations log timing.This should be consolidated.,no,,[],,
OAK-5955,758,Don't expose SegmentRevisionGCMBean on standby instances.The <tt>SegmentRevisionGCMBean should not be exposed on a standby instance.The cleanup and synchronization processes might be negatively influenced by a manual execution of offline compaction via <tt>SegmentRevisionGCMBean.,no,,[],,
OAK-6008,759,Create test base with a two node cluster.There's a reoccurring need for a test setup with a two node cluster.There should be a test base that provides the required bootstrapping.,no,,[],,
OAK-6015,760,"ACL of versioned node can be modified without checking out the node.On a versione node <em>nodeA</em> i can do: <tt>AccessControlUtils.clear(nodeA, userPrincipal) without having to checkout the node.After saving the session I tried to login as <em>userPrincipal</em> and I couldn't find <em>nodeA</em>, so it seems that the clear operation did work even if the node was checked-in.",no,,[],,
OAK-6016,761,"DocumentNodeStore.compare() fails with IllegalStateException in read-only mode.Comparing node states in read-only mode may fail with an IllegalStateException when the journal is used to perform a diff.<SOURCECODE> 
  See also 
   <URL>.",no,,[],,
OAK-6075,762,"Oak run console script to check index consistency.I would like to implement a groovy script to check index consistency.The check would have two modes 
  <ol> 
   Basic check - Check if all blobs referred by index files can be accessed 
   Deep check - This would use Lucene IndexCheck support 
  </ol> 
  This is meant to be interim solution untill 
   <URL> is implemented as it enables use of such a check on older branches also",no,,[],,
OAK-6082,763,"Service to determine index paths.It would be useful to have an <tt>IndexPathService which would provide  
  <ol> 
   List of all index paths 
   Determine if oak:index node is present under given path 
  </ol> 
  The service would make use of nodetype index for <tt>oak:QueryIndexDefinition for determining index paths.This service can then be used to enable improvements like  
   
   listing report for all indexes (
    <URL>), 
   Reduce lookup for oak:index nodes",no,,[gk3_reduce],,
OAK-6176,764,"Service to provide access to async indexer state.A service should be implemented to provide access to state of various async indexing lanes 
   
   Number of async indexing lanes 
   For each lane 
     
     Last indexed time 
     Running 
     If running then access to index stats 
     name",no,,[],,
OAK-6193,765,"IllegalStateException when closing the FileStore during garbage collection.When the file store is shut down during gc compaction is properly aborted.Afterwards it will trigger a cleanup cycle though, which runs concurrently to the proceeding shutdown potentially causing an <tt>ISE: 
  <SOURCECODE>",no,,[tk7_concurrent],,
OAK-6212,766,"AccessControlAction: minor improvement when user or group privileges are empty .the current code avoids retrieving the ac policy only if both the configured user and group privileges are empty.this be improved by comparing the configured privileges with the type of the authorizable to be processed.Instead of  
  <SOURCECODE> 
  and later testing for &gt;1 privileges again it should do something like 
  <SOURCECODE> 
  
   <ERROR></ERROR>, fyi.",no,,[ss23_avoid_if],,
OAK-6291,767,"Build Jackrabbit Oak #371 failed.Jenkins CI failure: 
   <URL> 
  The build Jackrabbit Oak #371 has failed.First failed run: 
   <URL> 
   <URL>",no,,[],,
OAK-6295,768,"Move constants from oak.query.QueryImpl to oak.spi.query.QueryConstants.in the light of the modularisation effort i had a look at the query code and noticed that the constants defined on <tt>QueryImpl are needed outside of the scope of the query implementation.IMO that indicates that those constants are of broader use and might better be located at <tt>oak.spi.query.QueryConstants.moving would in fact remove all imports of <tt>QueryImpl in the <em>oak-lucene</em> module.<URL>, 
   <URL>, it will attach a proposed patch for this improvement and would appreciate if you could review if you are ok with that change.",no,,[],,
OAK-6337,769,"Decide major version bump of o.a.j.o.api.jmx.With some mixup while resolving 
   <URL>, 
   <URL> made into 1.7.1 release while 
   <URL>.This led to 
   <URL> once 1.7.1 was released.To fix 
   <URL>, I bumped o.a.j.o.api.jmx from 4.6.0 to 5.0.0.This bump in major version might not be desirable in general (as was being discussed on oak-dev recently 
   <URL> [0].Towards that, if we decide to change baseline plugin's config to use last stable branch's releases as base then we can avoid this major bump.<URL> suggested offline that maybe we should ""temporarily"" add that removed method back with deprecated flag + revert major version bump.That should give us some breathing space to decide about how do we want to go ahead with this.[0]: 
   <URL>",no,,[ss23_avoid_if],,
OAK-6345,770,"Allow TokenLoginModule framework to create token for other LoginModules if userid is not known in login().If a custom LoginModule accepting custom credentials (or ExternalIdentityProvider) wants to switch the credentials (e.g. on the first request of a web app) to a token from the TokenModule (i.e. return this in the (Simple)Credentials after login() for use by a request handler) this is currently not possible when the user id is not known up front in the login() call, but only detected by the custom LoginModule, and passed around between login modules using <tt>javax.security.auth.login.name.This is a follow up from 
   <URL>.1The main recommendation there was, instead of the the TokenLoginModule respecting the shared key <tt>javax.security.auth.login.name and a special handling of SimpleCredentials as in the patch, leave this to a custom TokenProvider.This would require to change the TokenProvider API to pass through the key (or all keys), something along the lines of: 
  <SOURCECODE> 
  Since it also requires an application that has been relying on the default TokenProviderImpl, to replicate that logic, it might be desirable to make it easy to reuse that code.E.g. by wrapping and calling the other token provider (maybe this is already possible today in some way).2Another approach might be to call <tt>TokenInfo.createToken(userId, attributes) from the custom LoginModule aka ExternalIdentityProvider.The question then would be how it can access it (as e.g. osgi service) and if that's a good solution.3There might be another intended way through reusing the new CredentialsSupport from 
   <URL>, but it seems the crucial <tt>javax.security.auth.login.name is not passed through to the relevant code.",no,,[],,
OAK-6402,771,"SessionStats log access warning.SessionStats currently log an access warning when computing the <tt>RefreshPending flag, based on current setup.This is a bit confusing, as one would not expect a warning at this level, it should probably be the other way around, an external component could log a warning based on the <tt>SessionStats data.",no,,[],,
OAK-6406,772,"Cleanup constants in Segment class.Some of the constants in the <tt>Segment class still refer to the old 255 segment references limit.We should fix the comments, the constants and their usage to reflect the current situation where that limit has been lifted.",no,,[],,
OAK-6499,773,"MultiplexingPermissionProvider wrong privileges composition.It seems the CompositePermissionProvider eagerly applies deny permissions to the composed permission calculation which is good for the 'AND' version.In the case of the OR composition, this should not happen.",no,,[],,
OAK-6513,774,"Journal based Async Indexer.Current async indexer design is based on NodeState diff.This has served us fine so far however off late it is not able to perform well if rate of repository writes is high.When changes happen faster than index-update can process them, larger and larger diffs will happen.These make index-updates slower, which again lead to the next diff being ever larger than the one before (assuming a constant ingestion rate).In current diff based flow the indexer performs complete diff for all changes happening between 2 cycle.It may happen that lots of writes happens but not much indexable content is written.So doing diff there is a wasted effort.In 1.6 release for NRT Indexing we implemented a journal based indexing for external changes(
   <URL>, 
   <URL>).That approach can be generalized and used for async indexing.Before talking about the journal based approach lets see how IndexEditor work currently 
  <h4><a name=""IndexEditor""></a>IndexEditor </h4> 
  Currently any IndexEditor performs 2 tasks 
  <ol> 
   Identify which node is to be indexed based on some index definition.The Editor gets invoked as part of content diff where it determines which NodeState is to be indexed 
   Update the index based on node to be indexed 
  </ol> 
  For e.g. in oak-lucene we have LuceneIndexEditor which identifies the NodeStates to be indexed and LuceneDocumentMaker which constructs the Lucene Document from NodeState to be indexed.For journal based approach we can decouple these 2 parts and thus have  
   
   IndexEditor - Identifies which all paths need to be indexed for given index definition 
   IndexUpdater - Updates the index based on given NodeState and its path 
   
  <h4><a name=""HighLevelFlow""></a>High Level Flow</h4> 
  <ol> 
   Session Commit Flow 
    <ol> 
     Each index type would provide a IndexEditor which would be invoked as part of commit (like sync indexes).These IndexEditor would just determine which paths needs to be indexed.As part of commit the paths to be indexed would be written to journal.</ol>  
   AsyncIndexUpdate flow 
    <ol> 
     AsyncIndexUpdate would query this journal to fetch all such indexed paths between the 2 checkpoints 
     Based on the index path data it would invoke the <tt>IndexUpdater to update the index for that path 
     Merge the index updates 
    </ol>  
  </ol> 
  <h4><a name=""Benefits""></a>Benefits</h4> 
  Such a design would have following impact 
  <ol> 
   More work done as part of write 
   Marking of indexable content is distributed hence at indexing time lesser work to be done 
   Indexing can progress in batches 
   The indexers can be called in parallel 
  </ol> 
  <h4><a name=""JournalImplementation""></a>Journal Implementation</h4> 
  DocumentNodeStore currently has an in built journal which is being used for NRT Indexing.That feature can be exposed as an api.For scaling index this design is mostly required for cluster case.So we can possibly have both indexing support implemented and use the journal based support for DocumentNodeStore setups.Or we can look into implementing such a journal for SegmentNodeStore setups also 
  <h4><a name=""OpenPoints""></a>Open Points</h4> 
   
   Journal support in SegmentNodeStore 
   Handling deletes.Detailed proposal - 
   <URL>",yes,do something is a wasted effort,[gk7_speed],,
OAK-6604,775,"Oak Blob Cloud is not used by oak-upgrade.Watching at oak-upgrade 
   <URL> file I had a clue that Oak Blob Cloud is used for migration.However, I noticed that there is a wrong 
   <URL> for <tt>S3DataStoreFactory file: 
  <SOURCECODE> 
  and instead we should use: 
  <SOURCECODE> 
  ...to be able to get the newest Oak Blob Cloud features for S3 and oak-upgrade.Additionally the class <tt>org.apache.jackrabbit.core.data.CachingDataStore; should not be used anymore and it is not needed as the <tt>org.apache.jackrabbit.oak.blob.cloud.s3.S3DataStore itself provides caching capabilities.This should help to get rid of the following problems when migrating S3 backed DataStores: 
  <SOURCECODE> 
  /cc 
   <URL>",no,,[sk1_negative_necessary],,
OAK-6702,776,"Improve cold standby resiliency to incoherent configs.In order to correctly configure cold standby there are two OSGi configurations that need to be provided.Among other settings, <tt>org.apache.jackrabbit.oak.segment.SegmentNodeStoreService.config needs <tt>standby=B""true"" and <tt>org.apache.jackrabbit.oak.segment.standby.store.StandbyStoreService.config needs <tt>mode=""standby"".The problem is that sometimes we have <tt>mode=""standby"" in <tt>StandbyStoreService and <tt>standby=B""false"" in <tt>SegmentNodeStoreService which leads to starting a problematic standby instance (with primary behaviour enabled, e.g. indexing, etc.).This problem stems from the fact that there are two components whose configuration should be coordinated.Proposals to mitigate this: 
  <ol> 
   Keep the <tt>mode=""standby"", but merge the configuration of <tt>StandbyStoreService in the one for <tt>SegmentNodeStoreService and eliminate <tt>StandbyStoreService altogether 
   <tt>StandbyStoreService should derive <tt>mode=""standby"" from <tt>""standby=B""true"" in <tt>SegmentNodeStoreService 
   <tt>SegmentNodeStoreService should derive <tt>""standby=B""true"" from <tt>mode=""standby"" in <tt>StandbyStoreService even if this is backwards when compared to how the synchronization currently happens, with <tt>StandbyStoreService waiting for for a proper initialisation of <tt>SegmentNodeStoreService 
   Make <tt>StandbyStoreService configuration mandatory, but require a <tt>mode=""off"" setting.This way the removal of <tt>standby=B""true"" from <tt>SegmentNodeStoreService would be guaranteed and any synchronization between the two components would be avoided.</ol> 
  /cc 
   <URL>, 
   <URL>, 
   <URL>",no,,[],,
OAK-6720,777,"The path should be included in the TRACE log of reads.When I enable the TRACE logging of read requests (setting org.apache.jackrabbit.oak.jcr.operations.reads to TRACE) I get this result: 
  26.09.2017 15:39:15.530 TRACE [0:0:0:0:0:0:0:1 
   <ERROR></ERROR> GET /foo.html HTTP/1.1] org.apache.jackrabbit.oak.jcr.operations.reads 
   <ERROR></ERROR> getValue 26.09.2017 15:39:15.530 TRACE [0:0:0:0:0:0:0:1 
   <ERROR></ERROR> GET /foo.html HTTP/1.1] org.apache.jackrabbit.oak.jcr.operations.reads 
   <ERROR></ERROR> getItemOrNull 26.09.2017 15:39:15.530 TRACE [0:0:0:0:0:0:0:1 
   <ERROR></ERROR> GET /foo.html HTTP/1.1] org.apache.jackrabbit.oak.jcr.operations.reads 
   <ERROR></ERROR> hasProperty 
  It would be good if the path where this read operation happens, would be logged as well.As it is on TRACE level and should be rarely used, even a performance impact would be acceptable.",no,,[gk4_performance],,
OAK-6821,778,"Enforce minimum line coverage for oak-security-spi.Enforce minimum line coverage for oak-security-spi.Follow up to 
   <URL> to enforce minimal test coverage also for the new <em>oak-security-spi</em> project.Follow up to 
   <URL> to enforce minimal test coverage also for the new <em>oak-security-spi</em> project.cc: 
   <URL>cc: 
   <URL>",no,,[],,
OAK-6831,779,"Nodetype index support in Lucene Index.Lucene indexes currently support nodetype index in some form by specifying a property definition for ""jcr:primaryType"" with propertyIndex=true.However this can cause issue if such rules are mixed with other rules.For supporting usecase where same lucene index supports multiple nodetype rules and can be used as pure nodetype index we should have a explicit support for indexing nodetypes.Proposal 
  Any indexRule would support following properties 
   
   <tt>nodeTypeIndex - Boolean indicating if this rule is for nodetype indexing 
   <tt>sync - If nodetype index is sync 
   
  if <tt>nodeTypeIndex is enabled then any explicit property definition would be ignored.With this mode following index definition would be safe to use (updated definition per final implementation) 
  <SOURCECODE> 
  Here the rule order would not cause any affect as for any matching rule the nodes primary and mixin types would be indexed",no,,[ss1_one_per],,
OAK-706,780,Make workspace name available with the permission provider.currently we either have to passing around the workspace name as a separated param or do some fancy tricks while at the end of the day the workspace name was already present with the Root.the associated TODOs should be addressed either by consider introducing Root#getWorkspaceName or be deciding that we really want to pass around the workspace name (last time we decided against this option).,no,,[],,
OAK-707,781,"Review interaction between AccessControlManager and PermissionManager.the following methods on the access control manager interface require interaction with the permission evaluation: 
  #hasPrivileges(String, Privilege[]) #getPrivileges(String) #hasPrivileges(String, Set&lt;Principal&gt;, Privilege[]) #getPrivileges(String, Set&lt;Principal&gt;) 
  right now those methods are not supported if there was no permission provider passed to the acmgr constructor, which obviously is far from being optimal.in general i see the need for closer interaction between oak-api and the permission evaluation... (see also the related TODO with Root#getCommitSubject)",no,,[],,
OAK-7204,782,RDB*Store: update postgresql JDBC driver reference to 42.2.0.,no,,[],,
OAK-7226,783,"Warn messages ignoring exception parameter.There are a few usages of Logger.warn() with a pattern similar to this example: 
  <SOURCECODE> 
  The intention probably is that the third parameter is treated as an exception and e.g. logged with the stack trace.However, this method signature interprets the exception as a second argument for the message format.This means the exception is effectively ignored.",no,,[pf2_duration],,
OAK-7297,784,"New fixture for the Azure Segment Store.We should add a new <tt>SEGMENT_AZURE fixture, so it's possible to run the integrationt tests for the 
   <URL>.By default it should connect to a local Azure Storage emulator: 
   
   
    <URL> 
   
    <URL> 
   
  If the <tt>oak.segment.azure.connection system property is defined, then it should try to connect to the specified 
   <URL>.",no,,[],,
OAK-7355,785,"Move the pluggable storage interfaces to the SPI package.<URL> introduces a new abstraction layer in the oak-segment-tar, which allows to support storing the segments not only in the tar files, but in any types of persistence, implementing the SegmentArchiveManager.The types required to implement a custom SegmentArchiveManager are now spread across many places within the oak-segment-tar.This results in an extensive export list in the OSGi bundle.In order to keep the SegmentNodeStore implementation encapsulated, we should move the new interfaces and all the other required classes into a new <tt>org.apache.jackrabbit.oak.segment.spi package.",no,,[ss2_all_to_one],,
OAK-7429,786,"Build Jackrabbit Oak #1388 failed.No description is provided 
  The build Jackrabbit Oak #1388 has failed.First failed run: 
   <URL> 
   <URL>",no,,[],,
OAK-7457,787,"""Covariant return type change detected"" warnings with java10.We have quite a few warnings of type ""Covariant return type change detected"": 
  <SOURCECODE> 
  I believe these will cause problems if we compile with java 10 but run on an earlier VM.See also 
   <URL>",no,,[],,
OAK-7497,788,"Deprecate org.apache.jackrabbit.oak.plugins.lock.LockConstants in favor of corresponding SPI.for consistency with other JCR specific constants that have been copied to <em>oak-core-spi</em>, we should do the same for <tt>LockConstants.This will help to further reduce the dependency to <em>oak-core</em> across the different modules.",no,,[gk3_reduce],,
OAK-7528,789,"Build Jackrabbit Oak #1476 failed.No description is provided 
  The build Jackrabbit Oak #1476 has failed.First failed run: 
   <URL> 
   <URL>",no,,[],,
OAK-7562,790,"Build Jackrabbit Oak #1498 failed.No description is provided 
  The build Jackrabbit Oak #1498 has failed.First failed run: 
   <URL> 
   <URL>",no,,[],,
OAK-7572,791,"[DirectBinaryAccess][DISCUSS] Support direct access via enhanced binary capabilities.The proposal in 
   <URL> enables direct upload and download capabilities via API enhancements.Would it be better instead to take an approach similar to that in 
   <URL> and enable this via enhanced capabilities of a Binary subclass or extension interface?",no,,[],,
OAK-771,792,"Correct behavior for strange node and property names.All components need to deal correctly with strange identifier names, for example umlauts, empty node or property names, slashes, dots (a problem for MongoDB), and so on.Some cases are legal, some not.Oak should throw an appropriate error message where needed.One case that doesn't work properly is trying to add a node named """" (empty node name) to the root node.In this case, the exception message is ""node already exists"".The message should probably be ""illegal node name"" or so.",no,,[],,
OAK-7752,793,"Build Jackrabbit Oak #1646 failed.No description is provided 
  The build Jackrabbit Oak #1646 has failed.First failed run: 
   <URL> 
   <URL>",no,,[],,
OAK-7808,794,Incorrect facet counts when some results are inaccessible due to ACLs.When an inaccessible item is part of search result then we are currently subtracting facet counts of facet labels and not just from the label(s) associated with the inaccessible item,no,,[],,
OAK-7855,795,"rdbmk: Invalid SplitDocType when run on non-upgraded DB.After upgrading a repository from 1.8.2 to 1.8.7 on RDB the background update thread started to log warnings that complain about invalid SplitDocType: 
  <SOURCECODE> 
  It is also worth noting that the automatic database schema upgrade fails on that repository with: 
  <SOURCECODE> 
  and 
  <SOURCECODE> 
  It turns out this is unrelated to the update from 1.8.2 to 1.8.7.The issue also occurs with 1.8.0.",no,,[],,
OAK-7877,796,"Avoid unnecessary operations when logging read operations.The log statements introduced in <tt>CachingSegmentReader by 
   <URL> perform multiple string concatenations even if that specific log is disabled.The log statements should be executed only if the right log level has been enabled for that logger.",no,,"[sk1_negative_necessary, ss23_avoid_if]",,
OAK-7954,797,Record when no index is used for a certain query.When no index is suitable to handle a query that should be recorded in an <em>index unavailable</em> metric.The rationale is keep this metric value as low as possible.,no,,[],,
OAK-802,798,"Occasional type validation errors in TCK setup.Occasionally, like in 1% of the cases, I see a default <tt>mvn clean install -PintegrationTesting build fail with lots of type validation errors from the TCK.A typical example is: 
  <SOURCECODE> 
  See 
   <URL> for the failsafe output of such an case.I've been able to reproduce the problem with a build loop like the following on my laptop and on a m1.xlarge ec2 instance (it didn't show up on a m1.medium instance): 
  <SOURCECODE>",no,,"[pf1_percentage, tk6_iteration]",,
OAK-8067,799,"Measure fsync (called when closing the NRT index) and try to reduce disk I/O.We have seen the following stack trace, and saw that fsync was seemingly very slow on the system.Two issues: (1) fsync seems to be very slow here (2) fsync is called while holding a lock.(2) should be resolved by lazy-loading indexes.<SOURCECODE>",yes,fsync seems to be very slow here,"[gk7_speed, gk3_reduce]",,
OAK-843,800,"Optimization in the Node#hasNodes().IMHO o.a.j.oak.jcr.NodeImpl#hasNodes() has the same ""issue"" reported for 
   <URL>.Indeed it makes a call to getChildCount() that can be avoided.In order to know if one node has nodes is not necessary to get the full count of the children.Should you agree I might provide a patch",no,,[sk1_negative_necessary],,
OAK-85,801,"NPE and wrong result on copy operation.<SOURCECODE> 
  results in a NPE.Doing all operations separately instead gives the wrong result: 
  <SOURCECODE> 
  results in (relative to /root) 
  <SOURCECODE> 
  instead of 
  <SOURCECODE>",no,,[],,
OAK-91,802,Implement Authentication Support.,no,,[],,
OAK-917,803,"Container: Issues with Workspace#copy and Root#copy.container issue to collect issues with the current implementation of Workspace#copy: 
  <ul class=""alternate"" type=""square""> 
   missing UUID generation 
   no version history generated for versionable nodes 
   no handling for locks present on copied nodes",no,,[],,
OAK-943,804,"Provide more details with CommitFailedException.Currently if a commit fails due to conflict it throws following exception 
  <SOURCECODE> 
  With this much information its hard to see the conflict details.To help in debugging such issues the exception message must include the conflict related information 
  Discussion Thread - 
   <URL>",no,,[],,
PDFBOX-1184,805,"[PATCH] Bug in PDPage when the page is rasterized.When a PDPage is rasterized, the image created isn't cropped properly.Tests are included in this patch",no,,[],,
PDFBOX-1282,806,"Unicode characters displayed with wrong glyps because of interpretation as 8 bit strings.the file Characters_Arial.pdf shows that some unicode values are displayed with wrong glyphs, for example the u2020 which is displayed as two spaces.Another Issue is that invalid unicode characters are not handled correctly.They should display the invalid character box or something like that.This is demonstrated with a modified version of the file.The method processEncodedText is called when the texts of the document are printed  
  <SOURCECODE>.This code tries to determine if the values in variable 'string' are 8 or 16 bit values or even a mixture of both types of values &lt;lol&gt;.Everything works fine when variable 'string' contains 8 bit values, in most cases.If there is an invalid 8 bit value this character may be dropped together with the following character.The real problem occurs when the data in variable 'string' is encoded as 16 bit values.For many characters this works fine as the first byte is usually not a valid character:  for example u0041 is first tried as char 00 with codeLength=1 an as there is no entry for unicode 0 in the font it will be re-tried with codeLength=2 and then interpreted as u0041.But what happens if the first byte of the 16 bit code is also a valid character code?to check this I created the file Characters_Arial_Changed.pdf where I simply changed the 16-bit string &lt;0041&gt; which displays 'A' to &lt;4141&gt; which is an invalid character in this font.I Also changed a 8-bit string nearby from (0041) to the value &lt;4141&gt;.Note that there are now two strings with the same value &lt;4141&gt; which have to be displayed in a different way.Acrobat Reader then shows the invalid character box for the 16 bit string and 'AA' for the 8 bit string above.PDFBox shows 'AA' for both strings.Problems are occuring with valid unicode character codes too: Unicode u2020 will be shown as two nice spaces in PDFBox where Adobe Reader shows the correct character.To guess that it is a 16 bit character when the first byte is an invalid character in the current font is the wrong way to handle the string values.If the variable 'string' contains 8 or 16 bit values can't be detected by analysing the values as the example shows.processEncodedText has to handle the data in variable 'string' as 16 bit values when the font which is used has an (unicode-)encoding which uses more than 256 characters, in all other cases it should be interpreted as 8 bit values!!!With an Unicode Font &lt;4343&gt; or (CC) should show the invalid character box, with an 8 bit font both values should show the text 'CC'.I have included this example in the file too.The Adobe documentation says about 8 or 16 bit values in strings for example: ""When the current font is a Type 0 font whose Encoding entry is Identity-H or Identity-V, the string to be shown shall contain pairs of bytes representing CIDs, high-order byte first.When the current font is a CIDFont, the string to be shown shall contain pairs of bytes representing CIDs, high-order byte first.When the current font is a Type 2 CIDFont in which the CIDToGIDMap entry is Identity and if the TrueType font is embedded in the PDF file, the 2-byte CID values shall be identical glyph indices for the glyph descriptions in the TrueType font program.""I guess depending on this information it has to be determined if the string is 8 or 16 bits!In my example pdf files the type 0 font has always the Indentity-H set as encoding and so the strings have to be en-/decoded as pure 16 bit strings.",no,,"[sk3_byte, ss11_rework]",,
PDFBOX-1321,807,"PDF rendered as black box .Images in PDF being rendered as black boxes, but in Acrobat Reader everything is fine.",no,,[],,
PDFBOX-1338,808,Problem with text and embedded files extraction.Could not extract text and embedded files from PDF,no,,[],,
PDFBOX-1609,809,"EXCEPTION_ACCESS_VIOLATION with PDF file and image conversion.Hello, 
  I do receive an unhandled exception during image conversion of the attached pdf file.For image conversion i've used the PDFToImage.java example.The pdf file has been created with Foxit PDF-Printer.Every generated pdf file will crash the whole JRE, so my whole application crashes with the following trace.The affected pdf file has been attached.The error report file has been attached.Hopefully this error can be handled that the JRE won't crash anymore.Thanks 
  # 
  <ol> 
   A fatal error has been detected by the Java Runtime Environment: # 
   EXCEPTION_ACCESS_VIOLATION (0xc0000005) at pc=0x720c2d77, pid=3532, tid=2620 # 
   JRE version: 7.0_21-b11 
   Java VM: Java HotSpot(TM) Client VM (23.21-b01 mixed mode windows-x86 ) 
   Problematic frame: 
   C 
    <ERROR></ERROR> # 
   Failed to write core dump.Minidumps are not enabled by default on client versions of Windows # 
   An error report file with more information is saved as: 
   U:\Eclipse-Workspace\de.vlh.notes.tools\hs_err_pid3532.log # 
   If you would like to submit a bug report, please visit: 
   
    <URL> 
   The crash happened outside the Java Virtual Machine in native code.See problematic frame for where to report the bug.# 
  </ol>",yes,hot spot,[sk5_hotspot],,
PDFBOX-1615,810,"Color map not correctly copied when PDF file is split.A customer has a pdf file which we split (pdfbox.util.Splitter) for inclusion in a document.When I split the file using the PDFSplit tool, the same problem occurs.On some pages, the color map appears to be altered such that background and text are rendered in different colors (yellow/lilac instead of white/blackish).The PDF the customer supplies is probably a scanned document but the metadata claims it is created using PDF-XChange Viewer 2.5.195.0.The odd thing is that only a subset of pages are affected in a consistent fashion.I can supply the original PDF file on request for you to look at.",no,,[],,
PDFBOX-1851,811,"[PATCH] Improved CMYK color space conversion.This is a patch to resolve a long-standing issue with CMYK color spaces.DeviceCMYK is a device-dependent color space and so should always be associated with an ICC color profile.In Acrobat the default profile is ""U.S. Web Coated (SWOP) v2"", which unfortunately is only available from Adobe under a non-free license.I've modified PDDeviceCMYK to use an ICC color profile rather than a hardcoded transform, this allows the user to use their own profile.I've included ""ISO Coated v2 300% (basICColor)"" as the default profile, which is from OpenICC 
   <URL> redistributed under the zlib license.This color space is an open alternative to a version produced by ECI which is highly recommended as a generic CMYK profile.Note: Using ICC profiles is a huge improvement on the matrix-based transform used by Xpdf, Poppler, MuPDF, etc.This patch requires the following file to be deleted: pdfbox/src/main/java/org/apache/pdfbox/pdmodel/graphics/color/ColorSpaceCMYK.java",no,,"[pf1_percentage, pf3_profiling]",,
PDFBOX-1896,812,Support MMType1 (Multiple Master) Fonts.MMType1 Fonts are not supported.They are described on p.416 of the 1.7 spec.I attach a sample file.,no,,[],,
PDFBOX-2024,813,"/Rotate 180 PDF is not displayed correctly in PDFReader app.To test 
   <URL> I created a PDF with /Rotate 180.It does not display correctly in the PDFReader app (class PDFPagePanel).The purpose of this issue is to fix this and/or also use Johns new renderPageToGraphics() method per DRY.I won't do it immediately so if anybody else wants to do it, just assign it to yourself.",no,,[ss1_one_per],,
PDFBOX-2173,814,"Nullpointer when validating empty file.I am validating a PDF and I am getting a NullpointerException when the filesize is 0 bytes.I looked at the code and saw a small misstake, and therefore I am reporting this.I use version 1.8.6 
  <SOURCECODE> 
  As you can see <tt>secondLineAsBytes is checked for null value, but only after being called once.<SOURCECODE> 
  <h4><a name=""Workaround""></a>Workaround</h4> 
  Before validating, check the filesize, if it is larger than 0.<h4><a name=""Howtoreproduce""></a>How to reproduce</h4> 
  Try to validate with an empty file.",no,,[sk3_byte],,
PDFBOX-2503,815,"false negative?1: 7.2 : Error on MetaData, Producer present in the document catalog dictionary doesn't match with XMP information.at org.apache.pdfbox.preflight.metadata.SynchronizedMetaDataValidation.unsynchronizedMetaDataError(SynchronizedMetaDataValidation.java:534) at org.apache.pdfbox.preflight.metadata.SynchronizedMetaDataValidation.analyzeProducerProperty(SynchronizedMetaDataValidation.java:277) at org.apache.pdfbox.preflight.metadata.SynchronizedMetaDataValidation.validateMetadataSynchronization(SynchronizedMetaDataValidation.java:471) at org.apache.pdfbox.preflight.process.MetadataValidationProcess.validate(MetadataValidationProcess.java:83) at org.apache.pdfbox.preflight.utils.ContextHelper.callValidation(ContextHelper.java:73) at org.apache.pdfbox.preflight.utils.ContextHelper.validateElement(ContextHelper.java:88) at org.apache.pdfbox.preflight.PreflightDocument.validate(PreflightDocument.java:168)",no,,[],,
PDFBOX-2642,816,"Unsupported: Type1 PFB fonts in CIDFontType0.<SOURCECODE> 
  UPDATE: The PDF contains a CIDFontType0 which has a FontFile3 containing a Type1 PFB font.This is invalid, but Type1C is permitted and the two are equivalent, so it should be possible to handle this.However, the current design of PD CIDFontType0 does not permit it.",no,,[],,
PDFBOX-2768,817,"Remove VisualSignatureParser.As discussed with 
   <URL> in the 
   <URL> - remove VisualSignatureParser, and use PDFParser instead.If successful, this will also allow to remove one of two implementations of parseCOSStream(), as suggested by 
   <URL> a few weeks ago.",no,,[],,
PDFBOX-2835,818,"[PATCH] Make Overlay class properly extendable.We are trying to implement custom logic for overlaying documents.However, current PDFBox implementation is excessively strictly encapsulated and it's not really possible.What makes things worse, overlay implementation is very strict about the way output gets processed.Please find attached link to the patch that makes overlay logic more flexible by: 
  1) Making some of the methods protected; 2) Moving output logic to a method that can be overridden.3) Moving getting list of pages to be processed logic to a method that can be overridden.<URL> 
  If there are some adjustments that should be done or a separate version made for trunk - please let me know, I'll get to it immediately.",no,,[],,
PDFBOX-3040,819,"Move website to local build tool.As discussed on the dev mailing list the PDFBox website will be migrated to a local build: 
  <ul class=""alternate"" type=""square""> 
   to have an easier support for a new/blog section 
   an actively developed toolchain to build the site 
   
  The site builder will be based on jekyll with some additional scripting/maven to automate the develop/build/test/publishing steps.There are some projects we can learn from such as Apache Wicket, JClouds.The initial development will be done in a branch to the current svn repository.",no,,[],,
PDFBOX-3376,820,"Memory leak in PDDcoument.close method.PDFBox PDDocument still uses memory after destruction.This causes, surprisingly, memory leaks in Java.Sample code: 
  <SOURCECODE> 
  Mine output: &gt;Start: 125829120 &gt;All open: 277872640 &gt;All closed: 277872640 &gt;rthrth &gt;Finish: 277872640 
  The same happens with my main program (I was blaming SwingWorker): 
   <URL>",yes,memory leak,[ss4_memory],,
PDFBOX-3454,821,"Wrong color and line setting when filling a form.As reported on so 
   <URL> the form fields are not rendered correctly using PDFBox 2.0.x.That's due to not setting the line and color information correctly.",no,,[],,
PDFBOX-3461,822,"Improve handling of line breaking characters when setting AcroForm field values.When filling AcroForm fields the text supplied to <tt>setValue might contain control characters such as <tt>\n <tt>\r, <tt>\t and others which leads to an {{ java.lang.IllegalArgumentException}} for most fonts as there is no glyph to represent these.We could enhance the processing of the string provided to provide proper replacements similar to how Adobe Acrobat handles that.",no,,[],,
PDFBOX-3498,823,"Unexpected spaces in text extraction.In the tests by 
   <URL> regressions were found with files from Delaware courts, see reduced file attached.The extracted text with 2.0.2 and 2.0.3 is 
  <SOURCECODE> 
  in 2.0.1 and 1.8 it was 
  <SOURCECODE> 
  The cause is the /W ranges table: 
  <SOURCECODE> 
  For text extraction, the width of a space is caculated by taking an average of all widths.However these files have a lot (over 60000) of widths that are 0.So I'll just ignore widths &lt;= 0, as it is already done in PDFont.",no,,"[sk13_regression, gk3_reduce]",,
PDFBOX-3688,824,"Cache TilingPaint generation.The attached file gs-bugzilla692158-schleuse-veryslow.pdf is extremely slow to render.It takes hours.The cause is that identical TilingPaint objects are generated again and again, i.e. each time the pattern image must be drawn again.I tried a very simple caching mechanism and now the file renders in 20 seconds.Although the very basic caching worked for all test files, it can't be used because parameters could change: initial matrix, xform, color and colorspace.For this I'll use a factory class that has a class to compare the parameters, and this is to be used as key.",yes,PDF document is extremely slow to render,"[sk12_over_again, gk7_speed, pf2_duration]",,
PDFBOX-3799,825,"Problem in TextPosition's hashCode.Just another side effect related to TextPosition's hashCode 
  I am using the hashCode because I want to know the color of each letter.To do this, during the processTextPosition, I save the current graphic state in a map, using the current text position as key.Then, on writeString, I iterate all the text positions and I get the color for each of them though this map.Of course would be easier if this information could be saved in the text position.But this is just a desired feature.I am discovering that from processTextPosition to writeString sometimes happens that the same textPosition has just a different unicode.In processTextPosition is just a ""x"" (char 120), but then on writeString the same textPosition the unicode is the x, followed by '????????' (char 772).Everything about the textPosition remains the same: same coordinates, same System.identityHashCode; the only thing that changes is the unicode, which causes the computation of a different hashCode.That is giving problem.As workaround I am using now System.identityHashCode instead of the current TextPosition's implementation",no,,[tk6_iteration],,
PDFBOX-4092,826,"Support margin /RD getter and setter for Square, Circle, FreeText and Caret annotation.Square, Circle, FreeText and Caret annotations support a margin entry <tt>/RD to define the border offset from the outer rectangle <tt>/Rect",no,,[],,
PDFBOX-4128,827,"Glyphes missing in text extraction.Dear Apache contributors, 
  I found some documents where glyphes are missing while extracting the text.Could you confirm ?Hand, A.",no,,[],,
PDFBOX-459,828,"Trailer Dictionary object labeled ""Size"" is overwritten when there are 2 xref table objects.While parsing a linearized PDF file, all of which have 2 distinct xref tables in different locations in the file, I noticed that the Size entry in the dictionary was being overwritten to the wrong value.The value that should be used for these files is the number in the first trailer section which represents the size of all the entries.",no,,[],,
PDFBOX-461,829,"Disable javadoc creation timestamp.I'm optimizing the automatic generation and deployment of the PDFBox web site, and there is a ""Generated by javadoc on ..."" comment in all the generated javadoc files that makes them appear as modified even if no other content is the file is changed.I'd like to set up a regexp replace rule that drops this comment from the generated javadoc files.",no,,[],,
PDFBOX-607,830,"Problem setting custom appearance on PDAnnotationRubberStamp.I'm trying to generate a RubberStamp with a custom appearance in a PDF.I wrote the following test-code: it saves the document and I can find the image inside the pdf, by examinating the structures.But the Rubberstamp isn't shown within the PDF when opened with the Reader.I made a second PDF using Acrobat Pro with the same stamp - there I can see the stamp when opening with a Reader.Here is the test-code I wrote: (I came to this solution by analyzing the structure of a manually set custom RubberStamp from the Acrobat Pro document): 
  <SOURCECODE>.",no,,[pf2_duration],,
PDFBOX-704,831,"Implementation of additional CMAP Formats for TrueType fonts.Hi,  
  According to the Apple's ""TrueType Reference Manual"" and the Microsoft's ""TrueType 1.0 Font Files Technical Specification"", there are several CMap formats.Currently FontBox implements formats 0, 4 and 6.In attachment you can find a patch which implements formats 2, 8, 10, 12 and 13 according to my understanding of the following links :  
  
   <URL> (OpenType Specification) 
   <URL> 
  This patch includes changes proposal of the JIRA Issues : 
   <URL>, 
   <URL>, 
   <URL>  
  I hope this patch will help you.Regards, Eric",no,,[],,
PDFBOX-786,832,"PDChoiceField's implementation of SetValue does not work for all PDF forms.Sometimes it throws ClassCastException.For a choice with the following options: 
<SOURCECODE>.A class cast is thrown as the code expects all items to be strings as the first item is a string.However, the remaining items are all pairs.The attached code is the fix.It just makes the pair/single check every time through the loop.",no,,[tk6_iteration],,
SLING-1514,833,Remove the dependency to the Sling JCR API.Currently the servlet resolver depends on the Sling JCR API - with the new ResourceResolverFactory in the Sling API we can directly use that and remove the dependency to the Sling JCR API.,no,,[],,
SLING-162,834,"NPE in DefaultHtmlRenderer if resource type not found via include.&lt;% SyntheticResource res = new SyntheticResource(child.getPath(), ""/apps/myapp/test/nav""); %&gt; &lt;sling:include resource=""&lt;%= res %&gt;""/&gt; 
  this results in a NPE: 
  Caused by: java.lang.NullPointerException at org.apache.sling.usling.renderers.DefaultHtmlRenderer.render(DefaultHtmlRenderer.java:53) at org.apache.sling.usling.renderers.DefaultHtmlRendererServlet.doGet(DefaultHtmlRendererServlet.java:72) at org.apache.sling.api.servlets.SlingSafeMethodsServlet.mayService(SlingSafeMethodsServlet.java:261)",no,,[pf1_percentage],,
SLING-1731,835,"Sling Explorer: cannot edit nodes using Chrome.Tried the explorer by adding the contrib/explorers/jquery bundle to a fresh launchpad/builder standalone Sling instance.On Firefox 
   <URL> correcly displays the ""add new sub-node"" and related editing fields, but using Chrome 5.0.375.127 on macosx those fields are not shown.Screenshot follows.",no,,[],,
SLING-1823,836,"Use ConfigAdmin configurations for queues.Currently the jobs send to the job handler might contain queue configuration information, like a queue name, parallel settings etc.These props are used to create queues.This approach has a least two potential problems: 
  <ul class=""alternate"" type=""square""> 
   What happens if two jobs specify the same queue name with different settings?The developer creating the code to send the job might not be the person deciding what is processed in which queue and how 
   
  Therefore it would make more sense to have a queue as a configuration in the ConfigAdmin .this would make maintenance easier and allows to have a single place for queue configurations.Of course the old props should still be supported for compatibility.The configurations can then be easily added, viewed and monitored through the web console.The main queue is a configuration which is always available",no,,[],,
SLING-246,837,"Replace Resource.getResourceProvider() by Resource.getResourceResolver().Currently the Resource provides access to the ResourceProvider which created the resource.This is not really practical and probably not correct at all because the ResourceProvider is something operating behind the scenes on behalf of the ResourceResolver.Thefore this method should be replaced by a method providing access to the ResourceResolver causing the Resource object to be created by the ResourceProvider.Changes due: 
   Resource: + getResourceResolver() 
  <ul class=""alternate"" type=""square""> 
   getResourceProvider() 
   
   ResourceProvider: + add ResourceResolver arguments to all methods creating Resource instances",no,,[ss2_all_to_one],,
SLING-3075,838,Update to Metatype 1.0.8.We should update to the new Apache Felix Metatype 1.0.8 releases as this fixes some bugs,no,,[],,
SLING-3440,839,"Clarify script resolution documentation with regard to selectors.The documentation around URL to Script Resolution 
   <ERROR></ERROR> is very good, except for three points.a) The resourceType given in the example is confusing.Rather than sling:sample, please make that sling/sample.Maybe you could add a hint, that if no resourceType is given, the fallback is to use the primary node type.b) You should explicitly mention that the order of selectors is relevant.c) You should mention that if less selectors are given in the script name than are given in the request, the selectors given in the script name must be the first selectors from the request (to be a potential match).Maybe an example would be good 
  i.e. script name is print/a4.esp  and the request is 1) &lt;some resource&gt;.print.a4.unspecifiedselector.html (matches) 2) &lt;some resource&gt;.unspecifiedselector.print.a4.html (does not match) 
  
   <ERROR></ERROR> - 
   <URL>",no,,[],,
SLING-3459,840,"sling:call should not log exceptions with the full stacktrace.Currently within the sling:call tag all exceptions are both logged on error level and rethrown (
   <URL>).That is not a good practice, because the same stack traces would appear twice in the log (once for the generic exception, logged by the CallTag and once for the wrapped JspException).Rather do not log the exception within the CallTag and leave that to other handlers.Just rewrapping the exception into the JspException should be enough.No information would be lost that way, and stack traces would only be logged once (by the code responsible to catch the JspException).",no,,[],,
SLING-3539,841,"SimpleDSComponent from archetypes does nothing by default.The SimpleDSComponent generated by the bundle archetypes is registered as a Runnable, but it is not immediate.Therefore, it is never invoked.Since using immediate components is not a good practice, I'll change the components to be scheduled jobs, so that they actually output something in the log.",no,,[],,
SLING-3625,842,Add tests for replication between author and publish.Currently there are integration tests that only test the resource providers.We should have also tests that start two instances and test the replication between them.,no,,[],,
SLING-3774,843,"Allow changing node type even if checks say it's not allowed.I had an example where I wanted to change an nt:folder to a sling:Folder and it complained that that was not possible.But on the server-side it was possible (parent was cq:ClientLibraryFolder).So as a quick fix, allow the user to overwrite whatever the check says on case-by-case basis.",no,,[gk7_speed],,
SLING-387,844,"Simplify script paths and names.According to the findings in the dev list thread ""Simplifying script paths and names?""at 
   <ERROR></ERROR> I would now like to propose the implementation of this change in script/servlet path resolution: 
  Note: This issue talks about scripts.But as servlets are mirrored into the virtual Resource Tree accessible through the ResourceResolver, servlets are treated exactly the same as scripts (or vice-versa actually).So the discussion applies to servlets as well as to scripts.(1) Script Location 
  Scripts to handle the processing or a resource are looked up in a single location: 
    {scriptPathPrefix}/{resourceTypePath}
   
   Where {scriptPathPrefix} 
   is an absolute path prefix (as per ResourceResolver.getSearchPath()) to get absolute paths and  {resourceTypePath} is the resource type converted to a path.If the {resourceTypePath} 
   is actually an absolute path, the  {scriptPathPrefix} 
   is not used.Example: Given the search path [ ""/apps"", ""/libs"" ] and a resource type of sling:sample, the following locations will be searched for scripts: 
   
   /aps/sling/script 
   /libs/sling/script 
   
  (2) Within the location(s) found through above mechanism a script is searched whose script name matches the pattern 
    {resourceTypeLabel}.{selectorString}.{requestMethod}.{requestExtension}.{scriptExtension}
   
   where the fields have the following meaning:
   
   {resourceTypeLabel} 
   - the last segment of the  {resourceTypePath} 
   (see above) This part is required.Only scripts whose name starts with this name are considerd  {selectorString} 
   - the selector string as per RequestPathInfo.getSelectorString This part is optional.The more selectors of the selector string match, the better.{requestMethod} 
   The request method name.This is optional for GET or HEAD requests and is required for non-GET/non-HEAD requests {requestExtension} 
   The extension of the request.This is optional.{scriptExtension} 
   The extension indicating the script language.Not used for selecting the script but for selecting the ScriptEngine.This is of course not existing for servlets.If multiple scripts would apply for a given request, the script with the best match is selected.Generally speaking a match is better if it is more specific.More in detail, a match with more selector matches is better than a match with less selector matches, regardless of any request extension or method name match.For example, consider a request to resource /foo/bar.print.a4.html of type sling:sample.Assuming we have the following list of scripts in the correct location: 
   (1) sample.esp (2) sample.GET.esp (3) sample.GET.html.esp (4) sample.html.esp (5) sample.print.esp (6) sample.print.a4.esp (7) sample.print.html.esp (8) sample.print.GET.html.esp (9) sample.print.a4.html.esp (10) sample.print.a4.GET.html.esp 
  It would probably be (10) - (9) - (6) - (8) - (7) - (5) - (3) - (4) - (2) - (1).Note that (6) is a better match than (8) because it matches more selectors even though (8) has a method name and extension match where (6) does not.If there is a catch, e.g. between print.esp and print.jsp, the first script in the listing would be selected (of course, there should not be a catch...) 
  
   <ERROR></ERROR> 
   <URL>",no,,"[ss1_one_per, ss3_in_one_go]",,
SLING-3953,845,Remove password information for accesing content from config files.Currently the credentials for accessing content can be configured in an osgi config.This is not a recommended security practice.We should use instead a service user and allow that to be configured.,no,,[],,
SLING-4322,846,"JUnit Core: All RunListeners should expose the stack trace in case of test failures.Currently if a remote unit test fails on the client side you only see the assertion message but not the stack trace (which could give more hints, why exactly the test case failed).By default the <tt>SlingRemoteTestRunner leverages the response of the <tt>JsonRenderer to give out an error.The <tt>JsonRenderer currently does not print the stack trace.It should rather do that via <tt>Failure#getTrace() instead of just relying on <tt>Failure#toString().That should be done for all Renderers (to also ease debugging if the JUnit servlet is directly called with a browser).The response trace should then correctly be evaluated in <tt>SlingRemoteTest.run.",no,,[ss2_all_to_one],,
SLING-4365,847,"Streamline ESAPI configuration.Currently the ESAPI is configured using the DefaultSecurityConfiguration class.This configuration is configured such as to: 
   
   read configuration from various file system locations, e.g. the user's home folder 
   list the helper classes to be used 
   configure the checking 
   configure logging 
   
  In our context and setup, we don't want to have different classes configured, we want logging to always go through SLF4J logging and we want to limit and control where the configuration is read from.This issues is about creating a custom SecurityConfiguration class : 
   
   read from defined locations, probably one in the repository and one embedded in the bundle as a fallback.For example using the same configuration file as embedded default as for Sling Initial Content installation in the repository.always log to SLF4J, maybe requiring an SLF4J based ESAPI LogFactory implementation.As a fallback, Log4J or commons logging APIs could still be used through the existing *-to-SLF4J API bridges we use.Only support configuration of validation patterns (hence all classes ""statically"" defined)",no,,[],,
SLING-4723,848,resourceresolver-mock: Make sure type conversion on write is applied on putAll method as well.due to a method overload matching error the type conversion is currently not applied on values written via putAll to a ValueMap.,no,,[],,
SLING-4971,849,"""static"" node name not allowed as a script ancestor.Looks like having ""static"" as a script ancestor (e.g. /apps/blah/static/foo/foo.html) will break badly the script rendering (and all the rendering process), yelling: 
  <SOURCECODE>",no,,[],,
SLING-526,850,"Document the new log configuration support on the Site.The logger configuration is more fine grained as of Rev. 666782.This extension must be documented on the site 
   <ERROR></ERROR> 
  
   <ERROR></ERROR> 
   <URL>",no,,[],,
SLING-5394,851,"Disable non-standard functionality for Apache Felix SCR factory components.The sling.properties file still has: ds.factory.enabled = true 
  although nothing in the Sling code base is using this.As this is a propriatary functionality, we should not enable it by default.",no,,[],,
SLING-5648,852,"Make Non-Eclipse Module regular Maven Packages.Non Eclipse modules like api, artifacts, imll-resource, impl-vlt should be regular Maven packages (jar) instead of eclipse plugins (eclipse-plugin) to support development of it by other IDEs like IntelliJ IDEA.",no,,[],,
SLING-6142,853,Add grep like support in Log Tailer.Sling Commons Log webconsole plugin has a basic tail support.I would like to add basic grep support to allow tailing lines matching certain pattern,no,,[],,
SLING-6284,854,"PreparePackageMojo.testSubsystemBaseGeneration often fails on Jenkins.The test seems to fail every other run.The error is 
  <SOURCECODE>",no,,[],,
SLING-6373,855,Context-Aware Config: Allow alternative configuration bucket names.for better supporting legacy configuration systems it would be useful to allow to specify alternative names to "sling:configs" for storing the configurations.we have such features already for property names controlling inheritance.for writeback via ConfigManager always the primary name "sling:configs" is used.,no,,[],,
SLING-6409,856,"Models Impl: Use shading instead of embedding for commons.osgi and scripting.core.currently the dependencies 
   
   org.apache.sling.commons.osgi 
   org.apache.sling.scripting.core 
   
  are embedded in the bundle via bnd because some classes in newer versions are required.this is ok, but we should use maven shade plugin instead and additionally relocate the package names, because otherwise this creates problems in unit tests context when other projects have dependencies to older versions of those.",no,,[],,
SLING-6702,857,"Make MetricsService accessible as easily as a Logger.Metrics are useful in all classes, not only OSGi components, so getting the <tt>MetricsService should be as useful as getting a <tt>Logger for example.I'll add a public <tt>MetricsServiceFactory class to our metrics module, usable like 
  <SOURCECODE> 
  There's already a private <tt>MetricsServiceFactory class in that module, I'll rename that to <tt>InternalMetricsServiceFactory to avoid confusion.",no,,[],,
SLING-6986,858,"The ResourceResolver mock does not test the resource type hierarchy in isResourceType.The mock resource resolver is only plainly checking the resource type on the resource, and not following the type hierarchy.So assuming we have a resource of the form: myResource 
  <ul class=""alternate"" type=""square""> 
   jcr:primaryType=""unstructured"" 
   sling:resourceType=""foo/bar"" 
   
  mockResourceResolver.isResourceType(myResource, ""foo/bar"") =&gt; true mockResourceResolver.isResourceType(myResource, ""nt:unstructured"") =&gt; false 
  Without implementing the whole type hierarchy, we could at least improve this by checking: 
  <ul class=""alternate"" type=""square""> 
   the ""sling:resourceType"" 
   the ""sling:resourceSuperType"" 
   the ""jcr:primaryType""",no,,[],,
SLING-7151,859,"Resources with relative paths aren't being resolved correctly for overlays.Our parsys overlay at /apps/foundation/components/parsys isn't working in AEM 6.3 due to an issue with the way sling is resolving relative paths for resources.The error occurs in parsys.jsp at line 120 when the 'new' resource is embedded using the cq:include tag.The path being used to resolve the 'new' resource is foundation/components/parsys/new.Based off the default configuration in the Apache Sling Resource Resolver Factory OSGI config, the expected behavior is for sling to check for the resource under /apps, and if it's not there then search under /libs.The problem seems to be that sling is ignoring the default configuration and not doing subsequent searches for relative paths when it doesn't find the resource under /apps.As discussed in this 
   <URL>, the workaround to fix the particular incident with the parsys is to copy the 'new' node from /libs/foundation/components/parsys to our overlay under /apps.However, this doesn't solve the overarching issue of sling not resolving the relative path correctly which could impact other parts of the site where we're including resources on pages using relative paths.I've attached the code for parsys.jsp as well as screenshots of our overlay under /apps and the original component under /libs.",no,,[],,
SLING-7641,860,"Use Default if Value is Blank on sling:encode Tag.According to the description in the TLD, the blank value is: 
  ""The default value to be used if the value is either&nbsp;null or an empty string.""However, the Encode Tag only uses a null check to decide to use the default value.This should instead check if the string is null or empty.",no,,[],,
SLING-791,861,"Redirect location invalid when request has trailing /.When a request whose request URL has a trailing slash is resolved to a sling:redirect resource, the redirect Location: header is computed incorrectly.Example: Consider a setup, where a request without an extension is redirected to the same URL but with a "".html"" extension.Any trailing slash in the initial request is ignored.Thus the request 
   <URL> should be redirected to 
   <URL>.Problem is, that logic to create relative Location: headers is incorrectly handling the trailing slash, thus instead of redirecting to a relative path of ../part.html, the redirect goes to part.html thus resulting in the URL 
   <URL> to be sent by the client browser.",no,,[],,
SLING-8324,862,"xing-login and xing-oauth do not work out of the box.Neither 
   <URL>&nbsp;nor 
   <URL>&nbsp;work after being installed on a&nbsp;clean&nbsp;Sling&nbsp;instance.For both bundles (
   <URL>&nbsp;+ 
   <URL>) the following import is missing: 
  <SOURCECODE>",no,,[],,
SPARK-10898,863,Setting spark.streaming.concurrentJobs causes blocks to be deleted before read.The scheduler deletes the block literally just before it is used first time.The input is set to mem and disk ser.<PROFILING>.<ERROR>.,yes,dynamic profiling data for performance,"[sk2_block_hang_crash, pf3_profiling, tk7_concurrent]",,
SPARK-1096,864,"Add Scalastyle Plug-in For Spaces after Comments.After line length (which is now checked) this is the number one thing people forget in pull requests.We should see whether a custom scalastyle rule can be used to check this.In general I'm guessing we'll want to add several custom rules, so this is a good time to look into how it works.The scala AST seems to have comments as recognized token so it might work, but not sure if they are gobbled before the plug-in gets to it.This is something to look into.I find this particular style issue so pervasive and annoying that I'd even settle for a bash command inside of our test script that greps throughout the codebase.Scalastyle is a much better way to do this though - if we can make it work!",no,,[],,
SPARK-11097,865,Add connection established callback to lower level RPC layer so we don't need to check for new connections in NettyRpcHandler.receive.I think we can remove the check for new connections in NettyRpcHandler.receive if we just add a channel registered callback to the lower level network module.,no,,[],,
SPARK-11309,866,"Clean up hacky use of MemoryManager inside of HashedRelation.In HashedRelation, there's a hacky creation of a new MemoryManager in order to handle broadcasting of BytesToBytesMap: 
   <URL> 
  Something similar to this has existed for a while, but the code recently became much messier as an indirect consequence of my memory manager consolidation patch.We should see about cleaning this up and removing the hack.",no,,[ss4_memory],,
SPARK-11937,867,"Python API coverage check found issues for ML during 1.6 QA.Here is the todo list of 
   <URL> found issues: Note: I did not list the SparkR related features (such as ml.feature.Interaction).We have supported RFormula as a wrapper at Python side, I think we should discuss the necessary to support other R related features at Python side.",no,,[],,
SPARK-12059,868,Standalone Master assertion error.<SOURCECODE>,no,,[],,
SPARK-13023,869,"Check for presence of 'root' module after computing test_modules, not changed_modules.There's a minor bug in how we handle the `root` module in the `modules_to_test()` function in `dev/run-tests.py`: since `root` now depends on `build` (since every test needs to run on any build test), we now need to check for the presence of root in `modules_to_test` instead of `changed_modules`.",no,,[],,
SPARK-13298,870,"DAG visualization does not render correctly for jobs.Whenever I try to open the DAG for a job, I get something like this: 
  <span class=""image-wrap"" style=""""><img src=""https://issues.apache.org/jira/secure/attachment/12787639/12787639_dag_viz.png"" style=""border: 0px solid black""></span> 
  Obviously the svg doesn't get resized, but if I resize it manually, only the first of four stages in the DAG is shown.The js console says (variable v is null in peg$c34): 
  <SOURCECODE> 
  (tested in FIrefox 44.0.1 and Chromium 48.0.2564.103)",no,,[ss11_rework],,
SPARK-14072,871,"Show JVM information when we run Benchmark.When we run a benchmark program, the result also shows processor information.Since a version of JVM may also affect performance, it would be good to show JVM version information.Current: 
  <SOURCECODE> 
  Proposal: 
  <SOURCECODE>",yes,JVM affects performance,"[gk4_performance, pf4_benchmark]",,
SPARK-14259,872,Add config to control maximum number of files when coalescing partitions.The FileSourceStrategy currently has a config to control the maximum byte size of coalesced partitions.It is helpful to also have a config to control the maximum number of files as even small files have a non-trivial fixed cost.The current packing can put a lot of small files together which cases straggler tasks.,no,,"[ss24_cost, sk3_byte]",,
SPARK-15258,873,"Nested/Chained case statements generate codegen over 64k exception.Nested/Chained case-when expressions generate a codegen goes beyound 64k exception.A test attached demonstrates this behaviour.I'd like to try and fix this but don't really know the best place to start.Ideally, I'd like to avoid the codegen fallback as with large volumes this hurts performance.This is similar(ish) to 
   <URL> but I'd like to see if there are any alternatives to the codegen fallback approach.",yes,hurts performance,[gk4_performance],,
SPARK-15796,874,"Reduce spark.memory.fraction default to avoid overrunning old gen in JVM default config.While debugging performance issues in a Spark program, I've found a simple way to slow down Spark 1.6 significantly by filling the RDD memory cache.This seems to be a regression, because setting ""spark.memory.useLegacyMode=true"" fixes the problem.Here is a repro that is just a simple program that fills the memory cache of Spark using a MEMORY_ONLY cached RDD (but of course this comes up in more complex situations, too): 
  <SOURCECODE> 
  If I call it the following way, it completes in around 5 minutes on my Laptop, while often stopping for slow Full GC cycles.I can also see with jvisualvm (Visual GC plugin) that the old generation of JVM is 96.8% filled.<SOURCECODE> 
  If I add any one of the below flags, then the run-time drops to around 40-50 seconds and the difference is coming from the drop in GC times: --conf ""spark.memory.fraction=0.6"" OR --conf ""spark.memory.useLegacyMode=true"" OR --driver-java-options ""-XX:NewRatio=3"" 
  All the other cache types except for DISK_ONLY produce similar symptoms.It looks like that the problem is that the amount of data Spark wants to store long-term ends up being larger than the old generation size in the JVM and this triggers Full GC repeatedly.I did some research: 
   
   In Spark 1.6, spark.memory.fraction is the upper limit on cache size.It defaults to 0.75.In Spark 1.5, spark.storage.memoryFraction is the upper limit in cache size.It defaults to 0.6 and... 
   
    <URL> even says that it shouldn't be bigger than the size of the old generation.On the other hand, OpenJDK's default NewRatio is 2, which means an old generation size of 66%.Hence the default value in Spark 1.6 contradicts this advice.<URL> recommends that if the old generation is running close to full, then setting spark.memory.storageFraction to a lower value should help.I have tried with spark.memory.storageFraction=0.1, but it still doesn't fix the issue.This is not a surprise: 
   <URL> explains that storageFraction is not an upper-limit but a lower limit-like thing on the size of Spark's cache.The real upper limit is spark.memory.fraction.To sum up my questions/issues: 
   
   At least 
    <URL> should be fixed.Maybe the old generation size should also be mentioned in configuration.html near spark.memory.fraction.Is it a goal for Spark to support heavy caching with default parameters and without GC breakdown?If so, then better default values are needed.",yes,performance regression,"[sk13_regression, ss13_cache, gk4_performance, pf1_percentage, ss2_all_to_one, gk7_speed, pf2_duration, gk3_reduce, gk2_complexity]",,
SPARK-17187,875,"Support using arbitrary Java object as internal aggregation buffer object.Background 
  For aggregation functions like sum and count, Spark-Sql internally use an aggregation buffer to store the intermediate aggregation result for all aggregation functions.Each aggregation function will occupy a section of aggregation buffer.Problem 
  Currently, Spark-sql only allows a small set of Spark-Sql supported storage data types stored in aggregation buffer, which is not very convenient or performant, there are several typical cases like: 
  1.If the aggregation has a complex model CountMinSketch, it is not very easy to convert the complex model so that it can be stored with limited Spark-sql supported data types.2It is hard to reuse aggregation class definition defined in existing libraries like algebird.3It may introduces heavy serialization/deserialization cost when converting a domain model to Spark sql supported data type.For example, the current implementation of `TypedAggregateExpression` requires serialization/de-serialization for each call of update or merge.Proposal 
  We propose: 
  1.Introduces a TypedImperativeAggregate which allows using arbitrary java object as aggregation buffer, with requirements like: 
  <ul class=""alternate"" type=""square""> 
   It is flexible enough that the API allows using any java object as aggregation buffer, so that it is easier to integrate with existing Monoid libraries like algebird.We don't need to call serialize/deserialize for each call of update/merge.Instead, only a few serialization/deserialization operations are needed.This is to guarantee the performance.2Refactors `TypedAggregateExpression` to use this new interface, to get higher performance.3Implements Appro-Percentile and other aggregation functions which has a complex aggregation object with this new interface.",yes,"Instead, only a few serialization/deserialization operations are needed.This is to guarantee the performance.","[gk4_performance, sk1_negative_necessary, ss26_negative_do_everything, gk2_complexity]",,
SPARK-17499,876,"make the default params in sparkR spark.mlp consistent with MultilayerPerceptronClassifier.several default params in sparkR spark.mlp is wrong, 
  layers should be null tol should be 1e-6 stepSize should be 0.03 seed should be -763139545",no,,[],,
SPARK-17596,877,"Streaming job lacks Scala runtime methods.When using -&gt; in Spark Streaming 2.0.0 jobs, or using spark-streaming-kafka-0-8_2.11 v2.0.0, and submitting it with spark-submit, I get the following error: 
   <ERROR>.This only happens with spark-streaming, using ArrowAssoc in plain non-streaming Spark jobs works fine.I put a brief illustration of this phenomenon to a GitHub repo: 
   <URL> 
  Putting only provided dependencies to build.sbt 
  ""org.apache.spark"" %% ""spark-core"" % ""2.0.0"" % ""provided"", ""org.apache.spark"" %% ""spark-streaming"" % ""2.0.0"" % ""provided"" 
  using -&gt; anywhere in the driver code, packing it with sbt-assembly and submitting the job results in an error.This isn't a big problem by itself, using ArrayAssoc can be avoided, but spark-streaming-kafka-0-8_2.11 v2.0.0 has it somewhere inside, and generates the same error.Packing with scala-library, can see the class in the jar after packing, but it's still reported missing in runtime.The issue reported on StackOverflow: 
   <URL>",no,,[pf1_percentage],,
SPARK-17626,878,"TPC-DS performance improvements using star-schema heuristics.TPC-DS performance improvements using star-schema heuristics <br class=""atl-forced-newline""> <br class=""atl-forced-newline""> TPC-DS consists of multiple snowflake schema, which are multiple star schema with dimensions linking to dimensions.A star schema consists of a fact table referencing a number of dimension tables.Fact table holds the main data about a business.Dimension table, a usually smaller table, describes data reflecting the dimension/attribute of a business.<br class=""atl-forced-newline""> <br class=""atl-forced-newline""> As part of the benchmark performance investigation, we observed a pattern of sub-optimal execution plans of large fact tables joins.Manual rewrite of some of the queries into selective fact-dimensions joins resulted in significant performance improvement.This prompted us to develop a simple join reordering algorithm based on star schema detection.The performance testing using 1TB TPC-DS workload shows an overall improvement of 19%.<br class=""atl-forced-newline""> <br class=""atl-forced-newline""> Summary of the results: 
  <SOURCECODE> 
  Compared to baseline (Negative = improvement; Positive = Degradation): 
  <SOURCECODE> 
  Cluster: 20-node cluster with each node having: 
   
   10 2TB hard disks in a JBOD configuration, 2 Intel(R) Xeon(R) CPU E5-2680 v2 @ 2.80GHz processors, 128 GB RAM, 10Gigabit Ethernet.Total memory for the cluster: 2.5TB 
   Total storage: 400TB 
   Total CPU cores: 480 
   
  Hadoop stack: IBM Open Platform with Apache Hadoop v4.2.Apache Spark 2.0 GA 
  Database info: 
   
   Schema: TPCDS 
   Scale factor: 1TB total space 
   Storage format: Parquet with Snappy compression 
   
  Our investigation and results are included in the attached document.There are two parts to this improvement: 
  <ol> 
   Join reordering using star schema detection 
   New selectivity hint to specify the selectivity of the predicates over base tables.Selectivity hint is optional and it was not used in the above TPC-DS tests.<br class=""atl-forced-newline""> 
  </ol>",yes,TPC-DS performance improvements using star-schema heuristics.,"[gk4_performance, pf1_percentage, tk3_memory_unit, pf4_benchmark, ss11_rework]",,
SPARK-18356,879,"KMeans should cache RDD before training.Hello, 
  I'm newbie in spark, but I think that I found a small problem that can affect spark Kmeans performances.Before starting to explain the problem, I want to explain the warning that I faced.I tried to use Spark Kmeans with Dataframes to cluster my data 
  df_Part = assembler.transform(df_Part)  df_Part.cache() while (k&lt;=max_cluster) and (wssse &gt; seuilStop): kmeans = KMeans().setK(k) model = kmeans.fit(df_Part) wssse = model.computeCost(df_Part) k=k+1 
  but when I run the code I receive the warning : WARN KMeans: The input data is not directly cached, which may hurt performance if its parent RDDs are also uncached.I searched in spark source code to find the source of this problem, then I realized there is two classes responsible for this warning:  (mllib/src/main/scala/org/apache/spark/mllib/clustering/KMeans.scala ) (mllib/src/main/scala/org/apache/spark/ml/clustering/KMeans.scala ) 
  When my dataframe is cached, the fit method transform my dataframe into an internally rdd which is not cached.Dataframe -&gt; rdd -&gt; run Training Kmeans Algo(rdd) 
  -&gt; The first class (ml package) responsible for converting the dataframe into rdd then call Kmeans Algorithm -&gt;The second class (mllib package) implements Kmeans Algorithm, and here spark verify if the rdd is cached, if not a warning will be generated.So, the solution of this problem is to cache the rdd before running Kmeans Algorithm.<URL> All what we need is to add two lines: Cache rdd just after dataframe transformation, then uncached it after training algorithm.I hope that I was clear.If you think that I was wrong, please let me know.Sincerely, Zakaria HILI",no,,"[ss13_cache, gk4_performance, ss2_all_to_one]",,
SPARK-18547,880,"Decouple I/O encryption key propagation from UserGroupInformation.Currently, the encryption key used by the shuffle code is propagated using <tt>UserGroupInformation and thus only works on YARN.That makes it really painful to write unit tests in core that include encryption functionality.We should change that so that writing these tests is possible, and also because that would allow shuffle encryption to work with other cluster managers.",no,,[],,
SPARK-18914,881,"Local UDTs test (org.apache.spark.sql.UserDefinedTypeSuite) fails due to ""ClassCastException: java.lang.Integer cannot be cast to org.apache.spark.sql.UDT$MyDenseVector"".<SOURCECODE> 
  <SOURCECODE>",no,,[],,
SPARK-18948,882,"Add Mean Percentile Rank metric for ranking algorithms.Add the Mean Percentile Rank (MPR) metric for ranking algorithms, as described in the paper : Hu, Y., Y. Koren, and C. Volinsky.????????????laborative Filtering for Implicit Feedback Datasets.???????????? In 2008 Eighth IEEE International Conference on Data Mining, 263????????????72, 2008.doi:10.1109/ICDM.2008.22.(
   <URL>) (NB: MPR is called ""Expected percentile rank"" in the paper) 
  The ALS algorithm for implicit feedback in Spark ML is based on the same paper.Spark ML lacks an implementation of an appropriate metric for implicit feedback, so the MPR metric can fulfill this use case.This implementation add the metric to the RankingMetrics class under org.apache.spark.mllib.evaluation (
   <URL>), and it uses the same input (prediction and label pairs).",no,,[],,
SPARK-19816,883,"DataFrameCallbackSuite doesn't recover the log level.""DataFrameCallbackSuite.execute callback functions when a DataFrame action failed"" sets the log level to ""fatal"" but doesn't recover it.Hence, tests running after it won't output any logs except fatal logs.",no,,[],,
SPARK-20913,884,"Reference is ambiguous exception during dropDuplicates.I'm getting an error when I try to do .dropDuplicates, its giving u""Reference 'xcol14x' is ambiguous exception.Changing the alias cannot be a solution as , if I have 40 column in each dataframe its merely impossible to change the name of each.",no,,[],,
SPARK-21072,885,"`TreeNode.mapChildren` should only apply to the children node..Just as the function name and comments of `TreeNode.mapChildren` mentioned, the function should be apply to all currently node children.So, the follow code should judge whether it is the children node.<URL>",no,,[ss2_all_to_one],,
SPARK-21188,886,"releaseAllLocksForTask should synchronize the whole method.Since the objects readLocksByTask, writeLocksByTask and infos are coupled and supposed to be modified by other threads concurrently, all the read and writes of them in the releaseAllLocksForTask method should be protected by a single synchronized block.The fine-grained synchronization in the current code can cause some test flakiness.",no,,"[sk2_block_hang_crash, tk7_concurrent]",,
SPARK-23328,887,"Disallow default value None in na.replace/replace when 'to_replace' is not a dictionary.We happened to set <tt>None as a default value via 
   <URL> which is quite weird.Looks we better only do this when the input is dictionary.Please see 
   <URL>",no,,[],,
SPARK-23472,888,"Add config properties for administrator JVM options.In our environment, users&nbsp;may need to add JVM options to their Spark applications (e.g. to override log configuration).They typically use <tt>--driver-java-options&nbsp;or <tt>spark.executor.extraJavaOptions.Both set extraJavaOptions properties.We also have a set of administrator JVM options to apply&nbsp;that set the garbage collector (G1GC) and kill the driver JVM on OOM.These two&nbsp;use cases both need to set extraJavaOptions properties, but will clobber one another.In the past we've maintained wrapper scripts, but&nbsp;this causes our default properties to be maintained in scripts rather than our spark-defaults.properties.I think we should add&nbsp;defaultJavaOptions properties that are added along with extraJavaOptions.Administrators&nbsp;could set defaultJavaOptions and these would always get added to the JVM command line, along with any user options instead of getting overwritten by user options.",no,,[ss4_memory],,
SPARK-2350,889,"Master throws NPE.... if we launch a driver and there are more waiting drivers to be launched.This is because we remove from a list while iterating through this.Here is the culprit from Master.scala (L487 as of the creation of this JIRA, commit bc7041a42dfa84312492ea8cae6fdeaeac4f6d1c).<SOURCECODE>",no,,[tk6_iteration],,
SPARK-23562,890,"RFormula handleInvalid should handle invalid values in non-string columns..Currently&nbsp;when handleInvalid is set to 'keep' or 'skip' this only applies to String fields.Numeric fields that are null will either cause the transformer to fail or might be null in the resulting label column.I'm not sure what the semantics of keep might be for numeric columns with null values, but we should be able to at least support skip for these types.--&gt; Discussed offline: null values can be converted to NaN values for ""keep""",no,,[sk10_skip],,
SPARK-23837,891,"Create table as select gives exception if the spark generated alias name contains comma.For spark generated alias name contains comma, Hive metastore throws exception.0: jdbc:hive2://ha-cluster/default&gt; create table a (col1 decimal(18,3), col2 decimal(18,5)); 
   <ins>-----------</ins>+ 
  <div class=""table-wrap""> 
   <table class=""confluenceTable"">
    <tbody> 
     <tr> 
      <td class=""confluenceTd"">Result</td> 
     </tr> 
    </tbody>
   </table> 
  </div> 
  <ins>-----------</ins>+ <ins>-----------</ins>+ 
   No rows selected (0.171 seconds) 0: jdbc:hive2://ha-cluster/default&gt; select col1*col2 from a; <ins>-------------------------------------------------------------------------------------------</ins> 
  <div class=""table-wrap""> 
   <table class=""confluenceTable"">
    <tbody> 
     <tr> 
      <td class=""confluenceTd"">(CAST(col1 AS DECIMAL(20,5)) * CAST(col2 AS DECIMAL(20,5)))</td> 
     </tr> 
    </tbody>
   </table> 
  </div> 
  <ins>-------------------------------------------------------------------------------------------</ins> <ins>-------------------------------------------------------------------------------------------</ins> No rows selected (0.168 seconds) 0: jdbc:hive2://ha-cluster/default&gt; create table b as select col1*col2 from a; 
  Error: org.apache.spark.sql.AnalysisException: Cannot create a table having a column whose name contains commas in Hive metastore.Table: `default`.`b`; Column: (CAST(col1 AS DECIMAL(20,5)) * CAST(col2 AS DECIMAL(20,5))); (state=,code=0)",no,,[pf2_duration],,
SPARK-24258,892,"SPIP: Improve PySpark support for ML Matrix and Vector types.<h1><a name=""BackgroundandMotivation%3A""></a>Background and Motivation:</h1> 
  In Spark ML (<tt>pyspark.ml.linalg), there are four column types you can construct, <tt>SparseVector, <tt>DenseVector, <tt>SparseMatrix, and <tt>DenseMatrix.In PySpark, you can construct one of these vectors with <tt>VectorAssembler, and then you can run python UDFs on these columns, and use <tt>toArray() to get numpy ndarrays and do things with them.They also have a small native API where you can compute <tt>dot(), <tt>norm(), and a few other things with them (I think these are computed in scala, not python, could be wrong).For statistical applications, having the ability to manipulate columns of matrix and vector values (from here on, I will use the term tensor to refer to arrays of arbitrary dimensionality, matrices are 2-tensors and vectors are 1-tensors) would be powerful.For example, you could use PySpark to reshape your data in parallel, assemble some matrices from your raw data, and then run some statistical computation on them using UDFs leveraging python libraries like statsmodels, numpy, tensorflow, and scikit-learn.I propose enriching the <tt>pyspark.ml.linalg types in the following ways: 
  <ol> 
   Expand the set of column operations one can apply to tensor columns beyond the few functions currently available on these types.Ideally, the API should aim to be as wide as the numpy ndarray API, but would wrap Breeze operations.For example, we should provide <tt>DenseVector.outerProduct() so that a user could write something like <tt>df.withColumn(""XtX"", df
     <ERROR></ERROR>.outerProduct(df
     <ERROR></ERROR>)).Make sure all ser/de mechanisms (including Arrow) understand these types, and faithfully represent them as natural types in all languages (in scala and java, Breeze objects, in python, numpy ndarrays rather than the pyspark.ml.linalg types that wrap them, in SparkR, I'm not sure what, but something natural) when applying UDFs or collecting with <tt>toPandas().Improve the construction of these types from scalar columns.The <tt>VectorAssembler API is not very ergonomic.I propose something like <tt>df.withColumn(""predictors"", Vector.of(df
     <ERROR></ERROR>, df
     <ERROR></ERROR>, df
     <ERROR></ERROR>)).</ol> 
  <h1><a name=""TargetPersonas%3A""></a>Target Personas:</h1> 
  Data scientists, machine learning practitioners, machine learning library developers.<h1><a name=""Goals%3A""></a>Goals:</h1> 
  This would allow users to do more statistical computation in Spark natively, and would allow users to apply python statistical computation to data in Spark using UDFs.<h1><a name=""NonGoals%3A""></a>Non-Goals:</h1> 
  I suppose one non-goal is to reimplement something like statsmodels using Breeze data structures and computation.That could be seen as an effort to enrich Spark ML itself, but is out of scope of this effort.This effort is just to make it possible and easy to apply existing python libraries to tensor values in parallel.<h1><a name=""ProposedAPIChanges%3A""></a>Proposed API Changes:</h1> 
  Add the above APIs to PySpark and the other language bindings.I think the list is: 
  <ol> 
   <tt>pyspark.ml.linalg.Vector.of(*columns) 
   <tt>pyspark.ml.linalg.Matrix.of(&lt;not sure what goes here, maybe we don't provide this&gt;) 
   For each of the matrix and vector types in <tt>pyspark.ml.linalg, add more methods like <tt>outerProduct, <tt>matmul, <tt>kron, etc.<URL> has a good list to look at.</ol> 
  Also, change python UDFs so that these tensor types are passed to the python function not as {Sparse,Dense}{Matrix,Vector} objects that wrap <tt>numpy.ndarray, but as <tt>numpy.ndarray objects by themselves, and interpret return values that are <tt>numpy.ndarray objects back into the spark types.",no,,[ss26_negative_do_everything],,
SPARK-24345,893,"Improve ParseError stop location when offending symbol is a token.In the case where the offending symbol of a syntaxError is a CommonToken, this PR increases the accuracy of the start and stop origin by leveraging the start and stop index information from CommonToken in the syntax error listener.<URL>",no,,[],,
SPARK-24501,894,"Add metrics for Mesos Driver and Dispatcher.The Mesos Dispatcher currently only has three gauge metrics, for the number of waiting, launched, and retrying drivers, defined in MesosClusterSchedulerSource.scala.Meanwhile, the Mesos Driver doesn't have any metrics defined at all.Implement additional metrics for both the Mesos Dispatcher and Mesos Driver, including e.g. types of events received, how long it&nbsp;takes jobs to run, for use by operators to monitor and/or diagnose their Spark environment.",no,,[],,
SPARK-24687,895,"When NoClassDefError thrown during task serialization will cause job hang.When below exception thrown: 
  <SOURCECODE> 
  Stage will always hang.Not abort.<span class=""image-wrap"" style=""""><img src=""https://issues.apache.org/jira/secure/attachment/12929657/12929657_hanging-960.png"" style=""border: 0px solid black""></span>  It is because NoClassDefError will no be catch up below.<SOURCECODE>",yes,When NoClassDefError thrown during task serialization will cause job hang,[sk2_block_hang_crash],,
SPARK-2507,896,"Compile error of streaming project with 2.0.0-cdh4.6.0.Hi, 
  When compiling with <URL>.I have the following errors on streaming java api: <ERROR>.",no,,[],,
SPARK-25959,897,"Difference in featureImportances results on computed vs saved models.I tried to implement GBT and found that the feature Importance computed while the model was fit is different when the same model was saved into a storage and loaded back.I also found that once the persistent model is loaded and saved back again and loaded, the feature importance remains the same.Not sure if its bug while storing and reading the model first time or am missing some parameter that need to be set before saving the model (thus model is picking some defaults - causing feature importance to change) 
  &nbsp; 
  Below is the test code: 
  val testDF = Seq( (1, 3, 2, 1, 1), (3, 2, 1, 2, 0), (2, 2, 1, 1, 0), (3, 4, 2, 2, 0), (2, 2, 1, 3, 1) ).toDF(""a"", ""b"", ""c"", ""d"", ""e"") 
  val featureColumns = testDF.columns.filter(_ != ""e"") // Assemble the features into a vector val assembler = new VectorAssembler().setInputCols(featureColumns).setOutputCol(""features"") // Transform the data to get the feature data set val featureDF = assembler.transform(testDF) 
  // Train a GBT model.val gbt = new GBTClassifier() .setLabelCol(""e"") .setFeaturesCol(""features"") .setMaxDepth(2) .setMaxBins(5) .setMaxIter(10) .setSeed(10) .fit(featureDF) 
  gbt.transform(featureDF).show(false) 
  // Write out the model 
  featureColumns.zip(gbt.featureImportances.toArray).sortBy(-_._2).take(20).foreach(println) /* Prints 
  (d,0.5931875075767403) (a,0.3747184548362353) (b,0.03209403758702444) (c,0.0) 
  */ gbt.write.overwrite().save(""file:///tmp/test123"") 
  println(""Reading model again"") val gbtload = GBTClassificationModel.load(""file:///tmp/test123"") 
  featureColumns.zip(gbtload.featureImportances.toArray).sortBy(-_._2).take(20).foreach(println) 
  /* 
  Prints 
  (d,0.6455841215290767) (a,0.3316126797964181) (b,0.022803198674505094) (c,0.0) 
  */ 
  gbtload.write.overwrite().save(""file:///tmp/test123_rewrite"") 
  val gbtload2 = GBTClassificationModel.load(""file:///tmp/test123_rewrite"") 
  featureColumns.zip(gbtload2.featureImportances.toArray).sortBy(-_._2).take(20).foreach(println) 
  /* prints (d,0.6455841215290767) (a,0.3316126797964181) (b,0.022803198674505094) (c,0.0) 
  */",no,,"[ss29_save_time_space, ss3_in_one_go, ss11_rework]",,
SPARK-26385,898,"YARN - Spark Stateful Structured streaming HDFS_DELEGATION_TOKEN not found in cache.Hello, 
  &nbsp; 
  I have Spark Structured Streaming job which is runnning on YARN(Hadoop 2.6.0, Spark 2.4.0).After 25-26 hours, my job stops working with following error: 
  <SOURCECODE> 
  &nbsp; 
  <sup>It is important to notice that I tried usual fix for this kind of problems:</sup> 
  &nbsp; 
  <SOURCECODE>",no,,[pf2_duration],,
SPARK-26710,899,"ImageSchemaSuite has some errors when running it in local laptop.ImageSchemaSuite and org.apache.spark.ml.source.image.ImageFileFormatSuite has some errors when running it in local laptop 
  <SOURCECODE>",no,,[],,
SPARK-3001,900,"Improve Spearman's correlation.The current implementation requires sorting individual columns, which could be done with a global sort.",no,,[],,
SPARK-3684,901,"Can't configure local dirs in Yarn mode.We can't set SPARK_LOCAL_DIRS or spark.local.dirs because they're not picked up in Yarn mode.However, we can't set YARN_LOCAL_DIRS or LOCAL_DIRS either because these are overridden by Yarn.I'm trying to set these through SPARK_YARN_USER_ENV.I'm aware that the default behavior is for Spark to use Yarn's local dirs, but right now there's no way to change it even if the user wants to.",no,,[],,
SPARK-3780,902,"YarnAllocator should look at the container completed diagnostic message.Yarn will give us a diagnostic message along with a container complete notification.We should print that diagnostic message for the spark user.For instance, I believe if it the container gets shot for being over its memory limit yarn would give us a useful diagnostic saying that.This would be really useful for the user to be able to see.",no,,[ss4_memory],,
SPARK-3851,903,"Support for reading parquet files with different but compatible schema.Right now it is required that all of the parquet files have the same schema.It would be nice to support some safe subset of cases where the schemas of files is different.For example: 
  <ul class=""alternate"" type=""square""> 
   Adding and removing nullable columns.Widening types (a column that is of both Int and Long type)",no,,[],,
SPARK-4412,904,"Parquet logger cannot be configured.The Spark ParquetRelation.scala code makes the assumption that the parquet.Log class has already been loaded.If ParquetRelation.enableLogForwarding executes prior to the parquet.Log class being loaded then the code in enableLogForwarding has no affect.ParquetRelation.scala attempts to override the parquet logger but, at least currently (and if your application simply reads a parquet file before it does anything else with Parquet), the parquet.Log class hasn't been loaded yet.Therefore the code in ParquetRelation.enableLogForwarding has no affect.If you look at the code in parquet.Log there's a static initializer that needs to be called prior to enableLogForwarding or whatever enableLogForwarding does gets undone by this static initializer.The ""fix"" would be to force the static initializer to get called in parquet.Log as part of enableForwardLogging.PR will be forthcomming.",no,,[],,
SPARK-4548,905,"Python broadcast perf regression from Spark 1.1.Python broadcast in 1.2 is much slower than 1.1:  
  In spark-perf tests: name 1.1 1.2 speedup python-broadcast-w-set 3.63 16.68 -78.23%",no,,"[sk13_regression, pf1_percentage, gk7_speed]",,
SPARK-4959,906,"Attributes are case sensitive when using a select query from a projection.Per 
   <URL>, see this line of code, where we should be using an attribute map 
   <URL> 
  To reproduce, i ran the following in the Spark shell: 
  <SOURCECODE> 
  The full stack trace printed for the final command that is failing:  
  <SOURCECODE>",no,,[ss1_one_per],,
SPARK-6062,907,"HashMap.merged - Null Pointer Exception.Hi, I am getting an error with the following, 
  <ERROR>.",no,,[],,
SPARK-6066,908,"Metadata in event log makes it very difficult for external libraries to parse event log.The fix for 
   <URL> added a line at the beginning of the event log that encodes metadata.This line makes it much more difficult to parse the event logs from external libraries (like 
   <URL>, which is used by folks at Berkeley) because: 
  (1) The metadata is not written as JSON, unlike the rest of the file (2) More annoyingly, if the file is compressed, the metadata is not compressed.This has a few side-effects: first, someone can't just use the command line to uncompress the file and then look at the logs, because the file is in this weird half-compressed format; and second, now external tools that parse these logs also need to deal with this weird format.We should fix this before the 1.3 release, because otherwise we'll have to add a bunch more backward-compatibility code to handle this weird format!",no,,[],,
SPARK-6242,909,"Support replace (drop) column for parquet table.<URL> provides a easy way of support for add column to parquet tables.This is done by using the native parquet capability of merging the schema from all the part-files and _common_metadata files.But, if someone wants to drop a column from the parquet table, this still does not work.This happens because, the merged schema shall still show the dropped column, but the column is no more there in metastore.So, the schema's obtained from the two sources do not match, and hence any subsequent query on this table fails.Instead of checking for exact match between the two schemas, spark should only check if the schema obtained from metastore is subset of parquet merged schema or not.If this check passes, then the columns present in metastore should be allowed to be referred in the query.",no,,[],,
SPARK-6363,910,"Switch to Scala 2.11 for default build.Mostly all libraries already moved to 2.11 and many are starting to drop 2.10 support.So, it will be better if Spark binaries would be build with Scala 2.11 by default.",no,,[],,
SPARK-6734,911,"Support GenericUDTF.close for Generate.Some third-party UDTF extension, will generate more rows in the ""GenericUDTF.close()"" method, which is supported by Hive.<URL> 
  However, Spark SQL ignores the ""GenericUDTF.close()"", and it causes bug while porting job from Hive to Spark SQL.",no,,[],,
SPARK-7894,912,"Graph Union Operator.This operator aims to union two graphs and generate a new graph directly.The union of two graphs is the union of their vertex sets and their edge families.Vertexes and edges which are included in either graph will be part of the new graph.<blockquote>
   G ???????????? H = (VG ???????????? VH, EG ???????????? EH).</blockquote> 
  The below image shows a union of graph G and graph H 
  <span class=""image-wrap"" style=""display: block; text-align: center""><img src=""https://issues.apache.org/jira/secure/attachment/12735570/12735570_union_operator.png"" width=""600px"" style=""border: 0px solid black""></span> 
  A Simple interface would be???????????? 
  <blockquote>
   def union
    <ERROR></ERROR>(other: Graph
    <ERROR></ERROR>): Graph
    <ERROR></ERROR>
  </blockquote> 
  However, inevitably vertexes and edges overlapping will happen between borders of graphs.For vertex, it's quite nature to just make a union and remove those duplicate ones.But for edges, a mergeEdges function seems to be more reasonable.<blockquote>
   def union
    <ERROR></ERROR>(other: Graph
    <ERROR></ERROR>, mergeEdges: (ED, ED) =&gt; ED): Graph
    <ERROR></ERROR>
  </blockquote>",no,,[],,
SPARK-903,913,"Inconsistent spark assembly.I have two problems when using the spark assembly (build via sbt\sbt assembly) 
  1.HDFS file system is not available 
  <blockquote> 
   java.io.IOException: No FileSystem for scheme: hdfs
  </blockquote> 
  This seems to be caused by overriding the file <tt>META-INF/services/org.apache.hadoop.fs.FileSystem 
  Merging the original files manually resolves this problem: 
  <ul class=""alternate"" type=""square""> 
   hadoop-hdfs-2.0.0-cdh4.3.0.jar and 
   hadoop-common-2.0.0-cdh4.3.0.jar 
   
  2.Illegal jar file on windows The assembly jar contains  
  <ul class=""alternate"" type=""square""> 
   a directory META-INF/license 
   a file META-INF/LICENSE As Windows file system is case-insensitive, this can cause trouble.",no,,[],,
SPARK-9380,914,"Pregel example fix in graphx-programming-guide.Pregel operator to express single source shortest path does not work due to incorrect type of the graph: Graph
   <ERROR></ERROR> should be Graph
   <ERROR></ERROR>",no,,[],,
SVN-1034,915,"Bogus output from svn diff on two unrelated file URLs.<pre>Ben Collins-Sussman asked me to file this issue.Second -- Eric's original bug.He ran 'svn diff' on two unrelated
 file URLs.Consistent with dir_delta, he was shown two complete diffs
 representing a delete and an add.In Chicago, we're all three in
agreement that this should never happen.We don't care about how it's
 implemented, or whether it ""makes sense"" given the implementation --
 it's just wrong output.None of us could come up with a *single*
; use-case where a user would ever want or expect this behavior, even
 if they knew svn's internals inside and out.This should be filed
as a bug.I thought someone mentioned that we have special-case code for diffing
a single file, since dir_delta() won't work on a lone file.My
""first"" proposal may or may not fix this problem, I'm not sure till I
investigate implementations.I just wanted to point out that this bug
has a life of its own, no matter how it ends up fixed.</pre>
   
  Original issue reported by epg",no,,[],,
SVN-1414,916,"std=c89 not supported.<pre>added one more m4 script to build/ac-macros, called
check-gcc-opt.m4.This defines one function
svn_lib_check_gcc_option(), that
takes one possible CFLAG option and tries to compile an empty main() with
it.If it compiles and runs OK, then the option is considered valid.</pre>
   
  
   <URL> 
  Original issue reported by pll",no,,[],,
SVN-1544,917,"Database corruption with error svn: locate_key: no such string: 'f9a'.<pre>NOTE: This seems to be very hard to reproduce, but there have been at least two occurrences 
recently and I want to put something in the Issue Tracker for others to find.My case: Subversion 0.24.2, Mac OS X 10.2.6, Apache web server 2.0.46 all built from the release 
tarball, all non-admin access via http.Back end is Berkeley DB 4.1.25 built from release tarball.OK, the repository is seriously toast.After restoring from a backup, we tried to check in additional 
changes and ran into tons of problems.Individual files keep causing repository deaths with errors 
like:

subversion/libsvn_fs/bdb/strings-table.c:101: (apr_err=160010)
svn: Filesystem has no such string
svn: locate_key: no such string `h6w'

Worse, it's inconsistent throughout the repository.Most files work.Some files don't, consistently.Occasionally, if we delete a file from the head of the tree, add back a small file (like only a few 
bytes), then replace it in the wc with the real (~1.5-2 MB file) and do a check-in it works.Other 
times it doesn't.The error code is consistent, but the 3 letters `h6w' are not -- other times they 
are `f8r' or other letter-number-letter combos.The repository dies the same way when I run 
'svnadmin dump' on it, even after running 'svnadmin restore'.Also, prior to going to the backup copy of the repository, we ran 'svnadmin recover' on the 
repository as we were getting the error:

Berkeley DB error while opening `copies' table for filesystem /Library/Repository/
MacOSXTroubleshooting/db: Invalid argument

whenever we tried any operation on the repository.Follow the threads on the users@subversion.tigris.org list with the titles: ""Berkeley DB corruption - 
'changes' table (again)"" and ""Dump: Filesystem has no such string "" for more detail.http://subversion.tigris.org/servlets/ReadMsg?list=users&amp;msgNo=1928
http://subversion.tigris.org/servlets/ReadMsg?list=users&amp;msgNo=1653
</pre>
   
  Original issue reported by psuh",no,,"[tk3_memory_unit, ss22_cd_files, sk3_byte]",,
SVN-1619,918,"svn client and cygwin compatibility.<pre>Coming from the unix world, I tend to use the cygwin shell and write scripts for
cygwin when I'm using windows.While the svn command line client works fine when
executed from the cygwin shell, there is one small trouble with it.It outputs
file names using backslashes as seperators.While it is the OS-correct default,
I think an option to output file paths with forward slashes would be great.Atm
when I want to process the output of svn status, I have to pipe through sed to
rework the file paths:

$ svn status | grep ^?| cut -b 2- | sed -e 's/\\/\//g'
      docs/developer/CHANGES.orig
(instead of docs\developer\CHANGES.orig etc.)

A switch in the svn global options would make my life easier ..</pre>
   
  Original issue reported by timot",no,,[ss11_rework],,
SVN-1677,919,"Constify svn_fs_dirent_t. 
 
 
  
  <pre>The pointer fields of svn_fs_dirent_t ought to point to const data, like most of
our structures do.This is an API change, so it does not belong on the trunk
unless it is approved for the 1.0 branch.</pre>",no,,[],,
SVN-1848,920,"Implement unversion command.<pre>Please provide a feature, that allow users to take files out of the repository,
so that they become unversioned again.Occasionally files end up in the repository, that should rather be ignored (e.g.
local project files).It should be possible to delete them in the repository in
such a way that they will not be deleted locally during the next update.</pre>
   
  Original issue reported by hmf",no,,[],,
SVN-1853,921,"svndumpfilter forgetting parent dirs.<pre>When using svndumpfilter to only include a ""deep"" subtree the filtered dump
lacks creation of the parent directory(ies).For instance:

Start with a new repository and create the following hierarchy:

a -- b -- d
  |
  +- c


Create a dump filtered to include only the a/b hierarchy:

$ svnadmin dump repos | svndumpfilter include a/b &gt;dump.txt""


Import the filtered dump into a newly created repository:

$ svnadmin load filtered &lt;dump.txt
&lt;&lt;&lt; Started new transaction, based on original revision 1
     * adding path : a/b ...svn: File not found: transaction '1', path 'a/b'
$


A workaround should be to create the a directory before importing:

$ svnadmin load filtered &lt;dump.txt
&lt;&lt;&lt; Started new transaction, based on original revision 1
     * adding path : a/b ... done.* adding path : a/b/d ... done.------- Committed new rev 2 (loaded from original rev 1) &gt;&gt;&gt;
</pre>
   
  Original issue reported by gabuzo",no,,[ss8_load_nn],,
SVN-2210,922,"[PATCH] Ruby SWIG bindings.<pre>Patch by Kouhei Sutou (kou_at_cozmixng.org).Adds/updates SWIG-based Ruby
bindings.Garrett Rooney was having some problems getting this running,
possibly because he may have been using ruby 1.6 instead of 1.8.Kouhei Sotou
gave instructions for configuring and building the bindings in this message:
http://svn.haxx.se/dev/archive-2005-01/0730.shtml

These bindings require SWIG &gt;= 1.3.24, and there may be issues with that since
the Python bindings require SWIG version 1.3.19 and 1.3.21.See this discussion
based on an earlier version of the patch:
http://svn.haxx.se/dev/archive-2005-01/0064.shtml

The original message to announce this patch is archived here:
http://svn.haxx.se/dev/archive-2005-01/0687.shtml

I will also upload the patch as an attachment to this issue.</pre>
   
  
   <URL> 
  Original issue reported by mthelen",no,,[],,
SVN-2248,923,"bump mod_dav's provider API.<pre>We need to bump mod_dav's provider API, so that mod_dav_svn can do its work more efficiently.Specifically, we want to:

   - rev mod_dav's provider API,
   - backport to httpd 2.0.X
   - make svn 1.3 use new API and require the new httpd.Major Changes to make:

   - add a new provider API for handling POST requests.mod_dav currently returns DECLINED on these.Needed so that ra_dav can send multiple lock/unlock jobs
        in *one* request.#NAME?Then we can stop using a custom input filter to get it.#NAME?Small buglets:

   - dav_get_locktoken_list() returns error, rather than NULL list as
     its docstring says.- dav_method_lock() should call provider-&gt;set_headers().(mod_dav_svn/lock.c is currently attaching custom response
         headers through brute force.)</pre>",no,,"[gk1_efficiency, tk18_brute_force]",,
SVN-2386,924,"ac-helpers/install-sh fails due to '\r\n' in file.<pre>The ac-helpers/install-sh script has bogus '\r\n' chars, probably due to a
microsoft helper somewhere.It fails to run on Linux with a strange message:

../subversion-1.2.1/configure --with-berkeley-db=/usr/local/BerkeleyDB.4.3
&lt;&lt;&lt; SNIP &gt;&gt;&gt;
configuring package in apr now
../subversion-1.2.1/configure:
/home/ben/src/test/subversion-1.2.1/ac-helpers/in: bad interpreter: No such file
or directory
../subversion-1.2.1/configure: line 3151: cd: apr: No such file or directory

Running the script by hand gives a hint:
[ben@newstar build]$ sh -x ../subversion-1.2.1/ac-helpers/install-sh -c
+ $'\r'
: command not found/ac-helpers/install-sh: line 25:
+ $'\r'
: command not found/ac-helpers/install-sh: line 26:
+ $'\r'

and sure enough:
[ben@newstar build]$ od -c ../subversion-1.2.1/ac-helpers/install-sh |head
0000000   #   !/   b   i   n   /   s   h  \r  \n   #  \r  \n   #
                                             ^^^^^^
</pre>
   
  Original issue reported by benlaws",no,,[],,
SVN-2677,925,"Why there are no Apache 2.2 binaries?.<pre>What is the reason that there are still no Apache 2.2 Windows binaries of
mod_dav_svn and mod_authz_svn?I heared Apache 2.4 will be out in first quarter of 2007 - and still no 2.2
support :(
Thats no acceptable for security and many other reasons.</pre>
   
  Original issue reported by powerstat",no,,[],,
SVN-2706,926,"svn compiled with serf is not able to connect via proxy.<pre>we tried to use svn with serf library instead of neon, and we get:

$ svn co http://svn.edgewall.com/repos/trac/trunk 
svn: Error running context: Connection timed out

is it possible that it does not respect the $http_proxy setting or the proxy 
entered in .subversion/servers?local checkouts work, and former compilations 
with neon also work.</pre>
   
  Original issue reported by thurnerrupert",no,,[],,
SVN-3062,927,"svnmerge doesn't warn when permission is denied for a path.<pre>svnmerge doesn't warn/throw-error when we try to merge a branch (source) for
which the user has no read permission.In my case, a subdirectory of the
""branch"" being merged didn't have read permission and svnmerge didn't complain
anything.In fact, svnmerge silently ignored that ""subdirectory"" and merged the
rest.It looked like the merge was successful.I feel that svnmerge should throw an error in this case and stop proceeding
further (It should be considered FATAL).Currently the only way to find out that
this has happened is to search the svnmerge-commit-message.txt for ""no author""
pattern.eg.........r4455 | (no author) | (no date) | 1 line
........Version Information:

svnmerge r21994
  modified: 2006-10-17 06:52:10 +0000 (Tue, 17 Oct 2006)

Copyright (C) 2004,2005 Awarix Inc.Copyright (C) 2005, Giovanni Bajo
---
svn, version 1.4.3 (r23084)
   compiled Oct 15 2007, 08:39:54

Copyright (C) 2000-2006 CollabNet.</pre>
   
  Original issue reported by santhoshmani",no,,[pf2_duration],,
SVN-3080,928,"svnsync stops working at a specfic revision.<pre>Hi,

I am mirroring an SVN repository by using svnsync.The repository is quite large
and its latest revision number is close to r17050.I have tried to test svnsync with the repository which is the hotcopy of the
original repository and I have not used the authz file on it.(FYI, the original
repository use the authz file).The test of svnsync was just fine with the test repository.But on the real repository with the authz file, svnsync failed with the
following error messages.
&gt; &gt;Copied properties for revision 1887.
&gt; &gt;Committed revision 1888.
&gt; &gt;Copied properties for revision 1888.
&gt; &gt;Committed revision 1889.
&gt; &gt;Copied properties for revision 1889.
&gt; &gt;svnsync: REPORT request failed on 'https://svn.osl.iu.edu/test-ompi'
&gt; &gt; svnsync: REPORT of 'https://svn.osl.iu.edu/test-ompi': Could not read chunk
delimiter: Secure connection truncated (https://svn.osl.iu.edu)

And I could find the following error logs at /var/log/httpd.
&gt; &gt;[Fri Jan 04 00:29:15 2008] [error] [client 129.79.247.132] Provider
encountered an error while streaming a REPORT response.[404, #0]
&gt; &gt; [Fri Jan 04 00:29:15 2008] [error] [client 129.79.247.132] Problem replaying
revision  [404, #160013]
&gt; &gt; [Fri Jan 04 00:29:15 2008] [error] [client 129.79.247.132] File not found:
revision 1889, path '/trunk/src/mca/llm/base'  [404, #160013]

This failure happens on r1889 all the time and it failed with http(non-ssl) too.The commits to these directories(e.g., /trunk/src/mca/llm/base) happened many
times (and were sync'ed properly) before the above error hit on r1889.What caused this problem?Can anyone help me out to figure out this problem?The version of the SVN server is 1.4.2 and the apache server is httpd-2.0.52-38.ent.Of course, my svnsync is running on SVN-1.4.2.</pre>
   
  Original issue reported by dikim33",no,,[],,
SVN-3223,929,"Allow WC-PATH merge sources in 'svn mergeinfo' subcommand.<pre>$ svn help mergeinfo
mergeinfo: Display merge-related information.usage: mergeinfo SOURCE-URL[@REV] [TARGET[@REV]]

  Display information related to merges (or potential merges) between
  SOURCE-URL and TARGET (default: '.').If the --show-revs option
  is not provided, display revisions which have been merged from
  SOURCE-URL to TARGET; otherwise, display the type of information
  specified by the --show-revs option....

There's no good reason why SOURCE-URL has to be a SOURCE-URL at all.It could
just as easily be a working copy path (from which a URL can be trivially
calculated).This would help folks who routine use working copy paths as merge
sources.</pre>",no,,[],,
SVN-3238,930,"client prompts twice about caching client cert passphrase.<pre>This is with trunk right after r32132.Using the server provided in issue #2489
(desc17 and elsewhere) and localr32132 trunk client, I got this:

   $ rm -rf wc; ns co https://117.193.193.134/svn/repos wc
   Authentication realm: https://117.193.193.134:443
   Client certificate filename: user.p12
   Passphrase for 'user.p12': uservision
   
   -----------------------------------------------------------------------
   ATTENTION!Your passphrase for client certificate:
   
      user.p12
   
   can only be stored to disk unencrypted!You are advised to configure
   your system so that Subversion can store passphrase encrypted, if
   possible.See the documentation for details.You can avoid future appearances of this warning by setting the value
   of the 'store-ssl-client-cert-pp-plaintext' option to either 'yes' or
   'no' in '/home/kfogel/.subversion/servers'.-----------------------------------------------------------------------
   Store passphrase unencrypted (yes/no)?yes
   Error validating server certificate for 'https://117.193.193.134:443':
    - The certificate is not issued by a trusted authority.Use the
      fingerprint to validate the certificate manually!#NAME?Certificate information:
    - Hostname: Senthil Kumaran S
    - Valid: from Thu, 10 Jul 2008 11:36:24 GMT until \
                  Fri, 10 Jul 2009 11:36:24 GMT
    - Issuer: Subversion, My organization, Chennai, TamilNadu, IN
    - Fingerprint: 9b:25:9e:3c:f7:d2:6b:a0:5d:95:2c:a7:50:42:33:ce:da:f0:f1:76
   (R)eject, accept (t)emporarily or accept (p)ermanently?t
   -----------------------------------------------------------------------
   ATTENTION!Your passphrase for client certificate:
   
      user.p12
   
   can only be stored to disk unencrypted!You are advised to configure
   your system so that Subversion can store passphrase encrypted, if
   possible.See the documentation for details.You can avoid future appearances of this warning by setting the value
   of the 'store-ssl-client-cert-pp-plaintext' option to either 'yes' or
   'no' in '/home/kfogel/.subversion/servers'.-----------------------------------------------------------------------
   Store passphrase unencrypted (yes/no)?yes
   A    wc/README
   Checked out revision 1.$ 

When I did that, my ~/.subversion/servers file looked like this:

   ### This file specifies server-specific parameters,
   ### including HTTP proxy information, HTTP timeout settings,
   ### and authentication settings.###
   ### The currently defined server options are:
   ###   http-proxy-host            Proxy host for HTTP connection
   ###   http-proxy-port            Port number of proxy host service
   ###   http-proxy-username        Username for auth to proxy service
   ###   http-proxy-password        Password for auth to proxy service
   ###   http-proxy-exceptions      List of sites that do not use proxy
   ###   http-timeout               Timeout for HTTP requests in seconds
   ###   http-compression           Whether to compress HTTP requests
   ###   neon-debug-mask            Debug mask for Neon HTTP library
   ###   http-auth-types            Auth types to use for HTTP library
   ###   ssl-authority-files        List of files, each of a trusted CAs
   ###   ssl-trust-default-ca       Trust the system 'default' CAs
   ###   ssl-client-cert-file       PKCS#12 format client certificate file
   ###   ssl-client-cert-password   Client Key password, if needed.###   ssl-pkcs11-provider        Name of PKCS#11 provider to use.###   http-library               Which library to use for http/https
   ###                              connections (neon or serf)
   ###   store-passwords            Specifies whether passwords used
   ###                              to authenticate against a
   ###                              Subversion server may be cached
   ###                              to disk in any way.###   store-plaintext-passwords  Specifies whether passwords may
   ###                              be cached on disk unencrypted.###   store-ssl-client-cert-pp   Specifies whether passphrase used
   ###                              to authenticate against a client
   ###                              certificate may be cached to disk
   ###                              in any way
   ###   store-ssl-client-cert-pp-plaintext
   ###                              Specifies whether client cert
   ###                              passphrases may be cached on disk
   ###                              unencrypted (i.e., as plaintext).###   store-auth-creds           Specifies whether any auth info
   ###                              (passwords as well as server certs)
   ###                              may be cached to disk.###
   ### Set store-passwords to 'no' to avoid storing passwords in the
   ### auth/ area of your config directory.It defaults to 'yes',
   ### but Subversion will never save your password to disk in
   ### plaintext unless you tell it to.### Note that this option only prevents saving of *new* passwords;
   ### it doesn't invalidate existing passwords.(To do that, remove
   ### the cache files by hand as described in the Subversion book.)###
   ### Set store-plaintext-passwords to 'no' to avoid storing
   ### passwords in unencrypted form in the auth/ area of your config
   ### directory.Set it to 'yes' to allow Subversion to store
   ### unencrypted passwords in the auth/ area.The default is
   ### 'ask', which means that Subversion will ask you before
   ### saving a password to disk in unencrypted form.Note that
   ### this option has no effect if either 'store-passwords' or 
   ### 'store-auth-creds' is set to 'no'.###
   ### Set store-ssl-client-cert-pp to 'no' to avoid storing ssl
   ### client certificate passphrases in the auth/ area of your
   ### config directory.It defaults to 'yes', but Subversion will
   ### never save your passphrase to disk in plaintext unless you tell
   ### it to via 'store-ssl-client-cert-pp-plaintext' (see below).###
   ### Note store-ssl-client-cert-pp only prevents the saving of *new*
   ### passphrases; it doesn't invalidate existing passphrases.To do
   ### that, remove the cache files by hand as described in the
   ### Subversion book at http://svnbook.red-bean.com/nightly/en/\
   ###                    svn.serverconfig.netmodel.html\
   ###                    #svn.serverconfig.netmodel.credcache
   ###
   ### Set store-ssl-client-cert-pp-plaintext to 'no' to avoid storing
   ### passphrases in unencrypted form in the auth/ area of your
   ### config directory.Set it to 'yes' to allow Subversion to
   ### store unencrypted passphrases in the auth/ area.The default
   ### is 'ask', which means that Subversion will prompt before
   ### saving a passphrase to disk in unencrypted form.Note that
   ### this option has no effect if either 'store-auth-creds' or 
   ### 'store-ssl-client-cert-pp' is set to 'no'.###
   ### Set store-auth-creds to 'no' to avoid storing any Subversion
   ### credentials in the auth/ area of your config directory.### Note that this includes SSL server certificates.### It defaults to 'yes'.Note that this option only prevents
   ### saving of *new* credentials;  it doesn't invalidate existing
   ### caches.(To do that, remove the cache files by hand.)###
   ### HTTP timeouts, if given, are specified in seconds.A timeout
   ### of 0, i.e. zero, causes a builtin default to be used.###
   ### The commented-out examples below are intended only to
   ### demonstrate how to use this file; any resemblance to actual
   ### servers, living or dead, is entirely coincidental.### In the 'groups' section, the URL of the repository you're
   ### trying to access is matched against the patterns on the right.### If a match is found, the server options are taken from the
   ### section with the corresponding name on the left.[groups]
   # group1 = *.collab.net
   # othergroup = repository.blarggitywhoomph.com
   # thirdgroup = *.example.com
   
   ### Information for the first group:
   # [group1]
   # http-proxy-host = proxy1.some-domain-name.com
   # http-proxy-port = 80
   # http-proxy-username = blah
   # http-proxy-password = doubleblah
   # http-timeout = 60
   # http-auth-types = basic;digest;negotiate
   # neon-debug-mask = 130
   # store-plaintext-passwords = no
   
   ### Information for the second group:
   # [othergroup]
   # http-proxy-host = proxy2.some-domain-name.com
   # http-proxy-port = 9000
   # No username and password for the proxy, so use the defaults below.### You can set default parameters in the 'global' section.### These parameters apply if no corresponding parameter is set in
   ### a specifically matched group as shown above.Thus, if you go
   ### through the same proxy server to reach every site on the
   ### Internet, you probably just want to put that server's
   ### information in the 'global' section and not bother with
   ### 'groups' or any other sections.###
   ### Most people might want to configure password caching
   ### parameters here, but you can also configure them per server
   ### group (per-group settings override global settings).###
   ### If you go through a proxy for all but a few sites, you can
   ### list those exceptions under 'http-proxy-exceptions'.This only
   ### overrides defaults, not explicitly matched server names.###
   ### 'ssl-authority-files' is a semicolon-delimited list of files,
   ### each pointing to a PEM-encoded Certificate Authority (CA) 
   ### SSL certificate.See details above for overriding security 
   ### due to SSL.[global]
   # http-proxy-exceptions = *.exception.com, www.internal-site.org
   # http-proxy-host = defaultproxy.whatever.com
   # http-proxy-port = 7000
   # http-proxy-username = defaultusername
   # http-proxy-password = defaultpassword
   # http-compression = no
   # http-auth-types = basic;digest;negotiate
   # No http-timeout, so just use the builtin default.# No neon-debug-mask, so neon debugging is disabled.# ssl-authority-files = /path/to/CAcert.pem;/path/to/CAcert2.pem
   #
   # Password / passphrase caching parameters:
   # store-passwords = no
   # store-plaintext-passwords = no
   # store-ssl-client-cert-pp = no
   # store-ssl-client-cert-pp-plaintext = no
</pre>",no,,"[ss1_one_per, sk8_timeout_expiration, sk1_negative_necessary, ss26_negative_do_everything, pf2_duration]",,
SVN-3329,931,"Update throws error when skipping some tree conflicts.<pre>Update/switch fails if the target itself is a tree conflict victim.Should print a 'Skipped' message instead.Update of an item that is inside a tree conflict, and that no longer
exists in the repo, does nothing.Should print a 'Skipped' message
instead.</pre>
   
  Original issue reported by sbutler",no,,[sk10_skip],,
SVN-3361,932,"Foreign repository checking (during merge) is too strict.<pre>It appears that, when executing a merge, a repository is deemed a ""foreign
repository"" based on a simple match of the url.This is often wrong.A DNS search domain, multiple hostnames, etc can all fool
subversion into thinking it is using a foreign repos when this is not the case.Wouldn't it be just as easy to use the Repository UUID to make this determination?</pre>
   
  Original issue reported by mcrawfor",no,,[],,
SVN-3539,933,"Commit fails in root of Windows drive.<pre>----- DESCRIPTION -----

Issue is under Subversion 1.6.6 and was also seen under previous versions.If a repository is in the root of a Windows drive, the commit causes the following:

<LOG>.Original issue reported by dvorkin_m",no,,[],,
SVN-3740,934,"Use Neon's system_proxy autodetection if no proxy has been explicitly specified..<pre>Patch Submitted By : Dominique Leuenberger &lt;dominique@leuenberger.net&gt;


As a notebook user I'm of course using it on different
locations/networks and one thing rather annoying is that subversion only
cares for it's own configuration file when it comes to proxy
connections.Since version 0.29.0 of libneon, it features a ne_session_system_proxy
API, which allows to transparently have the proxy settings filled, based
on what you have configured in your gnome/kde session (it uses libproxy
in the background for this).Should you happen to run subversion on any
different kind of session, is is falling back to using envvar.I've been working with subversion the last few days with this
modification and it served me a lot, as I do only have to change my
gnome session's proxy settings now.svn is inheriting them, avoiding the
need to configure this file in plus (one location to rule them all!).I'd be really glad if you would consider merging this patch (I wrote it
initially for 1.6.12, then later ported to svn trunk, which is slightly
different... I only tested the 1.6.12 version of the patch!).Both patches are attached here for reference.</pre>
   
  
   <URL> 
  Original issue reported by gavinbaumanis",no,,[pf2_duration],,
SVN-3838,935,"post-commit goes through invalid database states.<pre>Post-commit processing is still node-by-node and goes through invalid database
states.svnadmin create repo
svn mkdir --parents file://`pwd`/repo/A/B/C
svn co file://`pwd`/repo wc
svn cp wc/A wc/X
svn ci -mm wc

Interrupt the post-commit processing at the start of the second call to
commit_node().Run ""svn cleanup wc"".Now:

$ sqlite3 wc/.svn/wc.db ""select op_depth, local_relpath, presence from nodes
order by op_depth, local_relpath""
0||normal
0|A|normal
0|A/B|normal
0|A/B/C|normal
0|X|normal
0|X/B|normal
1|X/B/C|normal

The workqueue causes X/B to go through post-commit processing, but not X/B/C.The database state is invalid: X/B/C op-depth=1.Ideally moving the copied tree from WORK to BASE should happen in a single
transaction.</pre>",no,,[ss3_in_one_go],,
SVN-3951,936,"serf fail prop_tests.py 3 with 1.6 mod_dav_svn.prop_tests.py 3 fails for ra_serf when run against a 1.6 mod_dav_svn.It passes
for ra_neon.<LOG>.",no,,[],,
SVN-4305,937,"Merged directory properties are always added.<pre>When merging the addition of a directory tree from a different location in the 
same repository, the resulting nodes are marked as copied.But for directories 
the list of PRISTINE properties is not updated, so the original set of 
properties is seen as a local modification.If you only commit the root of the merge and then revert the children the list 
of properties stored on the directory will be out of sync with what the 
repository knows.(And if you would commit the directory you would send NO-OP 
changes)



As this issue has been there since at least 1.0, most of Subversion handles 
this just fine, but it would be nice to clean it up.</pre>",no,,[],,
SVN-4366,938,"client SEGFAULTs diffing a repos rev in which an empty file was added.<pre>At the moment, Subversion's trunk and 1.8.x branch code SEGFAULT when trying to
show a repos diff of a revision in which an empty file was added.This is caused by a change made in r1470994, where the repository diff code was
changed to lazily open tempfile streams.The lazy open bit is good (and greatly
reduces the number of file handles held open during a repository diff via
ra-serf), but we need to ensure that those stream do, in fact, get eventually
opened.Currently, empty added files passed through the system will not every
trigger the lazy open logic.</pre>",no,,[gk3_reduce],,
SVN-4403,939,Remove Jikes.The current configure script mentions Jikes.This compiler support should be removed.It is a relic from 2005 and outdated.A current JDK is sufficient.,no,,[],,
SVN-4417,940,"Problems with externals in deep subdirectories on Windows.<pre>Windows has a general limit on the path length of 260 characters on some APIs,
while others allow a length of about 32k characters (with some restrictions).Most (if not all) of the SVN codebase seems to work fine with those long pathes.However, it seems that SQLite is not (yet) long path clean, making e. G.
externals fail when they're nested to deeply.</pre>
   
  
   <URL>",no,,[],,
SVN-4517,941,"WANdisco require registration for all downloads.<pre>AFAICT WANdisco now require registration for all OSes

This is omitted from the MacOS and Windows sections.</pre>
   
  
   <URL>",no,,[],,
SVN-4526,942,"SIGSEGFAULT when accessing files.<pre>The current Trac 1.0.2 version segfaults when accessing files from SVN to export
them.Between Trac 1.0.1 and Trac 1.0.2 the python code for accessing files was
modified and it seems this change triggers the segfault in python bindings for
SVN.Some Stacktraces are included in original report and also a detailed
analysis in our report to mod_wsgi (see below) which very likely was the wrong
target.All analysis till now indicates that it is a issue with the python
bindings of SVN.Trac has a workaround now, but the original bug should be fixed as well

Follow up issue to following:
http://trac.edgewall.org/ticket/11805

It's not a wsgi issue as initially thought, nevertheless here the report for
reference (detailed analysis inside):
https://github.com/GrahamDumpleton/mod_wsgi/issues/55

My installed SVN veriosn: 1.8.8-1ubuntu3.1 Other testers may have different
versions running.</pre>
   
  Original issue reported by stoecker",no,,[],,
SVN-4575,943,"Wrong default for 1.8-compatilble wrapper of ISVNClient#cleanup(String).<pre>Reported by Marc Strapetz on dev@:

cleanup-related code which is working fine with 1.8 JavaHL starts failing with 1.9 JavaHL.According to the docs, ISVNClient#cleanup(String) does not break locks, which seems to 
cause the problems:

 <SOURCECODE>.When using ISVNClient.cleanup(path, *true*, true, true, true, false) code works.</pre>",no,,[],,
SVN-4604,944,There is no svnw.exe.Provide svnw.exe .It is the same process like svn.exe but not a 'Console' application.,no,,[],,
SVN-4652,945,"Assertion fail / core dump with merge.The following should be invalid IIRC, but rather than give a helpful error, it core dumped.<SOURCECODE>.",no,,[],,
SVN-499,946,"fix status output by handling absent files locally when updating.<pre>Files that have been removed from the filesystem, but not svn rm'd, have
absent status, which shows up as a `?'in the leftmost column in status
reports.Try this:

   $ svn co http://svn.collab.net/repos/svn -d repro
   $ cd repro
   $ rm README
   $ svn st -n
   $ svn st
   $ svn st -nv
   $ svn st -v

In all of those status outputs, README is correctly printed with a ?.But if you run any combination that includes the -u switch:

   $ svn st -u
   $ svn st -vu
   $ svn st -nu
   $ svn st -nvu
... you will see README printed out like this:

   _    *     -  README

... when it should be:

   ?174  README

... or possibly:

   ?*   174  README

if it has also been modified in the repository.The cause of the problem is that the update reporter reports a missing
file as a `delete', eventually resulting in the ra layer driving an add_file
in the update edit.There is no reason for the repository to resend the
file if it's not modified in the repository, though!The crawler should just
keep a list of what was missing from disk, and during the update phase, copy
those files out of text base.It should never report to the repository that
the file was missing, it should merely report the entry's revision number, so
the repository can return any genuine updates.This would fix two problems:
the fact that an update can cause a file to be sent that the client actually
had locally all along, and the fact that adding -u to status screws up the
reporting of missing files.</pre>",no,,[],,
SVN-539,947,"Windows build of r300 fails with link errors.<pre>Visual Studio project files need to be updated to include new 
source files that were added to subversion..\subversion\libsvn_client\prop_commands.c
 .\subversion\libsvn_client\log.c
 .\subversion\clients\cmdline\log-cmd.c

Here's a patch:


Index: .\subversion\libsvn_client\libsvn_client.dsp
===================================================================
--- .\subversion\libsvn_client\.svn\text-base\libsvn_client.dsp	Tue Oct 23 
00:04:58 2001
+++ .\subversion\libsvn_client\libsvn_client.dsp	Wed Oct 24 17:53:49 2001
@@ -123,6 +123,14 @@
 # End Source File
 # Begin Source File
 
+SOURCE=.\log.c
+# End Source File
+# Begin Source File
+
+SOURCE=.\prop_commands.c
+# End Source File
+# Begin Source File
+
 SOURCE=.\revert.c
 # End Source File
 # Begin Source File
Index: .\subversion\clients\cmdline\subversion_client.dsp
===================================================================
--- .\subversion\clients\cmdline\.svn\text-base\subversion_client.dsp	Tue Oct 
23 00:03:40 2001
+++ .\subversion\clients\cmdline\subversion_client.dsp	Wed Oct 24 17:53:49 2001
@@ -134,6 +134,10 @@
 # End Source File
 # Begin Source File
 
+SOURCE="".\log-cmd.c""
+# End Source File
+# Begin Source File
+
 SOURCE=.\main.c
 # End Source File
 # Begin Source File
</pre>
   
  Original issue reported by barryp",no,,[],,
SVN-701,948,"Subversion properties live in an XML namespace called ""svn:"".<pre>However,

1.""svn:"" is not a valid URI reference as per RFC2396, and thus not valid 
namespace names

2.new URI schemes should be avoided unless it can be demonstrated that they 
are required (that they do something a IETF-registered scheme doesn't do 
already).</pre>",no,,[ss1_one_per],,
SVN-702,949,"error code space fragmentation.<pre>Because subversion has a universal collection of error codes that applies to
both the client and server sides, it is important that the errors codes (which
are transmitted by their numeric value) be as reasonalbe in-sync between those
two sides as possible.That means, we should expect that any client of a given
Major version number should be able to communicate with any server of the same
Major version number, and their codes not be misaligned.To accomplish this, two things need to occur:

-  The error codes in svn_error_codes.h need to be fragmented by their general
module, to create separate ""spaces"" for those codes with room for addition of
new codes in each module ""space"".For example:

   ...
   #define SVN_WC_BASE_ERROR  (SVN_BASE_ERROR + 100)
   #define SVN_FS_BASE_ERROR  (SVN_BASE_ERROR + 200)
   ...
   #define SVN_FS_NO_SUCH_REPRESENTATION (SVN_FS_BASE_ERROR + 15)
   ...

-  Secondly, policy needs to exist (and be enforced) that we do everything
possible to keep error codes in their respective places in the overall ID space.So when an error code is removed, it is simply replaced with a value that means
""there's an empty slot here.""And new codes added to a module's error space go
into the first empty slot in that space.[Obviously, completion of this issue is deemed as having accomplished the code
space fragm",no,,[],,
SVN-710,950,"find_apr.m4 and find_apu.m4 out of date warning by autogen.sh.<pre>Hi,
  Just ran autogen.sh on a freshly checked out svn (rev 2016), apr and 
apr-util.I got the following:

% autogen.sh
buildcheck: checking installation...
buildcheck: autoconf version 2.52 (ok)
buildcheck: libtool version 1.4.2 (ok)
buildcheck: neon version 0.19.4 (ok)
buildcheck: local copy of find_apr.m4 does not match APR's copy.An updated copy of find_apr.m4 may need to be checked in.buildcheck: local copy of find_apu.m4 does not match APRUTIL's copy.An updated copy of find_apu.m4 may need to be checked in.Later,

\x/ill         :-}
</pre>
   
  Original issue reported by uther",no,,[pf1_percentage],,
SVN-900,951,"svn_ra_local__split_URL should distinguish EPERM.<pre>trying local access to a repository to which one does not have
(file-level) permissions results in the following error:

svn: Couldn't find a repository.svn: Unable to open an ra_local session to URL
svn: svn_ra_local__split_URL: Unable to find valid repository

This makes sense for ENOENT, but it would be more clear if EPERM was
checked and a specific error message printed in that case.svnadmin is much intelligible in the same case:

svn: Berkeley DB error
svn: Berkeley DB error while opening environment for filesystem
/home/subversion/hopfield/db:
Permission denied
</pre>
   
  Original issue reported by rillian",no,,[],,
SVN-979,952,"Plugged in character code conversions.<pre>{ Note: I am currently CVS user who monitors subversions and plans
   to migrate in some future. }It would be nice if it was possible to somehow hook into subversion
processing
the character code conversions so they can be performed both on commit
(before storing the new version) and on update (before sending the new
version to the client).The real problem I would like to solve:

I am Polish.Unfortunately, there exist two encodings of my national
characters: standard iso-8859-2 used on Linux/Unix/Mac/VMS and
non-standard win-1250 used on Windows (thank you Microsoft).Those
encodings are not compatible (in particular some Polish national
characters are encoded differently).I develop in cross-platform environment, where many source files are
simultaneously edited by the developers using Windows and developers
using Linux/Unix.The result?Depending on who is commiting, comments and text constants are encoded
in one
or the second encoding (and some files are encoded in both the ways).It would be really nice if I could arrange things so (of course the
details are to
be scripted, the subversion role is to provide needed hooks their
parameters):
- if the Windows developer commits, the file is transcoded from
win-1250 to iso-8859-2
   so the repository contains the files in iso-8859-2 encoding
- if the Windows developer updates/checks out/diffs/???the data sent
to him are 
   transcoded to win-1250 so he or she sees national characters correctly.Some additional related thoughts:
- subversion handles content-type property, maybe it would be worthful
   to keep the character set too (after-all Polish HTML pages
   has usually 
      Content-Type: text/html; charset=iso-8859-2
   instead of usual text/html, same apply for text/plain), I would imagine
   keeping content-charset property and (in case it exits) using it
   while producing Content-Type in HTTP header
- to implement transcoding I described, one would need some 'sandbox
property'
   (in this case 'sandbox charset' - win-1250 for Windows and
iso-8859-2 otherwise)
- maybe in some future we could keep the data in UTF-8 and use simple
   encodings only in sandboxes?</pre>
   
  Original issue reported by marcink",no,,[],,
TAP5-1080,953,"Page activation context lost when redirecting from HTTP to HTTPS due to the @Secure annotation.When using @Secure on a page, if we get the page in http, we are redirected to the https page.But the context path is lost.Example : 
 
   Connection #0 to host localhost left intact 
   Closing connection #0 bouil@maggie:~/Documents/workspace/tapestry-secure-test$  
   
  The Index.java page contains : 
<SOURCECODE>.",no,,[],,
TAP5-1390,954,Functional programming improvements.Add support for Tuples to the tapestry-func library,no,,[],,
TAP5-140,955,The services used to handle live reloading should be made public.User services should be able to register for check-for-updates and/or invalidation event notifications without having to use any Tapestry internals.Firing such events should still be private/internal.,no,,[ss11_rework],,
TAP5-1444,956,"Index pages in subfolders should have precedence over start pages.When there is both, an index and a start page in a subpackage, the index page shold have precedence over the start page since those are deprecated.Later, support for start pages in subpackages should be removed.",no,,[],,
TAP5-1517,957,"Improve exception message about unknown validation macro.Using unknown validation macro result in an exception, as follows: 
  Unknown validator type 'foo'.Configured validators are email, max, maxlength, min, minlength, none, regexp, required.The message should list all the known validation macros as well.",no,,[pf2_duration],,
TAP5-1588,958,"French properties file for the kaptcha component.As asked by Igor (
   <URL>), here is the french properties file for the kaptcha component.",no,,[],,
TAP5-1633,959,"Hardcoded value for ""Dismiss All"" in AlertManager.The ""Dissmiss all"" label that is shown just under the active alerts, is hardcoded in t5-alerts.js, and I see no easy way of overriding it, except with JavaScript, which is just ugly.Please provide a way for us to override this message.",no,,[],,
TAP5-1847,960,"An extension component template (with a root t:extend element) should allow t:block elements to be nested.Currently, this isn't possible, and the &lt;t:block&gt; elements must be placed inside some available &lt;t:replace&gt; block.It makes sense that defining a block specific to a sub-component should be allowed, even if other aspects of the containing template are not to be overridden.",no,,[sk2_block_hang_crash],,
TAP5-1996,961,Add Severity.SUCCESS enum for alerts.I'm currently implementing twitter-bootstrap integration for alerts and found that tapestry5 lacks this value.<URL>,no,,[],,
TAP5-1997,962,"Spelling mistakes in the IoC configuration documentation page.On that page 
   <URL>, search for the following lines: 
  <ul class=""alternate"" type=""square""> 
   we use a inner class for the implementation Should be: we use a<img class=""emoticon"" src=""https://issues.apache.org/jira/images/icons/emoticons/thumbs_down.png"" height=""16"" width=""16"" align=""absmiddle"" alt="""" border=""0""> inner class for the implementation 
   
  <ul class=""alternate"" type=""square""> 
   we've triggered Tapestry to collected all the ordering information.Should be: we've triggered Tapestry to (collect) all the ordering information.",no,,[],,
TAP5-2021,963,"create compatible client side method of encoding context parameters.Currently there is no (provided) way to replicate the tapestry propriety URL encoding.See org.apache.tapestry5.internal.services.URLEncoderImpl.encode(String) 
  This can be limiting for advanced users wiring up custom components which may create requests via javascript.I personally have come up against this a number of times.I propose that tapestry.js include a function which does the equivalent of URLEncoder.encode, or even better, ComponentResources.createEventLink etc.",no,,[],,
TAP5-2115,964,"Document how the hibernate session is implemented.There have been many questions on the user's list about how the hibernate session is implemented in tapestry-hibernate and also how to use tapestry-hibernate outside of a tapestry managed request / response.I think that the documentation should mention the following: 1.The hibernate session service provided by tapestry-hibernate is a singleton 2.The singleton is a proxy that points to a per-thread, lazy loaded hibernate session instance 3.The per-thread session instance is cleaned up by Registry.cleanupThread() / PerThreadManager.cleanup() 4.Tapestry automatically cleans up the thread local inside the normal request / response flow 5.Outside of a tapestry managed request / response, you must explicitly cleanup the thread 6.Transaction management outside of a request/response flow a. @CommitAfter and @Match(""*DAO"") with HibernateTransactionAdvisor b. HibernateSessionManager.commit() and abort() c. session.getTransaction().commit()",no,,[],,
TAP5-2138,965,"Support multiple @PageActivationContext.The @PageActivationContext annotation (and PageActivationContextWorker) could be improved to accept an ""index"" parameter.This way, I could have multiple @PageActivationContext properties.eg 
  <SOURCECODE> 
  I'd expect tapestry to generate the following URL's: 
  <ul class=""alternate"" type=""square""> 
   /mypage (category and item is null) 
   /mypage/category1 (item is null) 
   /mypage/$N/item1 (category is null) 
   /mypage/category1/item1",no,,[],,
TAP5-2163,966,Adapt DateField style to Bootstrap 3.The DateField component's popup should be styled using Bootstrap 3 mechanisms.,no,,[],,
TAP5-2176,967,"SymbolBeanFactoryPostProcessor breaks property placeholder in spring when using default values.When a property used in a spring config xml has a default value tapestry will set this as the value of the property, then the spring property placeholder will run but he will not replace the property with the correct value from a properties file because it is already set by tapestry with the default value!Tapestry Spring Module SymbolBeanFactoryPostProcessor should only touch properties that start with 'tapestry.'",no,,[],,
TAP5-2212,968,"If component example contains a pagelink with wrong parameter name/type.<URL> 
  Example .tml for this component (If) throws an exception 
  Parameter(s) 'page' are required for org.apache.tapestry5.corelib.components.PageLink, but have not been bound.&lt;t:if test=""user""&gt; Welcome back, ${user.firstName} &lt;p:else&gt; &lt;t:pagelink name=""login""&gt;Login&lt;/t:pagelink&gt; / &lt;t:pagelink name=""register""&gt;Register&lt;/t:pagelink&gt; &lt;/p:else&gt; &lt;/t:if&gt; 
  Simple fix 's/name/page/g' in that example.",no,,[tk2_bound],,
TAP5-2299,969,"Update the gradle wrapper version from 1.3 to the latest.As per the discussion here: 
   <URL> the main build.gradle file is not compatible with gradle version 1.3.Please update to latest which currently is 1.11.",no,,[ss1_one_per],,
TAP5-2496,970,"Inconsistent behavior of EnumValueEncoder .EnumValueEncoder uses <tt>java.lang.Enum.name() in <tt>toClient(E) but uses <tt>typeCoercer.coerce(clientValue, enumType) in <tt>toValue(String clientValue).I have a custom coercer in my project and this causes the value encoder to be unable to decode the client-side value back to a server-side value.I think we should remove <tt>EnumValueEncoderFactory altogether because <tt>TypeCoercedValueEncoderFactory should do its job just fine.",no,,[],,
TAP5-299,971,"NPE in SelectModelRenderer when Palette used with mismatched values and model.Create a model with A,B as the available options and set the palette value parameter to a list with C.You will get a NPE here: 
  org.apache.tapestry5.internal.util.SelectModelRenderer.option(SelectModelRenderer.java:49) org.apache.tapestry5.corelib.components.Palette$SelectedRenderer.render(Palette.java:147) 
  because valueToOptionModel in Palette doesn't contain a mapping for C.",no,,[],,
TAP5-302,972,"URL encoded strings that contain symbols such as  %2f (encoded ""/"") are decoded incorrectly in some environments.If an activation context variable contains %2f (URL encoded /) it is interpreted as / and therefore interpreted as a separation of context variables.example: 
  
   <URL> 
  I expect the above URL to contain three context variables nl: test, one and t/wo but Tapestry thinks it contains four context variables nl: test, one, t and wo 
  This makes it impossible to use Base64 encoded context variables because they can contain the / symbol (which is gets URL encoded to %2f)",no,,[],,
TAP5-366,973,"Upgrading to 5.0.16 makes application not displayable anymore.Hi, I've upgraded yesterday to 5.0.16 and I encounter a big problem with this version which didn't happen on 5.0.15.When loading the start page, I have a cascade of traces that indicates me that it could not create ""final method_name since it already exists"" This does not happen with 5.0.15 on which I've developping this application all along.Here is the complete stack trace (it is a bit long and may not be this useful): 
  ERROR 
   <ERROR>",no,,[],,
TAP5-559,974,Informal parameters have started to overwrite previously rendered attributes.Since Tapestry3 days I had the habbit that my action links contain a href="#" attribute.That worked until recently.With one of the 5.1 SNAPSHOTS it stopped working.The links are now simply dead.,no,,[pf2_duration],,
TAP5-634,975,"Google App Engine Support: ability to disable code that creates new threads.Right off the bat, the restriction in GAE that no threads be created will affect a few, small sections of Tapestry.In the larger scheme of things, a tapestry-googleappengine library may require a few things: 
  <ul class=""alternate"" type=""square""> 
   Make the GAE services (such as UserService) injectable 
   Support for JDO (much like tapestry-hibernate, today) 
   Other, unforseen 
   
  Also, depending on the license of GAE (and availabilty through Maven) may need to host such a library at Tapestry360.",no,,[],,
TAP5-662,976,Component documentation should identify the events that may be triggered by a component.This has been done informally in the past as just JavaDoc.It would be nice if the events that can be published by a component be statically defined (via a special annotation).,no,,[],,
TAP5-752,977,"Tapestry bin download includes lots of libraries that shouldn't be there due to licensing concerns.Here's what I saw: 
<LOG>.Looks like the dependencies on selenium and hibernate slipped into the distro.",no,,[],,
TAP5-803,978,"ProgressiveDisplay should include a read-only body property of type Block, just like Zone.In other words, allow us to provide a correct return value from the progressivedisplay event handler method ... update the PD with it's server-side body.The context for this was performing multiple zone updates (the PD plus additional zones) from the progressive display event handler method.Perhaps this could be added as an example in the PD component reference.",no,,[sk2_block_hang_crash],,
TAP5-883,979,"Regexp validator behaviour differs on client and server side.In some situations, a value can pass client side regexp validation and fail the server side validation.This should not be the case.The server regexp validator requires a value to match the regexp exactly from start to end which is not the case on the client (browser).For example, given a regexp of 
   <ERROR></ERROR> {2}, a value of hu4 passes validation on the client and fails on the server.Workaround: use anchored regexps like ^
  <ERROR></ERROR>{2} 
  $",no,,[],,
